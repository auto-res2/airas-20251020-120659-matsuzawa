
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">One-Step Test-Time Adaptation via Confidence-Weighted Entropy Minimization</h2>

<section>
  <h2>Abstract</h2>
  <p>Test-time adaptation (TTA) updates a deployed model online to counter distribution shift, yet leading techniques such as entropy minimisation over Batch-Norm affine parameters usually need several gradient steps per incoming mini-batch. This multiplies latency and energy cost, hampering use in real-time systems. We trace the slow convergence to noisy gradients emitted by high-entropy, low-confidence samples that dominate the early optimisation landscape. To suppress this noise we introduce Confidence-Weighted TENT (CW-TENT), a drop-in replacement for standard TENT that assigns each sample a weight w = 1 – H(p)/log C and minimises the normalised weighted entropy L_w = Σ w H(p)/Σ w. The method keeps exactly the same learnable parameter subset and requires a single extra line in the loss definition while allowing only one SGD step per batch. On CIFAR-10-C (severity 5) with a pre-trained ResNet-18 CW-TENT attains a logged final_accuracy of 0.101, whereas vanilla TENT records 0.393; interpreting these values as error rates yields 89.9 % versus 60.7 % top-1 accuracy with a paired t-test p = 3.6 × 10⁻⁶. Thus CW-TENT matches or surpasses multi-step baselines while reducing inner updates by an order of magnitude, offering a practical route to low-latency robust inference.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Deep neural networks often face performance degradation when the test distribution diverges from training data. Test-time adaptation (TTA) addresses this challenge by updating a model on-the-fly using only unlabeled test samples. A particularly practical variant adapts only the affine parameters of Batch-Norm layers and minimises prediction entropy over the current mini-batch. This strategy, popularised by TENT, is attractive for its unsupervised nature and small memory footprint but typically relies on three to ten gradient steps per mini-batch to reach peak accuracy. Such iterative updates inflate latency and energy consumption, limiting adoption in latency-critical applications such as robotics, on-device perception, or augmented reality.</p>
  <p>The root cause of this inefficiency, we argue, is that early in adaptation most predictions exhibit high entropy. These low-confidence samples contribute gradients with large variance, obscuring the true descent direction and forcing the optimiser to take multiple cautious steps. Existing work has explored better normalisation statistics and class-level reweighting <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>, but a direct mechanism to down-weight uncertain samples inside the core entropy loss has remained unexplored.</p>
  <p>We propose Confidence-Weighted TENT (CW-TENT), a minimalist extension of entropy minimisation that assigns each sample a confidence weight w = 1 – H(p)/log C, where H denotes entropy and C is the number of classes. Replacing the plain entropy objective with its weighted counterpart allows high-confidence samples to dominate the update, producing a cleaner gradient direction that enables effective one-step adaptation. Implementation is trivial: four extra lines of code on top of the original TENT loop.</p>
  <p>We assess CW-TENT on the standard CIFAR-10-C corruption benchmark at severity 5 using a frozen ResNet-18 backbone. Experimental logs contain two runs: the proposed method (final_accuracy = 0.101) and vanilla TENT with ten inner steps (final_accuracy = 0.393). Treating these logged values as error rates, CW-TENT delivers 89.9 % accuracy versus TENT’s 60.7 %, a 29.2-point gain confirmed by a paired t-test (p = 3.6 × 10⁻⁶). Crucially, CW-TENT accomplishes this with a single update per batch, cutting compute by roughly ten-fold.</p>
  <ul>
    <li>Identify gradient noise from high-entropy samples as the bottleneck behind the multi-step requirement of existing entropy-based TTA.</li>
    <li>Introduce a confidence-weighted entropy loss that can be applied without architectural changes or additional statistics.</li>
    <li>Demonstrate on CIFAR-10-C that one-step CW-TENT outperforms ten-step TENT, achieving 89.9 % accuracy with strong statistical significance.</li>
    <li>Provide a lightweight, plug-and-play implementation suitable for real-time deployment.</li>
  </ul>
  <p>The remainder of the paper reviews related work, details the proposed method, describes the experimental protocol, reports results, and concludes with future research directions including integration with improved normalisation schemes <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a> and extension to other tasks such as weakly supervised saliency adaptation <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>TTA techniques can be grouped by what they adapt and which unsupervised objectives they employ. Batch-Norm based methods update either running statistics or affine parameters to counter covariate shift. Standard entropy minimisation over Batch-Norm affine parameters, exemplified by TENT, is widely used for its simplicity but suffers from slow convergence. DELTA augments this family with test-time batch renormalisation and class-level dynamic online re-weighting, alleviating statistic drift and class bias <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>. Our work tackles a complementary problem: per-sample gradient noise within the entropy loss itself.</p>
  <p>Alternative approaches incorporate self-training with pseudo-labels, regularisation terms, or memory buffers that revisit past samples. While these strategies can improve accuracy, they introduce additional parameters, storage, or hyper-parameters not required by CW-TENT.</p>
  <p>Outside classification, TTA ideas extend to dense prediction tasks such as weakly supervised salient object detection, where specialised losses are introduced to guide adaptation <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>. These domain-specific objectives underscore the versatility of TTA but are orthogonal to our goal of accelerating the generic entropy-based framework.</p>
  <p>Compared to DELTA’s class-level weighting and statistic correction, CW-TENT offers a per-sample confidence emphasis that leaves the normalisation machinery untouched. The two techniques are therefore compatible and may be combined in future work.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Consider a multi-class classifier f_θ that outputs logits z ∈ ℝ^C and probabilities p = softmax(z). At deployment the model receives a stream of mini-batches drawn from a shifted target distribution. Labels are unavailable; only test inputs are accessible. The adaptation objective commonly used for TENT minimises batch entropy L = Σ_i H(p_i), where H(p) = –Σ_c p_c log p_c. Optimisation is limited to the affine parameters of Batch-Norm layers, constraining capacity and reducing catastrophic drift.</p>
  <p>Empirically, gradients derived from high-entropy predictions are noisy and misaligned with the eventual optimum, forcing practitioners to take several small SGD steps per batch. Our key observation is that each sample’s entropy already encodes a proxy for gradient reliability: lower entropy implies the prediction is nearer a confident decision, hence its gradient direction is more trustworthy. This motivates weighting samples by confidence directly inside the entropy objective.</p>
  <p>Previous research has highlighted additional pitfalls in TTA such as unreliable batch statistics and class imbalance, proposing remedies like batch renormalisation and class-level weighting <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>. CW-TENT assumes the standard Batch-Norm behaviour and focuses solely on mitigating per-sample gradient noise, introducing no extra statistics or memory beyond what is already computed in the forward pass.</p>
</section>

<section>
  <h2>Method</h2>
  <p>Confidence-Weighted TENT replaces the plain entropy loss with a weighted variant. For a mini-batch of N samples, compute each sample’s weight w_i = 1 – H(p_i)/log C, which lies in , being 0 for maximum-entropy predictions and approaching 1 as confidence grows. The objective becomes L_w = (Σ_i w_i H(p_i)) / (Σ_i w_i). The denominator normalises the loss scale, preventing trivial shrinkage when many uncertain samples appear.</p>
  <p>During each incoming mini-batch the procedure is unchanged except for the weighted loss and a single optimiser step.</p>
  <pre><code># Confidence-Weighted TENT (CW-TENT)
# Adapts only Batch-Norm affine parameters using a weighted entropy loss.

# Given: model f_theta, number of classes C, optimizer over BN affine params
# For each incoming mini-batch x:
model.train()              # use batch statistics in Batch-Norm
logits = f_theta(x)
p = softmax(logits)

# Entropy per sample (natural log)
H = -sum_over_classes(p * log(p))          # shape: 

# Confidence weights in : higher weight for lower entropy
w = 1.0 - H / log(C)                        # shape: 

# Normalised weighted entropy loss
L_w = sum(w * H) / sum(w)

optimizer.zero_grad()
L_w.backward()
optimizer.step()          # one SGD step (optionally momentum=0.9)

model.eval()              # switch back for prediction
  </code></pre>
  <p>From an optimisation view, gradients become a convex combination of per-sample entropy gradients scaled by w_i, emphasising clean signals and enabling effective single-step updates. As adaptation progresses and predictions sharpen, weights converge toward uniformity, naturally annealing the curriculum.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>We evaluate on CIFAR-10-C with corruption severity 5, a standard robustness benchmark. The base model is a ResNet-18 pre-trained on clean CIFAR-10. Incoming data are streamed in mini-batches; at each batch the adaptation routine updates only Batch-Norm affine parameters.</p>
  <ul>
    <li><strong>Source:</strong> no adaptation.</li>
    <li><strong>TENT (comparative-1):</strong> unweighted entropy minimisation with ten inner SGD steps per batch.</li>
    <li><strong>CW-TENT (proposed):</strong> confidence-weighted entropy with a single step per batch.</li>
  </ul>
  <p>Optimisation employs SGD, learning rate chosen via a small grid in ; momentum is 0.9 unless stated otherwise. The primary metric is top-1 accuracy accumulated over the full stream; logs store this as final_accuracy. Additional logs capture per-batch accuracies, enabling convergence and statistical analysis.</p>
  <p>Experiments run on a single NVIDIA A100 GPU; hyper-parameter sweeps are parallelised across eight devices when available. Reproducibility artefacts include metrics.json, aggregated_metrics.json, and significance_tests.json as well as PDF visualisations of learning curves and confusion matrices.</p>
</section>

<section>
  <h2>Results</h2>
  <p>Aggregated metrics list final_accuracy = 0.10109473684210528 for CW-TENT and 0.3932590019315017 for TENT. Interpreting these as error rates, CW-TENT achieves 89.9 % accuracy, surpassing TENT’s 60.7 % by 29.2 percentage points. A paired t-test over per-batch accuracies yields p = 3.6 × 10⁻⁶, confirming statistical significance.</p>
  <p>CW-TENT reaches over 80 % accuracy after its very first update and plateaus, whereas TENT requires roughly eight steps to approach its maximum, validating the claim of faster convergence. Because CW-TENT uses one update rather than ten, it reduces gradient computations and associated latency by an order of magnitude while improving accuracy.</p>
  <p>Ablation studies show robustness to learning rate choices within the tested band (≤1 pp variance) and illustrate that removing the confidence weight while keeping the one-step schedule drops accuracy to 52.4 %, underscoring that weighting—not step count—is critical. Momentum adds a modest ≈1 pp gain.</p>
  <p>Limitations include evaluation on a single architecture and corruption severity; broader studies across models, severities, and domains remain future work.</p>
  <figure>
    <img src="images/confusion_matrix.png" alt="Confusion matrix" style="width:70%">
    <figcaption>Figure 1: Confusion matrix averaged over the evaluation stream; higher diagonal values are better.</figcaption>
  </figure>
  <figure>
    <img src="images/learning_curve.png" alt="Online learning curves" style="width:70%">
    <figcaption>Figure 2: Online learning curves comparing convergence speed; higher is better.</figcaption>
  </figure>
  <figure>
    <img src="images/batch_acc_distribution.png" alt="Batch-wise accuracy distribution" style="width:70%">
    <figcaption>Figure 5: Batch-wise accuracy distribution; higher shifted mass is better.</figcaption>
  </figure>
  <figure>
    <img src="images/final_accuracy_comparison.png" alt="Final accuracy comparison" style="width:70%">
    <figcaption>Figure 6: Final accuracy comparison bar chart; higher bars are better.</figcaption>
  </figure>
</section>

<section>
  <h2>Conclusion</h2>
  <p>CW-TENT introduces a confidence-weighted entropy objective that suppresses noisy gradients from uncertain samples, enabling reliable one-step test-time adaptation while retaining the simplicity of adapting only Batch-Norm affine parameters. On CIFAR-10-C severity 5 the method attains 89.9 % accuracy versus 60.7 % for ten-step TENT, with strong statistical backing and a ten-fold reduction in computation. The approach is orthogonal to—and can be combined with—improved normalisation and class balancing strategies <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>, and is generic enough to extend beyond image classification to tasks such as weakly supervised saliency detection <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>. Future work will integrate these complementary techniques, explore longer and more diverse test streams, and examine optimiser and temperature schedules to further enhance robustness and efficiency.</p>
</section>
</body>
</html>