
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Confidence-Weighted Entropy Minimization for Test-Time Adaptation: Promise and Pitfalls on CIFAR-10-C</h2>

<section>
  <h2>Abstract</h2>
  <p>Test-time adaptation (TTA) updates a pre-trained model on an unlabeled test stream to mitigate distribution shift. The dominant approach, TENT, adapts only BatchNorm affine parameters by minimizing prediction entropy, but typically relies on three to ten inner gradient steps per incoming batch, which inflates inference latency. We hypothesize that slow convergence stems from noisy gradients produced by high-entropy, low-confidence samples that dominate early in adaptation. We therefore propose Confidence-Weighted TENT (CW-TENT). CW-TENT keeps the original entropy objective but assigns each sample a weight w = 1 – H(p)/log C, down-weighting uncertain predictions and computing a normalized weighted loss L_w = Σ w·H / Σ w. The expected benefit is a cleaner gradient that permits a single update step per batch. We evaluate CW-TENT on CIFAR-10-C (severity 5) with a pre-trained ResNet-18 and compare it to a static source model and a ten-step TENT baseline. Contrary to the hypothesis, CW-TENT remains at chance-level accuracy (10.1 %), whereas TENT reaches 39.4 %; the gap is statistically significant (p &lt; 0.01). Analysis shows that under severe corruption, predictions are nearly uniform, weights collapse toward zero, and gradients vanish. We discuss why naïve confidence weighting fails in this regime and outline concrete remedies, providing a cautionary tale for uncertainty-aware TTA.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Modern vision models suffer noticeable degradation when deployed under distribution shift. Test-time adaptation (TTA) tackles this problem by updating a source-trained model online, using only the unlabeled target stream. TENT epitomizes a simple yet effective family of TTA methods: switch BatchNorm layers to training mode, freeze all other parameters, and minimize the prediction entropy of the current batch. Empirically, TENT recovers a large fraction of lost accuracy on synthetic corruptions and real-world shifts, but typically performs 3–10 inner gradient steps per batch to reach its best accuracy. In latency-sensitive settings—mobile devices, robotics, interactive systems—this extra compute is unwelcome.</p>
  <p>Why is multi-step optimisation needed? Early in adaptation, the model is highly uncertain; its softmax outputs are almost uniform, yielding high entropy. These samples produce gradients that point in noisy and inconsistent directions, so multiple steps are required to average out the noise. A missing ingredient is an explicit mechanism that trusts confident samples more than uncertain ones when computing the update.</p>
  <p>We draw inspiration from weighting strategies that correct class bias or regularise losses in other test-time settings <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a><a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a> and put forward Confidence-Weighted TENT (CW-TENT). CW-TENT leaves the optimisation loop, architectural constraints, and objective type untouched but multiplies each entropy term by a confidence weight w_i = 1 – H_i/log C. As the weight is zero for a uniform prediction and one for a deterministic one-hot prediction, high-confidence examples receive full influence while low-confidence ones are attenuated. The weighted loss is normalised by the sum of the weights to keep its scale stable.</p>
  <p>We test whether this tiny modification is sufficient to reduce the inner-loop budget from ten steps to a single step without harming accuracy. Our evaluation uses the standard CIFAR-10-C corruption benchmark at severity 5 and a ResNet-18 source model. The experimental design comprises three adapters: (1) Source—no adaptation, (2) TENT—ten inner steps, and (3) CW-TENT—one inner step. All share the same optimiser (SGD) and update only BatchNorm affine parameters.</p>
  <p>The findings defy the optimistic hypothesis. CW-TENT never rises above chance-level accuracy, whereas TENT steadily climbs to 39 %. Learning curves reveal that CW-TENT’s accuracy is flat, its weights collapse, and its gradients vanish. Statistical tests confirm the significance of the gap.</p>
  <ul>
    <li>We introduce CW-TENT, a confidence-weighted variant of entropy minimisation intended to enable one-step test-time adaptation.</li>
    <li>We conduct a controlled study on CIFAR-10-C with ResNet-18, directly comparing CW-TENT, vanilla TENT, and a non-adaptive source model.</li>
    <li>We provide a detailed negative result: CW-TENT is ineffective under severe corruption, performing far below the baseline.</li>
    <li>We analyse failure modes—weight collapse, gradient starvation—and discuss remedies such as temperature scaling, weight clipping, or pairing with improved normalisation statistics <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>.</li>
  </ul>
  <p>These insights help practitioners avoid naïve confidence-based designs and motivate more robust uncertainty-aware adaptation strategies. Future work should test calibrated confidence estimates, combine weighting with Batch Renormalisation, and explore multi-step schedules tailored to the weighted objective.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Entropy-based BatchNorm adaptation. Several works exploit BatchNorm’s affine parameters for TTA by minimising auxiliary self-supervised losses such as entropy or consistency. TENT exemplifies this stream, combining low memory overhead with strong empirical gains, but at the cost of multiple inner steps. Our study keeps the same objective and parameter subset but questions whether a confidence-aware weighting can obviate the step budget.</p>
  <p>Remedies for fully test-time adaptation. DELTA uncovers two pitfalls: unreliable batch statistics and class-biased updates. It proposes Test-time Batch Renormalisation (TBR) and Dynamic Online re-weighTing (DOT) to address them <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>. CW-TENT shares the re-weighting spirit but differs in goal—denoising gradients rather than debiasing classes—and in mechanism—entropy-derived weights rather than class frequency estimates. The incompatibility of our results with DELTA’s success hints that reliable normalisation statistics might be a prerequisite for any weighting to be effective.</p>
  <p>Regularised objectives in test-time scenarios. Work on weakly supervised salient object detection demonstrates that adding a regularised loss can stabilise adaptation <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>. CW-TENT can be interpreted as adaptive regularisation of the entropy loss, although its naïve form proves fragile.</p>
  <p>Comparison. Whereas DELTA adds statistical correction and class-level re-weighting, and regularised losses add auxiliary penalties, CW-TENT tries to accelerate plain entropy minimisation via sample-level confidence weights. Our empirical evidence shows that this narrower intervention is insufficient under heavy corruption, delineating the boundary between effective and ineffective re-weighting schemes.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Problem setting and notation. A pre-trained source model f_θ, trained on clean CIFAR-10, receives a stream of target samples x_t from CIFAR-10-C (severity 5) without labels. At each time step t, a mini-batch of size B is processed. The model outputs softmax probabilities p_i ∈ ℝ^C for each sample i. Only the affine BatchNorm parameters (γ, β) are updated; all other weights stay frozen.</p>
  <p>Entropy minimisation. The per-sample entropy is H_i = −Σ_c p_{i,c} log p_{i,c}. TENT minimises the batch-averaged entropy L = (1/B) Σ_i H_i via SGD over γ and β, performing several gradient steps before moving to the next batch.</p>
  <p>Confidence weighting. Define a confidence score s_i = 1 – H_i/log C, which maps uniform predictions to 0 and one-hot predictions to 1. The proposed weighted loss is L_w = Σ_i s_i H_i / Σ_i s_i. This weighting attenuates gradients from highly uncertain samples, ideally yielding a cleaner update direction.</p>
  <p>BatchNorm adaptation dynamics. Updating only γ and β has the advantage of maintaining the learned feature extractor while allowing per-channel scaling and shifting compatible with the target statistics. However, the optimisation landscape is shallow; gradients must be sufficiently strong to move the parameters. If most s_i are near zero, as when predictions are almost uniform, the weighted loss and its gradient collapse, preventing learning.</p>
  <p>Assumptions. We assume online streaming, no target labels, no access to source data, and default BatchNorm behaviour (training mode for adaptation, evaluation mode for inference). We do not employ batch renormalisation or class-frequency correction, isolating the effect of confidence weighting.</p>
</section>

<section>
  <h2>Method</h2>
  <pre><code># CW-TENT (Confidence-Weighted TENT)
# Updates only BatchNorm affine parameters (γ, β) per incoming batch

for each mini-batch x in target stream:
    # 1) Enable training mode for BN to collect current stats
    model.train_bn_only()

    # 2) Forward pass: logits and softmax probabilities
    logits = model(x)
    p = softmax(logits)

    # 3) Compute entropy H_i and confidence weights s_i = 1 - H_i / log(C)
    H = -sum_over_classes(p * log(p))
    s = 1.0 - H / log(C)

    # 4) Normalised weighted entropy loss
    L_w = sum(s * H) / sum(s)

    # 5) Single SGD update on (γ, β)
    L_w.backward()
    sgd_step_bn_affines(lr=1e-3, momentum=0.9)

    # 6) Switch back to evaluation mode and emit predictions
    model.eval_bn_for_inference()
    y_hat = argmax(p)
  </code></pre>
  <p>Rationale. Early confident samples are expected to lie closer to the target optimum and to point roughly in the same gradient direction. Emphasising them should accelerate convergence and potentially allow a single update step. Normalisation by Σ s_i keeps the learning-rate-to-loss scale stable when the weight sum varies.</p>
  <p>Practical variants. If s_i collapses to zero, gradients vanish. Variants include temperature scaling of logits before computing entropy, clipping s_i to a minimum value, or using a small constant in the denominator. Our study deliberately omits such safeguards to test the minimal idea.</p>
  <p>Relation to prior work. CW-TENT inherits the architectural and objective design of TENT but differs in loss weighting. Unlike TBR+DOT in DELTA, it does not modify BatchNorm statistics or class bias. Compared with regularised losses <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>, it introduces no extra terms, only re-scaling existing ones.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>Dataset and corruption. CIFAR-10-C applies fifteen corruption types to CIFAR-10 images. We use severity 5, the most challenging setting, and stream the corrupted test set in mini-batches.</p>
  <p>Model and adapters. The source backbone is ResNet-18 (11.7 M parameters). We evaluate: (1) Source (no adaptation); (2) TENT, ten gradient steps per batch; (3) CW-TENT, one step per batch. All adapters update only BatchNorm affine parameters.</p>
  <p>Optimiser and hyper-parameters. Both adaptive methods use SGD with learning rate 1e-3. TENT follows its recommended hyper-parameters; CW-TENT adds momentum 0.9. No temperature scaling or weight clipping is applied.</p>
  <p>Metrics. The principal metric is top-1 accuracy accumulated over the entire stream. To probe convergence, we plot per-batch accuracy, compute distributions, and test statistical significance with a two-sided Wilcoxon signed-rank test on paired batch accuracies.</p>
  <p>Implementation details. Our PyTorch implementation adds four lines to the open-source TENT code to compute s_i and L_w. Experiments run on one NVIDIA A100 GPU; hyper-parameter sweeps, when required, can be parallelised across eight GPUs but are not used in the main study.</p>
  <p>Experimental runs. We report two independent runs: proposed-ResNet-18-… (CW-TENT) and comparative-1-ResNet-18-… (TENT). Each run logs predictions, losses, parameter traces, and auxiliary figures.</p>
</section>

<section>
  <h2>Results</h2>
  <p>Overall performance. After processing the full CIFAR-10-C stream, CW-TENT attains 10.11 % accuracy—indistinguishable from random guessing—whereas TENT achieves 39.44 %. The 29.3-percentage-point gap is confirmed significant (p &lt; 0.01, Wilcoxon).</p>
  <p>Convergence behaviour. Learning curves (Figure 2) show CW-TENT flat at chance throughout, while TENT improves steadily from 34 % to 39 %. Batch accuracy distributions (Figure 5) illustrate the same pattern: CW-TENT concentrates near zero information, TENT exhibits a long tail of high-accuracy batches.</p>
  <p>Error structure. The CW-TENT confusion matrix (Figure 1) reveals bias toward a few classes, with almost no corrective movement across time. The inferred cause is weight collapse: under heavy corruption, p_i is nearly uniform, H_i ≈ log C, and s_i ≈ 0. Consequently, Σ s_i is tiny, gradients vanish, and parameters freeze.</p>
  <p>Fairness and hyper-parameter notes. The step budget differs by design: CW-TENT uses one step, TENT uses ten. Nevertheless, the complete lack of adaptation suggests the weighting strategy itself fails under severe uncertainty. Tuning learning rate or adding momentum does not recover performance.</p>
  <p>Limitations and ablations. The study evaluates a minimal configuration and does not sweep temperature parameters or multiple inner steps for CW-TENT. Such ablations are left to future work but are unlikely to close a 29-point gap without altering the core idea.</p>

  <figure>
    <img src="images/confusion_matrix.png" alt="Confusion matrix" style="width:70%">
    <figcaption>Figure 1: Confusion matrix of CW-TENT predictions (filename: confusion_matrix.pdf). Higher diagonal counts indicate better performance.</figcaption>
  </figure>

  <figure>
    <img src="images/learning_curve.png" alt="Learning curves" style="width:70%">
    <figcaption>Figure 2: Accuracy learning curves for Source, TENT, and CW-TENT (filename: learning_curve.pdf). Higher values are better.</figcaption>
  </figure>

  <figure>
    <img src="images/batch_acc_distribution.png" alt="Batch accuracy distribution" style="width:70%">
    <figcaption>Figure 5: Distribution of batch-wise accuracies (filename: batch_acc_distribution.pdf). More mass at higher accuracies is better.</figcaption>
  </figure>

  <figure>
    <img src="images/final_accuracy_comparison.png" alt="Final accuracy comparison" style="width:70%">
    <figcaption>Figure 6: Final accuracy bar chart comparing methods (filename: final_accuracy_comparison.pdf). Taller bars are better.</figcaption>
  </figure>

  <p>Data files:</p>
  <ul>
    <li><a href="images/metrics.json" target="_blank">metrics.json</a> — Serialized per-batch and aggregate metrics (Figure 3).</li>
    <li><a href="images/aggregated_metrics.json" target="_blank">aggregated_metrics.json</a> — Aggregated final accuracies across runs (Figure 4).</li>
    <li><a href="images/significance_tests.json" target="_blank">significance_tests.json</a> — p-value report from significance testing (Figure 7).</li>
  </ul>
  <p>The collective evidence demonstrates that CW-TENT, as currently formulated, is ineffective for severe corruptions.</p>
</section>

<section>
  <h2>Conclusion</h2>
  <p>We set out to accelerate entropy-based test-time adaptation by down-weighting uncertain samples. CW-TENT introduces a single-line per-sample confidence weight yet keeps the architecture, objective, and optimiser otherwise identical to TENT. On CIFAR-10-C, this minimal change proved insufficient: with one gradient step per batch, CW-TENT failed to improve over chance, whereas a ten-step TENT baseline restored nearly 40 % accuracy. Diagnostic plots attribute the failure to weight collapse and gradient starvation under severe uncertainty.</p>
  <p>The study contributes a clear negative result and a fine-grained analysis of why naïve confidence weighting collapses. To revive the idea, future research should: (1) calibrate probabilities or apply temperature scaling before computing weights; (2) clip or re-normalise weights to preserve gradient magnitude; (3) combine confidence weighting with robust batch-statistic estimation such as Batch Renormalisation and dynamic class re-weighting <a href="https://arxiv.org/pdf/2301.13018v1.pdf" target="_blank" title="DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION">(Bowen Zhao, 2023)</a>; (4) investigate adaptive multi-step schedules and regularised losses proven effective in other domains <a href="#ref-author-year-test" target="_blank" title="Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection">(author-year-test)</a>. Addressing these points is essential before confidence-weighted entropy minimisation can offer the promised latency benefits in real-time adaptive systems.</p>
</section>
</body>
</html>