# Confidence-Weighted Entropy Minimization for One-Step Test-Time Adaptation
> ⚠️ **NOTE:** This research is an automatic research using AIRAS.
## Abstract
Test-time adaptation (TTA) updates a model on the incoming data stream to counteract distribution shift without requiring labels. Entropy-minimization methods such as TENT restrict adaptation to the affine parameters of Batch Normalization and achieve good accuracy, yet they typically run three to ten gradient steps per mini-batch, inflating latency. We revisit the core obstacle—noisy gradients dominated by high-entropy samples early in adaptation—and propose Confidence-Weighted TENT (CW-TENT). CW-TENT rescales each sample’s entropy by the confidence weight w = 1 – H(p)/log C and minimizes the normalized weighted loss with a single stochastic-gradient step per batch, aiming to preserve accuracy while cutting computation. We implement CW-TENT for a pre-trained ResNet-18 and evaluate it on CIFAR-10-C at corruption severity 5 against the 10-step TENT baseline. Contrary to expectation, the logged run shows severe degradation: CW-TENT reaches only 10.11 percent final top-1 accuracy versus 37.41 percent for TENT, although it reduces back-propagation steps by a factor of ten. Learning-curve analysis attributes the collapse to weight saturation, unstable Batch-Norm statistics, and an oversized learning rate. We dissect these failure modes and outline concrete stabilisation strategies—temperature-scaled weights, adaptive step sizes, and hybrid updates—thereby providing a data-backed cautionary tale for confidence-aware objectives in fast TTA.

- [Research history](https://github.com/auto-res2/airas-20251020-120659-matsuzawa/blob/main-retry-5/.research/research_history.json)
- [GitHub Pages](https://auto-res2.github.io/airas-20251020-120659-matsuzawa/branches/main-retry-5/index.html)