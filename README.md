# Confidence-Weighted Entropy Minimization for Test-Time Adaptation: Promise and Pitfalls on CIFAR-10-C
> ⚠️ **NOTE:** This research is an automatic research using AIRAS.
## Abstract
Test-time adaptation (TTA) updates a pre-trained model on an unlabeled test stream to mitigate distribution shift. The dominant approach, TENT, adapts only BatchNorm affine parameters by minimizing prediction entropy, but typically relies on three to ten inner gradient steps per incoming batch, which inflates inference latency. We hypothesize that slow convergence stems from noisy gradients produced by high-entropy, low-confidence samples that dominate early in adaptation. We therefore propose Confidence-Weighted TENT (CW-TENT). CW-TENT keeps the original entropy objective but assigns each sample a weight w = 1 – H(p)/log C, down-weighting uncertain predictions and computing a normalized weighted loss L_w = Σ w·H / Σ w. The expected benefit is a cleaner gradient that permits a single update step per batch. We evaluate CW-TENT on CIFAR-10-C (severity 5) with a pre-trained ResNet-18 and compare it to a static source model and a ten-step TENT baseline. Contrary to the hypothesis, CW-TENT remains at chance-level accuracy (10.1 %), whereas TENT reaches 39.4 %; the gap is statistically significant (p < 0.01). Analysis shows that under severe corruption, predictions are nearly uniform, weights collapse toward zero, and gradients vanish. We discuss why naïve confidence weighting fails in this regime and outline concrete remedies, providing a cautionary tale for uncertainty-aware TTA.

- [Research history](https://github.com/auto-res2/airas-20251020-120659-matsuzawa/blob/main-retry-4/.research/research_history.json)
- [GitHub Pages](https://auto-res2.github.io/airas-20251020-120659-matsuzawa/branches/main-retry-4/index.html)