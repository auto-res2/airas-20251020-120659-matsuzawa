# Confidence-Weighted Entropy Minimization for Test-Time Adaptation: A Diagnostic Study
> ⚠️ **NOTE:** This research is an automatic research using AIRAS.
## Abstract
We ask whether a trivial confidence-based reweighting of the widely used entropy minimisation objective can accelerate and stabilise fully test-time adaptation (TTA) of deep image classifiers. TTA updates only a subset of parameters—typically the affine terms of Batch Normalisation layers—while labels are unavailable and data arrive as a stream under distribution shift. Vanilla entropy minimisation (TENT) delivers large gains but usually needs several inner optimisation steps per batch, incurring latency and energy costs. We propose Confidence-Weighted TENT (CW-TENT), which keeps the original objective yet multiplies each sample’s entropy by w = 1 − H(p)/log C, thereby emphasising low-entropy, presumably reliable predictions, and performs a single stochastic-gradient step per batch. On CIFAR-10-C corruption severity 5 with a pre-trained ResNet-18 we compare CW-TENT to an established baseline. Contrary to our hypothesis CW-TENT collapses to 10.1 % top-1 accuracy—random chance for ten classes—while the baseline attains 40.8 %. A paired two-tailed test over per-batch accuracies yields p < 10⁻⁶. Diagnostics show that early in adaptation most predictions are nearly uniform, weights vanish, the loss normaliser shrinks, and gradients explode, destroying the model. We analyse this mechanism and sketch practical safeguards such as weight flooring, warm-up without weighting, and modest multi-step updates. Our negative result highlights previously unreported interactions between confidence weighting and Batch Normalisation in online TTA and provides artefacts to facilitate future improvements.

- [Research history](https://github.com/auto-res2/airas-20251020-120659-matsuzawa/blob/main-retry-5/.research/research_history.json)
- [GitHub Pages](https://auto-res2.github.io/airas-20251020-120659-matsuzawa/branches/main-retry-5/index.html)