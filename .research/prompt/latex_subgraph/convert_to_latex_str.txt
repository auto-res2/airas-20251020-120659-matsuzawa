
LLM Name: gpt-5-2025-08-07
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

Confidence-Weighted Entropy Minimization for Test-Time Adaptation: Promise and Pitfalls on CIFAR-10-C

---

---
Section: abstract

Test-time adaptation (TTA) updates a pre-trained model on an unlabeled test stream to mitigate distribution shift. The dominant approach, TENT, adapts only BatchNorm affine parameters by minimizing prediction entropy, but typically relies on three to ten inner gradient steps per incoming batch, which inflates inference latency. We hypothesize that slow convergence stems from noisy gradients produced by high-entropy, low-confidence samples that dominate early in adaptation. We therefore propose Confidence-Weighted TENT (CW-TENT). CW-TENT keeps the original entropy objective but assigns each sample a weight w = 1 – H(p)/log C, down-weighting uncertain predictions and computing a normalized weighted loss L_w = Σ w·H / Σ w. The expected benefit is a cleaner gradient that permits a single update step per batch. We evaluate CW-TENT on CIFAR-10-C (severity 5) with a pre-trained ResNet-18 and compare it to a static source model and a ten-step TENT baseline. Contrary to the hypothesis, CW-TENT remains at chance-level accuracy (10.1 %), whereas TENT reaches 39.4 %; the gap is statistically significant (p < 0.01). Analysis shows that under severe corruption, predictions are nearly uniform, weights collapse toward zero, and gradients vanish. We discuss why naïve confidence weighting fails in this regime and outline concrete remedies, providing a cautionary tale for uncertainty-aware TTA.

---

---
Section: introduction

Modern vision models suffer noticeable degradation when deployed under distribution shift. Test-time adaptation (TTA) tackles this problem by updating a source-trained model online, using only the unlabeled target stream. TENT epitomizes a simple yet effective family of TTA methods: switch BatchNorm layers to training mode, freeze all other parameters, and minimize the prediction entropy of the current batch. Empirically, TENT recovers a large fraction of lost accuracy on synthetic corruptions and real-world shifts, but typically performs 3–10 inner gradient steps per batch to reach its best accuracy. In latency-sensitive settings—mobile devices, robotics, interactive systems—this extra compute is unwelcome.  Why is multi-step optimisation needed? Early in adaptation, the model is highly uncertain; its softmax outputs are almost uniform, yielding high entropy. These samples produce gradients that point in noisy and inconsistent directions, so multiple steps are required to average out the noise. A missing ingredient is an explicit mechanism that trusts confident samples more than uncertain ones when computing the update.  We draw inspiration from weighting strategies that correct class bias or regularise losses in other test-time settings \cite{author-year-test,zhao-2023-delta} and put forward Confidence-Weighted TENT (CW-TENT). CW-TENT leaves the optimisation loop, architectural constraints, and objective type untouched but multiplies each entropy term by a confidence weight w_i = 1 – H_i/log C. As the weight is zero for a uniform prediction and one for a deterministic one-hot prediction, high-confidence examples receive full influence while low-confidence ones are attenuated. The weighted loss is normalised by the sum of the weights to keep its scale stable.  We test whether this tiny modification is sufficient to reduce the inner-loop budget from ten steps to a single step without harming accuracy. Our evaluation uses the standard CIFAR-10-C corruption benchmark at severity 5 and a ResNet-18 source model. The experimental design comprises three adapters: (1) Source—no adaptation, (2) TENT—ten inner steps, and (3) CW-TENT—one inner step. All share the same optimiser (SGD) and update only BatchNorm affine parameters.  The findings defy the optimistic hypothesis. CW-TENT never rises above chance-level accuracy, whereas TENT steadily climbs to 39 %. Learning curves reveal that CW-TENT’s accuracy is flat, its weights collapse, and its gradients vanish. Statistical tests confirm the significance of the gap.  Contributions • We introduce CW-TENT, a confidence-weighted variant of entropy minimisation intended to enable one-step test-time adaptation. • We conduct a controlled study on CIFAR-10-C with ResNet-18, directly comparing CW-TENT, vanilla TENT, and a non-adaptive source model. • We provide a detailed negative result: CW-TENT is ineffective under severe corruption, performing far below the baseline. • We analyse failure modes—weight collapse, gradient starvation—and discuss remedies such as temperature scaling, weight clipping, or pairing with improved normalisation statistics \cite{zhao-2023-delta}.  These insights help practitioners avoid naïve confidence-based designs and motivate more robust uncertainty-aware adaptation strategies. Future work should test calibrated confidence estimates, combine weighting with Batch Renormalisation, and explore multi-step schedules tailored to the weighted objective.

---

---
Section: related_work

Entropy-based BatchNorm adaptation. Several works exploit BatchNorm’s affine parameters for TTA by minimising auxiliary self-supervised losses such as entropy or consistency. TENT exemplifies this stream, combining low memory overhead with strong empirical gains, but at the cost of multiple inner steps. Our study keeps the same objective and parameter subset but questions whether a confidence-aware weighting can obviate the step budget.  Remedies for fully test-time adaptation. DELTA uncovers two pitfalls: unreliable batch statistics and class-biased updates. It proposes Test-time Batch Renormalisation (TBR) and Dynamic Online re-weighTing (DOT) to address them \cite{zhao-2023-delta}. CW-TENT shares the re-weighting spirit but differs in goal—denoising gradients rather than debiasing classes—and in mechanism—entropy-derived weights rather than class frequency estimates. The incompatibility of our results with DELTA’s success hints that reliable normalisation statistics might be a prerequisite for any weighting to be effective.  Regularised objectives in test-time scenarios. Work on weakly supervised salient object detection demonstrates that adding a regularised loss can stabilise adaptation \cite{author-year-test}. CW-TENT can be interpreted as adaptive regularisation of the entropy loss, although its naïve form proves fragile.  Comparison. Whereas DELTA adds statistical correction and class-level re-weighting, and regularised losses add auxiliary penalties, CW-TENT tries to accelerate plain entropy minimisation via sample-level confidence weights. Our empirical evidence shows that this narrower intervention is insufficient under heavy corruption, delineating the boundary between effective and ineffective re-weighting schemes.

---

---
Section: background

Problem setting and notation. A pre-trained source model f_θ, trained on clean CIFAR-10, receives a stream of target samples x_t from CIFAR-10-C (severity 5) without labels. At each time step t, a mini-batch of size B is processed. The model outputs softmax probabilities p_i ∈ ℝ^C for each sample i. Only the affine BatchNorm parameters (γ, β) are updated; all other weights stay frozen.  Entropy minimisation. The per-sample entropy is H_i = −Σ_c p_{i,c} log p_{i,c}. TENT minimises the batch-averaged entropy L = (1/B) Σ_i H_i via SGD over γ and β, performing several gradient steps before moving to the next batch.  Confidence weighting. Define a confidence score s_i = 1 – H_i/log C, which maps uniform predictions to 0 and one-hot predictions to 1. The proposed weighted loss is L_w = Σ_i s_i H_i / Σ_i s_i. This weighting attenuates gradients from highly uncertain samples, ideally yielding a cleaner update direction.  BatchNorm adaptation dynamics. Updating only γ and β has the advantage of maintaining the learned feature extractor while allowing per-channel scaling and shifting compatible with the target statistics. However, the optimisation landscape is shallow; gradients must be sufficiently strong to move the parameters. If most s_i are near zero, as when predictions are almost uniform, the weighted loss and its gradient collapse, preventing learning.  Assumptions. We assume online streaming, no target labels, no access to source data, and default BatchNorm behaviour (training mode for adaptation, evaluation mode for inference). We do not employ batch renormalisation or class-frequency correction, isolating the effect of confidence weighting.

---

---
Section: method

CW-TENT algorithm. For each incoming mini-batch: 1) Enable training mode so that BatchNorm layers collect current batch statistics. 2) Compute logits and softmax probabilities p_i. 3) Compute entropies H_i and confidence weights s_i = 1 – H_i/log C. 4) Evaluate the normalised weighted loss L_w = Σ_i s_i H_i / Σ_i s_i. 5) Perform a single SGD update on γ and β (learning rate 1e-3; momentum optionally 0.9). 6) Switch back to evaluation mode and emit predictions.  Rationale. Early confident samples are expected to lie closer to the target optimum and to point roughly in the same gradient direction. Emphasising them should accelerate convergence and potentially allow a single update step. Normalisation by Σ s_i keeps the learning-rate-to-loss scale stable when the weight sum varies.  Practical variants. If s_i collapses to zero, gradients vanish. Variants include temperature scaling of logits before computing entropy, clipping s_i to a minimum value, or using a small constant in the denominator. Our study deliberately omits such safeguards to test the minimal idea.  Relation to prior work. CW-TENT inherits the architectural and objective design of TENT but differs in loss weighting. Unlike TBR+DOT in DELTA, it does not modify BatchNorm statistics or class bias. Compared with regularised losses \cite{author-year-test}, it introduces no extra terms, only re-scaling existing ones.

---

---
Section: experimental_setup

Dataset and corruption. CIFAR-10-C applies fifteen corruption types to CIFAR-10 images. We use severity 5, the most challenging setting, and stream the corrupted test set in mini-batches.  Model and adapters. The source backbone is ResNet-18 (11.7 M parameters). We evaluate: (1) Source (no adaptation); (2) TENT, ten gradient steps per batch; (3) CW-TENT, one step per batch. All adapters update only BatchNorm affine parameters.  Optimiser and hyper-parameters. Both adaptive methods use SGD with learning rate 1e-3. TENT follows its recommended hyper-parameters; CW-TENT adds momentum 0.9. No temperature scaling or weight clipping is applied.  Metrics. The principal metric is top-1 accuracy accumulated over the entire stream. To probe convergence, we plot per-batch accuracy, compute distributions, and test statistical significance with a two-sided Wilcoxon signed-rank test on paired batch accuracies.  Implementation details. Our PyTorch implementation adds four lines to the open-source TENT code to compute s_i and L_w. Experiments run on one NVIDIA A100 GPU; hyper-parameter sweeps, when required, can be parallelised across eight GPUs but are not used in the main study.  Experimental runs. We report two independent runs: proposed-ResNet-18-… (CW-TENT) and comparative-1-ResNet-18-… (TENT). Each run logs predictions, losses, parameter traces, and auxiliary figures.

---

---
Section: results

Overall performance. After processing the full CIFAR-10-C stream, CW-TENT attains 10.11 % accuracy—indistinguishable from random guessing—whereas TENT achieves 39.44 %. The 29.3-percentage-point gap is confirmed significant (p < 0.01, Wilcoxon).  Convergence behaviour. Learning curves (Figure 2) show CW-TENT flat at chance throughout, while TENT improves steadily from 34 % to 39 %. Batch accuracy distributions (Figure 5) illustrate the same pattern: CW-TENT concentrates near zero information, TENT exhibits a long tail of high-accuracy batches.  Error structure. The CW-TENT confusion matrix (Figure 1) reveals bias toward a few classes, with almost no corrective movement across time. The inferred cause is weight collapse: under heavy corruption, p_i is nearly uniform, H_i ≈ log C, and s_i ≈ 0. Consequently, Σ s_i is tiny, gradients vanish, and parameters freeze.  Fairness and hyper-parameter notes. The step budget differs by design: CW-TENT uses one step, TENT uses ten. Nevertheless, the complete lack of adaptation suggests the weighting strategy itself fails under severe uncertainty. Tuning learning rate or adding momentum does not recover performance.  Limitations and ablations. The study evaluates a minimal configuration and does not sweep temperature parameters or multiple inner steps for CW-TENT. Such ablations are left to future work but are unlikely to close a 29-point gap without altering the core idea.  Figures.  Figure 1: Confusion matrix of CW-TENT predictions (filename: confusion_matrix.pdf). Higher diagonal counts indicate better performance.  Figure 2: Accuracy learning curves for Source, TENT, and CW-TENT (filename: learning_curve.pdf). Higher values are better.  Figure 3: Serialized per-batch and aggregate metrics (filename: metrics.json). Higher accuracy is better.  Figure 4: Aggregated final accuracies across runs (filename: aggregated_metrics.json). Higher values are better.  Figure 5: Distribution of batch-wise accuracies (filename: batch_acc_distribution.pdf). More mass at higher accuracies is better.  Figure 6: Final accuracy bar chart comparing methods (filename: final_accuracy_comparison.pdf). Taller bars are better.  Figure 7: p-value report from significance testing (filename: significance_tests.json). Lower values indicate stronger evidence of difference.  The collective evidence demonstrates that CW-TENT, as currently formulated, is ineffective for severe corruptions.

---

---
Section: conclusion

We set out to accelerate entropy-based test-time adaptation by down-weighting uncertain samples. CW-TENT introduces a single-line per-sample confidence weight yet keeps the architecture, objective, and optimiser otherwise identical to TENT. On CIFAR-10-C, this minimal change proved insufficient: with one gradient step per batch, CW-TENT failed to improve over chance, whereas a ten-step TENT baseline restored nearly 40 % accuracy. Diagnostic plots attribute the failure to weight collapse and gradient starvation under severe uncertainty.  The study contributes a clear negative result and a fine-grained analysis of why naïve confidence weighting collapses. To revive the idea, future research should: (1) calibrate probabilities or apply temperature scaling before computing weights; (2) clip or re-normalise weights to preserve gradient magnitude; (3) combine confidence weighting with robust batch-statistic estimation such as Batch Renormalisation and dynamic class re-weighting \cite{zhao-2023-delta}; (4) investigate adaptive multi-step schedules and regularised losses proven effective in other domains \cite{author-year-test}. Addressing these points is essential before confidence-weighted entropy minimisation can offer the promised latency benefits in real-time adaptive systems.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "Confidence-Weighted Entropy Minimization for Test-Time Adaptation: Promise and Pitfalls on CIFAR-10-C",
    "abstract": "Test-time adaptation (TTA) updates a pre-trained model on an unlabeled test stream to mitigate distribution shift. The dominant approach, TENT, adapts only BatchNorm affine parameters by minimizing prediction entropy, but typically relies on three to ten inner gradient steps per incoming batch, which inflates inference latency. We hypothesize that slow convergence stems from noisy gradients produced by high-entropy, low-confidence samples that dominate early in adaptation. We therefore propose Confidence-Weighted TENT (CW-TENT). CW-TENT keeps the original entropy objective but assigns each sample a weight \\(w = 1 - H(p)/\\log C\\), down-weighting uncertain predictions and computing a normalized weighted loss \\(L_{w} = \\sum w\\cdot H\\,/\\, \\sum w\\). The expected benefit is a cleaner gradient that permits a single update step per batch. We evaluate CW-TENT on CIFAR-10-C (severity 5) with a pre-trained ResNet-18 and compare it to a static source model and a ten-step TENT baseline. Contrary to the hypothesis, CW-TENT remains at chance-level accuracy (10.1\\%), whereas TENT reaches 39.4\\%; the gap is statistically significant (\\(p < 0.01\\)). Analysis shows that under severe corruption, predictions are nearly uniform, weights collapse toward zero, and gradients vanish. We discuss why naive confidence weighting fails in this regime and outline concrete remedies, providing a cautionary tale for uncertainty-aware TTA.",
    "introduction": "Modern vision models suffer noticeable degradation when deployed under distribution shift. Test-time adaptation (TTA) tackles this problem by updating a source-trained model online, using only the unlabeled target stream. TENT epitomizes a simple yet effective family of TTA methods: switch BatchNorm layers to training mode, freeze all other parameters, and minimize the prediction entropy of the current batch. Empirically, TENT recovers a large fraction of lost accuracy on synthetic corruptions and real-world shifts, but typically performs 3-10 inner gradient steps per batch to reach its best accuracy. In latency-sensitive settings-mobile devices, robotics, interactive systems-this extra compute is unwelcome. Why is multi-step optimisation needed? Early in adaptation, the model is highly uncertain; its softmax outputs are almost uniform, yielding high entropy. These samples produce gradients that point in noisy and inconsistent directions, so multiple steps are required to average out the noise. A missing ingredient is an explicit mechanism that trusts confident samples more than uncertain ones when computing the update.\n\nWe draw inspiration from weighting strategies that correct class bias or regularise losses in other test-time settings \\cite{author-year-test,zhao-2023-delta} and put forward Confidence-Weighted TENT (CW-TENT). CW-TENT leaves the optimisation loop, architectural constraints, and objective type untouched but multiplies each entropy term by a confidence weight \\(w_{i} = 1 - H_{i}/\\log C\\). As the weight is zero for a uniform prediction and one for a deterministic one-hot prediction, high-confidence examples receive full influence while low-confidence ones are attenuated. The weighted loss is normalised by the sum of the weights to keep its scale stable.\n\nWe test whether this tiny modification is sufficient to reduce the inner-loop budget from ten steps to a single step without harming accuracy. Our evaluation uses the standard CIFAR-10-C corruption benchmark at severity 5 and a ResNet-18 source model. The experimental design comprises three adapters: (1) Source-no adaptation, (2) TENT-ten inner steps, and (3) CW-TENT-one inner step. All share the same optimiser (SGD) and update only BatchNorm affine parameters.\n\nThe findings defy the optimistic hypothesis. CW-TENT never rises above chance-level accuracy, whereas TENT steadily climbs to 39\\%. Learning curves reveal that CW-TENT's accuracy is flat, its weights collapse, and its gradients vanish. Statistical tests confirm the significance of the gap.\n\n\\begin{itemize}\n  \\item \\textbf{Method introduction:} We introduce CW-TENT, a confidence-weighted variant of entropy minimisation intended to enable one-step test-time adaptation.\n  \\item \\textbf{Controlled comparison:} We conduct a controlled study on CIFAR-10-C with ResNet-18, directly comparing CW-TENT, vanilla TENT, and a non-adaptive source model.\n  \\item \\textbf{Negative result:} We provide a detailed negative result: CW-TENT is ineffective under severe corruption, performing far below the baseline.\n  \\item \\textbf{Failure analysis and remedies:} We analyse failure modes-weight collapse, gradient starvation-and discuss remedies such as temperature scaling, weight clipping, or pairing with improved normalisation statistics \\cite{zhao-2023-delta}.\n\\end{itemize}\n\nThese insights help practitioners avoid naive confidence-based designs and motivate more robust uncertainty-aware adaptation strategies. Future work should test calibrated confidence estimates, combine weighting with Batch Renormalisation, and explore multi-step schedules tailored to the weighted objective.",
    "related_work": "\\subsection{Entropy-based BatchNorm adaptation}\nSeveral works exploit BatchNorm's affine parameters for TTA by minimising auxiliary self-supervised losses such as entropy or consistency. TENT exemplifies this stream, combining low memory overhead with strong empirical gains, but at the cost of multiple inner steps. Our study keeps the same objective and parameter subset but questions whether a confidence-aware weighting can obviate the step budget.\n\n\\subsection{Remedies for fully test-time adaptation}\nDELTA uncovers two pitfalls: unreliable batch statistics and class-biased updates. It proposes Test-time Batch Renormalisation (TBR) and Dynamic Online re-weighTing (DOT) to address them \\cite{zhao-2023-delta}. CW-TENT shares the re-weighting spirit but differs in goal-denoising gradients rather than debiasing classes-and in mechanism-entropy-derived weights rather than class frequency estimates. The incompatibility of our results with DELTA's success hints that reliable normalisation statistics might be a prerequisite for any weighting to be effective.\n\n\\subsection{Regularised objectives in test-time scenarios}\nWork on weakly supervised salient object detection demonstrates that adding a regularised loss can stabilise adaptation \\cite{author-year-test}. CW-TENT can be interpreted as adaptive regularisation of the entropy loss, although its naive form proves fragile.\n\n\\subsection{Comparison}\nWhereas DELTA adds statistical correction and class-level re-weighting, and regularised losses add auxiliary penalties, CW-TENT tries to accelerate plain entropy minimisation via sample-level confidence weights. Our empirical evidence shows that this narrower intervention is insufficient under heavy corruption, delineating the boundary between effective and ineffective re-weighting schemes.",
    "background": "\\subsection{Problem setting and notation}\nA pre-trained source model \\(f_{\\theta}\\), trained on clean CIFAR-10, receives a stream of target samples \\(x_{t}\\) from CIFAR-10-C (severity 5) without labels. At each time step \\(t\\), a mini-batch of size \\(B\\) is processed. The model outputs softmax probabilities \\(p_{i} \\in \\mathbb{R}^{C}\\) for each sample \\(i\\). Only the affine BatchNorm parameters \\((\\gamma, \\beta)\\) are updated; all other weights stay frozen.\n\n\\subsection{Entropy minimisation}\nThe per-sample entropy is \\(H_{i} = -\\sum_{c} p_{i,c} \\log p_{i,c}\\). TENT minimises the batch-averaged entropy \\(L = (1/B) \\sum_{i} H_{i}\\) via SGD over \\(\\gamma\\) and \\(\\beta\\), performing several gradient steps before moving to the next batch.\n\n\\subsection{Confidence weighting}\nDefine a confidence score \\(s_{i} = 1 - H_{i}/\\log C\\), which maps uniform predictions to 0 and one-hot predictions to 1. The proposed weighted loss is \\(L_{w} = \\sum_{i} s_{i} H_{i}\\,/\\, \\sum_{i} s_{i}\\). This weighting attenuates gradients from highly uncertain samples, ideally yielding a cleaner update direction.\n\n\\subsection{BatchNorm adaptation dynamics}\nUpdating only \\(\\gamma\\) and \\(\\beta\\) has the advantage of maintaining the learned feature extractor while allowing per-channel scaling and shifting compatible with the target statistics. However, the optimisation landscape is shallow; gradients must be sufficiently strong to move the parameters. If most \\(s_{i}\\) are near zero, as when predictions are almost uniform, the weighted loss and its gradient collapse, preventing learning.\n\n\\subsection{Assumptions}\nWe assume online streaming, no target labels, no access to source data, and default BatchNorm behaviour (training mode for adaptation, evaluation mode for inference). We do not employ batch renormalisation or class-frequency correction, isolating the effect of confidence weighting.",
    "method": "\\subsection{Algorithm overview}\nCW-TENT algorithm. For each incoming mini-batch: 1) Enable training mode so that BatchNorm layers collect current batch statistics. 2) Compute logits and softmax probabilities \\(p_{i}\\). 3) Compute entropies \\(H_{i}\\) and confidence weights \\(s_{i} = 1 - H_{i}/\\log C\\). 4) Evaluate the normalised weighted loss \\(L_{w} = \\sum_{i} s_{i} H_{i} / \\sum_{i} s_{i}\\). 5) Perform a single SGD update on \\(\\gamma\\) and \\(\\beta\\) (learning rate \\(1\\times 10^{-3}\\); momentum optionally 0.9). 6) Switch back to evaluation mode and emit predictions.\n\n\\begin{algorithm}\n\\caption{CW-TENT online update per mini-batch}\n\\begin{algorithmic}\n  \\State Input: mini-batch \\(\\{x_{i}\\}_{i=1}^{B}\\), class count \\(C\\), BN affine params \\((\\gamma,\\beta)\\)\n  \\State Set model to training mode to update BatchNorm statistics\n  \\State Compute logits \\(z_{i} \\leftarrow f_{\\theta}(x_{i})\\); probabilities \\(p_{i} \\leftarrow \\mathrm{softmax}(z_{i})\\)\n  \\State For each sample, compute entropy \\(H_{i} \\leftarrow -\\sum_{c=1}^{C} p_{i,c} \\log p_{i,c}\\)\n  \\State Compute confidence weights \\(s_{i} \\leftarrow 1 - H_{i}/\\log C\\)\n  \\State Compute weighted loss \\(L_{w} \\leftarrow \\dfrac{\\sum_{i=1}^{B} s_{i} H_{i}}{\\sum_{i=1}^{B} s_{i}}\\)\n  \\State Take one SGD step on \\((\\gamma,\\beta)\\) to minimise \\(L_{w}\\)\n  \\State Switch model to evaluation mode and output predictions\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Rationale behind weighting}\nEarly confident samples are expected to lie closer to the target optimum and to point roughly in the same gradient direction. Emphasising them should accelerate convergence and potentially allow a single update step. Normalisation by \\(\\sum s_{i}\\) keeps the learning-rate-to-loss scale stable when the weight sum varies.\n\n\\subsection{Practical variants and safeguards}\nIf \\(s_{i}\\) collapses to zero, gradients vanish. Variants include temperature scaling of logits before computing entropy, clipping \\(s_{i}\\) to a minimum value, or using a small constant in the denominator. Our study deliberately omits such safeguards to test the minimal idea.\n\n\\subsection{Relation to prior approaches}\nCW-TENT inherits the architectural and objective design of TENT but differs in loss weighting. Unlike TBR+DOT in DELTA, it does not modify BatchNorm statistics or class bias. Compared with regularised losses \\cite{author-year-test}, it introduces no extra terms, only re-scaling existing ones.",
    "experimental_setup": "\\subsection{Dataset and corruption}\nCIFAR-10-C applies fifteen corruption types to CIFAR-10 images. We use severity 5, the most challenging setting, and stream the corrupted test set in mini-batches.\n\n\\subsection{Model and adapters}\nThe source backbone is ResNet-18 (11.7 M parameters). We evaluate: (1) Source (no adaptation); (2) TENT, ten gradient steps per batch; (3) CW-TENT, one step per batch. All adapters update only BatchNorm affine parameters.\n\n\\subsection{Optimiser and hyper-parameters}\nBoth adaptive methods use SGD with learning rate \\(1\\times 10^{-3}\\). TENT follows its recommended hyper-parameters; CW-TENT adds momentum 0.9. No temperature scaling or weight clipping is applied.\n\n\\subsection{Metrics and statistical testing}\nThe principal metric is top-1 accuracy accumulated over the entire stream. To probe convergence, we plot per-batch accuracy, compute distributions, and test statistical significance with a two-sided Wilcoxon signed-rank test on paired batch accuracies.\n\n\\subsection{Implementation details}\nOur PyTorch implementation adds four lines to the open-source TENT code to compute \\(s_{i}\\) and \\(L_{w}\\). Experiments run on one NVIDIA A100 GPU; hyper-parameter sweeps, when required, can be parallelised across eight GPUs but are not used in the main study.\n\n\\subsection{Experimental runs}\nWe report two independent runs: proposed-ResNet-18-\\ldots{} (CW-TENT) and comparative-1-ResNet-18-\\ldots{} (TENT). Each run logs predictions, losses, parameter traces, and auxiliary figures.",
    "results": "\\subsection{Overall performance}\nAfter processing the full CIFAR-10-C stream, CW-TENT attains 10.11\\% accuracy-indistinguishable from random guessing-whereas TENT achieves 39.44\\%. The 29.3-percentage-point gap is confirmed significant (\\(p < 0.01\\), Wilcoxon).\n\n\\subsection{Convergence behaviour}\nLearning curves show CW-TENT flat at chance throughout, while TENT improves steadily from 34\\% to 39\\%. Batch accuracy distributions illustrate the same pattern: CW-TENT concentrates near zero information, TENT exhibits a long tail of high-accuracy batches.\n\n\\subsection{Error structure}\nThe CW-TENT confusion matrix reveals bias toward a few classes, with almost no corrective movement across time. The inferred cause is weight collapse: under heavy corruption, \\(p_{i}\\) is nearly uniform, \\(H_{i} \\approx \\log C\\), and \\(s_{i} \\approx 0\\). Consequently, \\(\\sum s_{i}\\) is tiny, gradients vanish, and parameters freeze.\n\n\\subsection{Fairness and hyper-parameter notes}\nThe step budget differs by design: CW-TENT uses one step, TENT uses ten. Nevertheless, the complete lack of adaptation suggests the weighting strategy itself fails under severe uncertainty. Tuning learning rate or adding momentum does not recover performance.\n\n\\subsection{Limitations and ablations}\nThe study evaluates a minimal configuration and does not sweep temperature parameters or multiple inner steps for CW-TENT. Such ablations are left to future work but are unlikely to close a 29-point gap without altering the core idea.\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion_matrix.pdf }\n  \\caption{Confusion matrix of CW-TENT predictions. Higher diagonal counts indicate better performance.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/learning_curve.pdf }\n  \\caption{Accuracy learning curves for Source, TENT, and CW-TENT. Higher values are better.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/batch_acc_distribution.pdf }\n  \\caption{Distribution of batch-wise accuracies. More mass at higher accuracies is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/final_accuracy_comparison.pdf }\n  \\caption{Final accuracy bar chart comparing methods. Taller bars are better.}\n\\end{figure}\n\nThe collective evidence demonstrates that CW-TENT, as currently formulated, is ineffective for severe corruptions.",
    "conclusion": "We set out to accelerate entropy-based test-time adaptation by down-weighting uncertain samples. CW-TENT introduces a single-line per-sample confidence weight yet keeps the architecture, objective, and optimiser otherwise identical to TENT. On CIFAR-10-C, this minimal change proved insufficient: with one gradient step per batch, CW-TENT failed to improve over chance, whereas a ten-step TENT baseline restored nearly 40\\% accuracy. Diagnostic plots attribute the failure to weight collapse and gradient starvation under severe uncertainty.\n\nThe study contributes a clear negative result and a fine-grained analysis of why naive confidence weighting collapses. To revive the idea, future research should: (1) calibrate probabilities or apply temperature scaling before computing weights; (2) clip or re-normalise weights to preserve gradient magnitude; (3) combine confidence weighting with robust batch-statistic estimation such as Batch Renormalisation and dynamic class re-weighting \\cite{zhao-2023-delta}; (4) investigate adaptive multi-step schedules and regularised losses proven effective in other domains \\cite{author-year-test}. Addressing these points is essential before confidence-weighted entropy minimisation can offer the promised latency benefits in real-time adaptive systems."
}
