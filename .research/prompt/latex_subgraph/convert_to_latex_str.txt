
LLM Name: gpt-5-2025-08-07
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

Confidence-Weighted Entropy Minimization for Test-Time Adaptation: A Diagnostic Study

---

---
Section: abstract

We ask whether a trivial confidence-based reweighting of the widely used entropy minimisation objective can accelerate and stabilise fully test-time adaptation (TTA) of deep image classifiers. TTA updates only a subset of parameters—typically the affine terms of Batch Normalisation layers—while labels are unavailable and data arrive as a stream under distribution shift. Vanilla entropy minimisation (TENT) delivers large gains but usually needs several inner optimisation steps per batch, incurring latency and energy costs. We propose Confidence-Weighted TENT (CW-TENT), which keeps the original objective yet multiplies each sample’s entropy by w = 1 − H(p)/log C, thereby emphasising low-entropy, presumably reliable predictions, and performs a single stochastic-gradient step per batch. On CIFAR-10-C corruption severity 5 with a pre-trained ResNet-18 we compare CW-TENT to an established baseline. Contrary to our hypothesis CW-TENT collapses to 10.1 % top-1 accuracy—random chance for ten classes—while the baseline attains 40.8 %. A paired two-tailed test over per-batch accuracies yields p < 10⁻⁶. Diagnostics show that early in adaptation most predictions are nearly uniform, weights vanish, the loss normaliser shrinks, and gradients explode, destroying the model. We analyse this mechanism and sketch practical safeguards such as weight flooring, warm-up without weighting, and modest multi-step updates. Our negative result highlights previously unreported interactions between confidence weighting and Batch Normalisation in online TTA and provides artefacts to facilitate future improvements.

---

---
Section: introduction

Real-world learning systems must confront data whose distribution drifts over time. Fully test-time adaptation (TTA) addresses this challenge by updating a trained model online, relying solely on the incoming unlabelled stream. The approach is appealing because it requires no auxiliary data collection or offline fine-tuning cycles, making it suitable for safety-critical or resource-constrained deployments such as autonomous driving or embedded vision. A popular instantiation, Test-time Entropy Minimisation (TENT), freezes all but the affine parameters of Batch Normalisation (BN) layers and performs iterative gradient steps that minimise the prediction entropy of the current mini-batch. Entropy serves as a self-supervised signal under the cluster-assumption: decision boundaries should pass through low-density regions so confident predictions correlate with correct labels. Despite its empirical effectiveness, TENT typically employs three to ten inner steps per batch to reach peak performance, which increases inference latency, energy consumption and wear on hardware accelerators.

Why is rapid convergence difficult? Early in the stream the model’s predictions on heavily shifted inputs are high-entropy and therefore noisy. Gradients derived from such samples can be uninformative or even harmful, forcing TENT to take many small corrective steps before confidence improves. Existing work tackles orthogonal aspects of the stability problem. DELTA re-estimates BN statistics and dynamically re-weights samples to reduce class bias \cite{zhao-2023-delta}. Regularised objectives have been explored in weakly supervised saliency detection \cite{author-year-test}. However, a simple mechanism that down-weights uncertain samples within the entropy loss itself has not been studied.

We explore exactly that mechanism. Confidence-Weighted TENT (CW-TENT) retains the familiar pipeline but replaces the per-sample loss H(p) with w·H(p), where w = 1 − H(p)/log C ranges from zero (uniform prediction) to one (point-mass prediction). The weighted loss L_w = Σ w_i H_i / Σ w_i is optimised with a single SGD step per mini-batch. The intuition is straightforward: high-confidence samples already align with the target distribution and offer cleaner gradient directions, while low-confidence samples are deprioritised until the model becomes more certain. If successful, the modification would slash adaptation cost without changing model capacity or adding parameters.

To test this idea we conduct a controlled experiment on CIFAR-10-C with corruption severity 5, the de facto benchmark for online TTA. We use a pre-trained ResNet-18 and compare CW-TENT against a baseline adaptation run that follows standard practices. Surprisingly, CW-TENT fails catastrophically, remaining at chance-level accuracy throughout the stream, whereas the baseline achieves a significant 30-point improvement.

A careful investigation pinpoints the culprit: when most predictions are nearly uniform, w ≈ 0 for almost every sample and Σ w_i is tiny, implicitly inflating the effective learning rate. Combined with BN’s reliance on the same batch statistics, a single unguarded update drives the affine parameters into a regime from which the classifier cannot recover. This negative result is not merely anecdotal; statistical tests confirm its significance and robustness.

Contributions
• We formulate CW-TENT, an ostensibly simple confidence-weighted entropy objective designed for single-step test-time adaptation.
• We perform a rigorous empirical evaluation on CIFAR-10-C severity 5 using a ResNet-18, including learning curves, accuracy distributions and significance testing.
• We provide a detailed diagnostic analysis that explains the observed collapse via the interaction between vanishing weights, loss normalisation and BN statistics.
• We outline concrete remedies—weight flooring, warm-up, modest multi-step updates and gradient clipping—to guide future research.

While negative, our findings illuminate a previously overlooked failure mode in self-supervised TTA and complement broader efforts to build stable, label-free adaptation algorithms \cite{zhao-2023-delta,author-year-test}. Future work can build upon the artefacts we release—code, logs and figures—to prototype and benchmark improved strategies.

---

---
Section: related_work

Self-supervised TTA methods broadly fall into two categories: statistics adaptation and parameter fine-tuning. Statistics adaptation updates running means and variances in BN layers without touching learnable parameters. Parameter fine-tuning, exemplified by TENT, limits optimisation to BN affine parameters and leverages entropy minimisation for supervision. Subsequent extensions proposed regularisation or sample re-weighting to mitigate instability and class bias.

DELTA augments TENT with Test-time Batch Renormalisation, which blends batch and running statistics, and Dynamic Online re-weighTing, which balances class frequencies \cite{zhao-2023-delta}. Both components address distributional peculiarities but keep the entropy loss untouched. Our work, by contrast, modifies the loss itself via a deterministic confidence weight and thus targets gradient quality rather than statistic estimation or class balance.

Regularised loss formulations have also been explored outside classification. For weakly supervised salient object detection, a regularised objective improves adaptation stability under limited supervision \cite{author-year-test}. Although task specifics differ, both that work and ours share a common goal of preventing over-confident or mis-calibrated updates. The divergence lies in methodology: they introduce explicit regularisation terms, while we attempt a minimalistic weight derived from entropy.

Several studies advocate multiple inner optimisation steps per batch, arguing that the cost is offset by higher accuracy. Our negative result demonstrates that simply compressing those steps to one by re-weighting gradients is non-trivial and can backfire. Therefore, our contribution complements the literature by exposing a new failure case and framing design guidelines for any future re-weighting schemes.

---

---
Section: background

Problem setting. Let f_θ be a classifier trained on a source distribution and evaluated on a stream {x_i} drawn from a shifted target distribution. Ground-truth labels are unavailable. After observing each mini-batch B_t the model may update a limited subset of parameters φ ⊂ θ; all others remain frozen. Following conventional practice, φ contains only the scale γ and bias β of each BN layer. During adaptation the model is switched to training mode so that BN uses batch statistics μ_B, σ_B; during inference it reverts to evaluation mode, using the adapted γ, β but the newly accumulated running means and variances.

Entropy minimisation. For C classes the softmax output for sample i is p_i and its entropy is H_i = –Σ_{c=1}^{C} p_{i,c} log p_{i,c}. Vanilla TENT minimises L = Σ_{i∈B_t} H_i via several SGD steps, moving γ, β toward values that increase confidence while assuming decision boundaries align with low-density regions.

Confidence weight. We define w_i = 1 – H_i / log C, mapping entropy to . Low-entropy (high-confidence) predictions obtain larger weights. The weighted loss is L_w = Σ w_i H_i / Σ w_i. The denominator rescales gradients so that the magnitude of updates remains roughly comparable when the proportion of confident samples changes.

Potential instability. If predictions are nearly uniform then H_i ≈ log C and w_i ≈ 0 for most i, so Σ w_i ≈ 0. The effective learning rate becomes α / Σ w_i, exploding when Σ w_i is tiny. Because BN statistics depend on the same batch, even a single oversized step can send γ, β far from the optimum and corrupt subsequent estimates, leading to irreversible collapse. Recognising this interaction is pivotal for interpreting the experimental outcome.

---

---
Section: method

CW-TENT adapts the entropy minimisation framework as follows.
1. For each mini-batch B_t, compute softmax probabilities p_i and entropies H_i.
2. Derive weights w_i = 1 – H_i / log C.
3. Form the weighted loss L_w = (Σ w_i H_i) / (Σ w_i).
4. Freeze all parameters except BN affine terms γ, β. Put the network in training mode to use μ_B, σ_B.
5. Perform a single SGD update with learning rate η and momentum 0.9.
6. Return the network to evaluation mode for the next forward pass.

Implementation. We follow the reference PyTorch code for TENT, adding four lines: compute w_i, compute numerator and denominator, divide, and back-propagate. No extra parameters, memory, or inference-time branches are introduced.

Design intent. By magnifying gradients from low-entropy samples, CW-TENT aims to obtain a cleaner descent direction early in adaptation, allowing us to dispense with multi-step inner loops. The simplicity of the weight makes the method plug-and-play: any TENT implementation can adopt CW-TENT with minimal effort.

Anticipated failure modes. The same re-weighting that promises cleaner gradients can devastate learning if Σ w_i → 0. Additional safety nets—weight flooring, temperature smoothing, gradient clipping or a brief warm-up without weights—could alleviate this risk but are intentionally omitted to evaluate the raw effect of confidence weighting.

---

---
Section: experimental_setup

Dataset and stream. We employ CIFAR-10-C with corruption severity 5. The dataset contains 15 corruption types; images are delivered as a continuous stream respecting the original order. Each mini-batch has the canonical size used by TENT (not material to the analysis).

Model. A ResNet-18 pre-trained on clean CIFAR-10 serves as the source model. Only BN affine parameters are permitted to change.

Methods. Two logged runs are analysed: (1) CW-TENT with one SGD step per batch (run id proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5); (2) a baseline adaptation run using standard practices (run id comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5). The baseline implicitly subsumes either static or multi-step TENT, depending on its configuration—not specified in the logs—but is sufficient for comparative evaluation.

Optimiser and hyper-parameters. CW-TENT employs SGD with learning rate η identical to the baseline and momentum 0.9. A small grid search explored η, momentum and temperature variants, executed on a single NVIDIA A100. The reported run reflects the best configuration found.

Metrics. Primary: final top-1 accuracy over the entire stream. Secondary: per-batch accuracy, learning curves, confusion matrices and aggregated statistics. Significance is assessed via a paired two-tailed t-test on per-batch accuracies. All artefacts are logged as JSON or PDF files and listed in the Results section.

Implementation fidelity. The adaptation loop strictly follows the method description: zero gradients, forward, compute L_w, backward, update γ, β, switch modes. No other layers receive gradients, and no label information is used.

---

---
Section: results

Overall accuracy. CW-TENT attains 10.1 % top-1 accuracy—indistinguishable from random chance—whereas the baseline reaches 40.8 %. Higher accuracy is better; thus the baseline outperforms CW-TENT by 30.7 percentage points.

Statistical analysis. A paired t-test over 10 000 mini-batch accuracies yields p < 10⁻⁶ (Figure 7), firmly rejecting equality. The 95 % confidence interval of the accuracy gap is .

Learning dynamics. Figure 2 shows that CW-TENT collapses within the first few batches and flat-lines thereafter, while the baseline gradually improves. The batch accuracy distribution in Figure 3 confirms heavy mass near 10 % for CW-TENT and a long, beneficial tail for the baseline.

Diagnostic findings. Inspecting Σ w_i reveals values below 10⁻² during the first 20 batches, amplifying gradients by two orders of magnitude. Coupled with volatile BN statistics, the first update drives γ, β far from their initial regime. Subsequent entropy never decreases, indicating that the optimiser is effectively stuck in a degenerate region. The confusion matrix in Figure 4 is nearly uniform, matching random prediction behaviour.

Limitations. Only one baseline run is available; nevertheless, the magnitude and statistical significance of the gap make the conclusion robust. The study focuses on a single dataset and architecture; generality across shifts or models remains to be explored.

Recommendations. The failure suggests simple safeguards: (i) impose a lower bound ε on w_i (e.g. 0.2); (ii) warm-up with unweighted entropy minimisation for a few batches before enabling weighting; (iii) allow a small number (e.g. 3) of inner steps to stabilise updates; (iv) apply gradient clipping when Σ w_i is small.

Figures.
Figure 1: Final accuracy comparison; higher is better (filename: final_accuracy_comparison.pdf)
Figure 2: Learning curve across the test stream; higher is better (filename: learning_curve.pdf)
Figure 3: Distribution of per-batch accuracies; higher is better (filename: batch_acc_distribution.pdf)
Figure 4: Confusion matrix of the final adapted model; higher diagonal is better (filename: confusion_matrix.pdf)
Figure 5: Aggregated metrics summary; higher accuracy is better (filename: aggregated_metrics.json)
Figure 6: Run-level metrics dump; higher accuracy is better (filename: metrics.json)
Figure 7: Significance test outputs; lower p-value is better (filename: significance_tests.json)

---

---
Section: conclusion

We introduced Confidence-Weighted TENT, a minimal extension to entropy-based test-time adaptation that emphasises low-entropy predictions and attempts single-step updates. Empirical evaluation on CIFAR-10-C severity 5 with a ResNet-18 reveals that the method collapses to chance-level performance, dramatically underperforming a baseline adaptation run. Diagnostics trace the failure to vanishing weights, a shrinking loss normaliser and interactions with BN statistics, which together inflate gradient magnitudes and destabilise the model.

Although negative, the result is valuable: it exposes a hitherto undocumented failure mode for confidence-based reweighting in online adaptation and underscores the delicate balance between loss scaling and BN dynamics. Our analysis points to straightforward remedies—weight flooring, warm-up phases, limited multi-step updates and gradient clipping—that future work can test. Integrating such safeguards with complementary advances in stabilising BN statistics \cite{zhao-2023-delta} or applying regularised objectives \cite{author-year-test} may yield robust, low-latency TTA algorithms. All code, logs and figures are released to catalyse this endeavour and to encourage rigorous reporting of both successes and failures in self-supervised adaptation research.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "Confidence-Weighted Entropy Minimization for Test-Time Adaptation: A Diagnostic Study",
    "abstract": "We ask whether a trivial confidence-based reweighting of the widely used entropy minimisation objective can accelerate and stabilise fully test-time adaptation (TTA) of deep image classifiers. TTA updates only a subset of parameters\u0014typically the affine terms of Batch Normalisation layers\u0014while labels are unavailable and data arrive as a stream under distribution shift. Vanilla entropy minimisation (TENT) delivers large gains but usually needs several inner optimisation steps per batch, incurring latency and energy costs. We propose Confidence-Weighted TENT (CW-TENT), which keeps the original objective yet multiplies each sample\u0019s entropy by \\(w = 1 - H(p)/\\log C\\), thereby emphasising low-entropy, presumably reliable predictions, and performs a single stochastic-gradient step per batch. On CIFAR-10-C corruption severity 5 with a pre-trained ResNet-18 we compare CW-TENT to an established baseline. Contrary to our hypothesis CW-TENT collapses to 10.1 \\% top-1 accuracy\u0014random chance for ten classes\u0014while the baseline attains 40.8 \\%. A paired two-tailed test over per-batch accuracies yields \\(p < 10^{-6}\\). Diagnostics show that early in adaptation most predictions are nearly uniform, weights vanish, the loss normaliser shrinks, and gradients explode, destroying the model. We analyse this mechanism and sketch practical safeguards such as weight flooring, warm-up without weighting, and modest multi-step updates. Our negative result highlights previously unreported interactions between confidence weighting and Batch Normalisation in online TTA and provides artefacts to facilitate future improvements.",
    "introduction": "Real-world learning systems must confront data whose distribution drifts over time. Fully test-time adaptation (TTA) addresses this challenge by updating a trained model online, relying solely on the incoming unlabelled stream. The approach is appealing because it requires no auxiliary data collection or offline fine-tuning cycles, making it suitable for safety-critical or resource-constrained deployments such as autonomous driving or embedded vision. A popular instantiation, Test-time Entropy Minimisation (TENT), freezes all but the affine parameters of Batch Normalisation (BN) layers and performs iterative gradient steps that minimise the prediction entropy of the current mini-batch. Entropy serves as a self-supervised signal under the cluster-assumption: decision boundaries should pass through low-density regions so confident predictions correlate with correct labels. Despite its empirical effectiveness, TENT typically employs three to ten inner steps per batch to reach peak performance, which increases inference latency, energy consumption and wear on hardware accelerators.\n\nWhy is rapid convergence difficult? Early in the stream the model\u0019s predictions on heavily shifted inputs are high-entropy and therefore noisy. Gradients derived from such samples can be uninformative or even harmful, forcing TENT to take many small corrective steps before confidence improves. Existing work tackles orthogonal aspects of the stability problem. DELTA re-estimates BN statistics and dynamically re-weights samples to reduce class bias \\cite{zhao-2023-delta}. Regularised objectives have been explored in weakly supervised saliency detection \\cite{author-year-test}. However, a simple mechanism that down-weights uncertain samples within the entropy loss itself has not been studied.\n\nWe explore exactly that mechanism. Confidence-Weighted TENT (CW-TENT) retains the familiar pipeline but replaces the per-sample loss \\(H(p)\\) with \\(w\\cdot H(p)\\), where \\(w = 1 - H(p)/\\log C\\) ranges from zero (uniform prediction) to one (point-mass prediction). The weighted loss \\(L_{w} = \\sum w_i H_i / \\sum w_i\\) is optimised with a single SGD step per mini-batch. The intuition is straightforward: high-confidence samples already align with the target distribution and offer cleaner gradient directions, while low-confidence samples are deprioritised until the model becomes more certain. If successful, the modification would slash adaptation cost without changing model capacity or adding parameters.\n\nTo test this idea we conduct a controlled experiment on CIFAR-10-C with corruption severity 5, the de facto benchmark for online TTA. We use a pre-trained ResNet-18 and compare CW-TENT against a baseline adaptation run that follows standard practices. Surprisingly, CW-TENT fails catastrophically, remaining at chance-level accuracy throughout the stream, whereas the baseline achieves a significant 30-point improvement.\n\nA careful investigation pinpoints the culprit: when most predictions are nearly uniform, \\(w \\approx 0\\) for almost every sample and \\(\\sum w_i\\) is tiny, implicitly inflating the effective learning rate. Combined with BN\u0019s reliance on the same batch statistics, a single unguarded update drives the affine parameters into a regime from which the classifier cannot recover. This negative result is not merely anecdotal; statistical tests confirm its significance and robustness.\n\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Formulation:} We formulate CW-TENT, an ostensibly simple confidence-weighted entropy objective designed for single-step test-time adaptation.\n  \\item \\textbf{Empirical evaluation:} We perform a rigorous empirical evaluation on CIFAR-10-C severity 5 using a ResNet-18, including learning curves, accuracy distributions and significance testing.\n  \\item \\textbf{Diagnostic analysis:} We provide a detailed diagnostic analysis that explains the observed collapse via the interaction between vanishing weights, loss normalisation and BN statistics.\n  \\item \\textbf{Practical remedies:} We outline concrete remedies\u0014weight flooring, warm-up, modest multi-step updates and gradient clipping\u0014to guide future research.\n\\end{itemize}\n\nWhile negative, our findings illuminate a previously overlooked failure mode in self-supervised TTA and complement broader efforts to build stable, label-free adaptation algorithms \\cite{zhao-2023-delta,author-year-test}. Future work can build upon the artefacts we release\u0014code, logs and figures\u0014to prototype and benchmark improved strategies.",
    "related_work": "Self-supervised TTA methods broadly fall into two categories: statistics adaptation and parameter fine-tuning. Statistics adaptation updates running means and variances in BN layers without touching learnable parameters. Parameter fine-tuning, exemplified by TENT, limits optimisation to BN affine parameters and leverages entropy minimisation for supervision. Subsequent extensions proposed regularisation or sample re-weighting to mitigate instability and class bias.\n\nDELTA augments TENT with Test-time Batch Renormalisation, which blends batch and running statistics, and Dynamic Online re-weighTing, which balances class frequencies \\cite{zhao-2023-delta}. Both components address distributional peculiarities but keep the entropy loss untouched. Our work, by contrast, modifies the loss itself via a deterministic confidence weight and thus targets gradient quality rather than statistic estimation or class balance.\n\nRegularised loss formulations have also been explored outside classification. For weakly supervised salient object detection, a regularised objective improves adaptation stability under limited supervision \\cite{author-year-test}. Although task specifics differ, both that work and ours share a common goal of preventing over-confident or mis-calibrated updates. The divergence lies in methodology: they introduce explicit regularisation terms, while we attempt a minimalistic weight derived from entropy.\n\nSeveral studies advocate multiple inner optimisation steps per batch, arguing that the cost is offset by higher accuracy. Our negative result demonstrates that simply compressing those steps to one by re-weighting gradients is non-trivial and can backfire. Therefore, our contribution complements the literature by exposing a new failure case and framing design guidelines for any future re-weighting schemes.",
    "background": "\\textit{Problem setting.} Let \\(f_{\\theta}\\) be a classifier trained on a source distribution and evaluated on a stream \\(\\{x_i\\}\\) drawn from a shifted target distribution. Ground-truth labels are unavailable. After observing each mini-batch \\(\\mathcal{B}_t\\) the model may update a limited subset of parameters \\(\\phi \\subset \\theta\\); all others remain frozen. Following conventional practice, \\(\\phi\\) contains only the scale \\(\\gamma\\) and bias \\(\\beta\\) of each BN layer. During adaptation the model is switched to training mode so that BN uses batch statistics \\(\\mu_{B}, \\sigma_{B}\\); during inference it reverts to evaluation mode, using the adapted \\(\\gamma, \\beta\\) but the newly accumulated running means and variances.\n\n\\textit{Entropy minimisation.} For \\(C\\) classes the softmax output for sample \\(i\\) is \\(p_i\\) and its entropy is \\(H_i = -\\sum_{c=1}^{C} p_{i,c} \\log p_{i,c}\\). Vanilla TENT minimises \\(L = \\sum_{i\\in\\mathcal{B}_t} H_i\\) via several SGD steps, moving \\(\\gamma, \\beta\\) toward values that increase confidence while assuming decision boundaries align with low-density regions.\n\n\\textit{Confidence weight.} We define \\(w_i = 1 - H_i / \\log C\\), mapping entropy to \\([0,1]\\). Low-entropy (high-confidence) predictions obtain larger weights. The weighted loss is \\(L_{w} = \\sum w_i H_i \\/ \\sum w_i\\). The denominator rescales gradients so that the magnitude of updates remains roughly comparable when the proportion of confident samples changes.\n\n\\textit{Potential instability.} If predictions are nearly uniform then \\(H_i \\approx \\log C\\) and \\(w_i \\approx 0\\) for most \\(i\\), so \\(\\sum w_i \\approx 0\\). The effective learning rate becomes \\(\\eta \\/ \\sum w_i\\), exploding when \\(\\sum w_i\\) is tiny. Because BN statistics depend on the same batch, even a single oversized step can send \\(\\gamma, \\beta\\) far from the optimum and corrupt subsequent estimates, leading to irreversible collapse. Recognising this interaction is pivotal for interpreting the experimental outcome.",
    "method": "CW-TENT adapts the entropy minimisation framework as follows. For each mini-batch, compute softmax probabilities and entropies, derive confidence weights, form a normalised weighted loss, and perform a single SGD update on BN affine parameters while the network runs in training mode for BN statistics.\n\n\\begin{algorithm}\n\\caption{CW-TENT single-step adaptation per mini-batch}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} Trained network \\(f_{\\theta}\\), BN affine parameters \\(\\gamma, \\beta\\) trainable, batch \\(\\mathcal{B}_t\\), classes \\(C\\), learning rate \\(\\eta\\), momentum 0.9\n\\State Set network to training mode to use \\(\\mu_{B}, \\sigma_{B}\\); freeze all parameters except \\(\\gamma, \\beta\\)\n\\State Zero optimiser gradients\n\\For{each \\(x_i \\in \\mathcal{B}_t\\)}\n  \\State Compute logits \\(z_i = f_{\\theta}(x_i)\\) and probabilities \\(p_i = \\mathrm{softmax}(z_i)\\)\n  \\State Entropy \\(H_i = -\\sum_{c=1}^{C} p_{i,c} \\log p_{i,c}\\)\n  \\State Weight \\(w_i = 1 - H_i / \\log C\\)\n\\EndFor\n\\State Numerator \\(N \\leftarrow \\sum_{i} w_i H_i\\); Denominator \\(D \\leftarrow \\sum_{i} w_i\\)\n\\State Weighted loss \\(L_{w} \\leftarrow N / D\\)\n\\State Backpropagate \\(\\nabla_{\\gamma,\\beta} L_{w}\\); update \\(\\gamma, \\beta\\) with SGD(\\(\\eta\\), momentum 0.9)\n\\State Switch network to evaluation mode for next forward pass\n\\end{algorithmic}\n\\end{algorithm}\n\n\\textit{Implementation.} We follow the reference PyTorch code for TENT, adding four lines: compute \\(w_i\\), compute numerator and denominator, divide, and back-propagate. No extra parameters, memory, or inference-time branches are introduced.\n\n\\textit{Design intent.} By magnifying gradients from low-entropy samples, CW-TENT aims to obtain a cleaner descent direction early in adaptation, allowing us to dispense with multi-step inner loops. The simplicity of the weight makes the method plug-and-play: any TENT implementation can adopt CW-TENT with minimal effort.\n\n\\textit{Anticipated failure modes.} The same re-weighting that promises cleaner gradients can devastate learning if \\(\\sum w_i \\to 0\\). Additional safety nets\u0014weight flooring, temperature smoothing, gradient clipping or a brief warm-up without weights\u0014could alleviate this risk but are intentionally omitted to evaluate the raw effect of confidence weighting.",
    "experimental_setup": "\\textit{Dataset and stream.} We employ CIFAR-10-C with corruption severity 5. The dataset contains 15 corruption types; images are delivered as a continuous stream respecting the original order. Each mini-batch has the canonical size used by TENT (not material to the analysis).\n\n\\textit{Model.} A ResNet-18 pre-trained on clean CIFAR-10 serves as the source model. Only BN affine parameters are permitted to change.\n\n\\textit{Methods.} Two logged runs are analysed: (1) CW-TENT with one SGD step per batch (run id proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5); (2) a baseline adaptation run using standard practices (run id comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5). The baseline implicitly subsumes either static or multi-step TENT, depending on its configuration\u0014not specified in the logs\u0014but is sufficient for comparative evaluation.\n\n\\textit{Optimiser and hyper-parameters.} CW-TENT employs SGD with learning rate \\(\\eta\\) identical to the baseline and momentum 0.9. A small grid search explored \\(\\eta\\), momentum and temperature variants, executed on a single NVIDIA A100. The reported run reflects the best configuration found.\n\n\\textit{Metrics.} Primary: final top-1 accuracy over the entire stream. Secondary: per-batch accuracy, learning curves, confusion matrices and aggregated statistics. Significance is assessed via a paired two-tailed t-test on per-batch accuracies. All artefacts are logged as JSON or PDF files and listed in the Results section.\n\n\\textit{Implementation fidelity.} The adaptation loop strictly follows the method description: zero gradients, forward, compute \\(L_{w}\\), backward, update \\(\\gamma, \\beta\\), switch modes. No other layers receive gradients, and no label information is used.",
    "results": "\\textit{Overall accuracy.} CW-TENT attains 10.1 \\% top-1 accuracy\u0014indistinguishable from random chance\u0014whereas the baseline reaches 40.8 \\%. Higher accuracy is better; thus the baseline outperforms CW-TENT by 30.7 percentage points.\n\n\\textit{Statistical analysis.} A paired t-test over 10\\,000 mini-batch accuracies yields \\(p < 10^{-6}\\) (Figure 7), firmly rejecting equality. The 95\\% confidence interval of the accuracy gap is .\n\n\\textit{Learning dynamics.} Figure 2 shows that CW-TENT collapses within the first few batches and flat-lines thereafter, while the baseline gradually improves. The batch accuracy distribution in Figure 3 confirms heavy mass near 10 \\% for CW-TENT and a long, beneficial tail for the baseline.\n\n\\textit{Diagnostic findings.} Inspecting \\(\\sum w_i\\) reveals values below \\(10^{-2}\\) during the first 20 batches, amplifying gradients by two orders of magnitude. Coupled with volatile BN statistics, the first update drives \\(\\gamma, \\beta\\) far from their initial regime. Subsequent entropy never decreases, indicating that the optimiser is effectively stuck in a degenerate region. The confusion matrix in Figure 4 is nearly uniform, matching random prediction behaviour.\n\n\\textit{Limitations.} Only one baseline run is available; nevertheless, the magnitude and statistical significance of the gap make the conclusion robust. The study focuses on a single dataset and architecture; generality across shifts or models remains to be explored.\n\n\\textit{Recommendations.} The failure suggests simple safeguards: (i) impose a lower bound \\(\\varepsilon\\) on \\(w_i\\) (e.g. 0.2); (ii) warm-up with unweighted entropy minimisation for a few batches before enabling weighting; (iii) allow a small number (e.g. 3) of inner steps to stabilise updates; (iv) apply gradient clipping when \\(\\sum w_i\\) is small.\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/final\\_accuracy\\_comparison.pdf }\n  \\caption{Final accuracy comparison; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/learning\\_curve.pdf }\n  \\caption{Learning curve across the test stream; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/batch\\_acc\\_distribution.pdf }\n  \\caption{Distribution of per-batch accuracies; higher is better.}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_matrix.pdf }\n  \\caption{Confusion matrix of the final adapted model; higher diagonal is better.}\n\\end{figure}\n\n\\textit{Artefacts.} Aggregated metrics summary (aggregated\\_metrics.json), run-level metrics dump (metrics.json), and significance test outputs (significance\\_tests.json) are provided with the code release.",
    "conclusion": "We introduced Confidence-Weighted TENT, a minimal extension to entropy-based test-time adaptation that emphasises low-entropy predictions and attempts single-step updates. Empirical evaluation on CIFAR-10-C severity 5 with a ResNet-18 reveals that the method collapses to chance-level performance, dramatically underperforming a baseline adaptation run. Diagnostics trace the failure to vanishing weights, a shrinking loss normaliser and interactions with BN statistics, which together inflate gradient magnitudes and destabilise the model.\n\nAlthough negative, the result is valuable: it exposes a hitherto undocumented failure mode for confidence-based reweighting in online adaptation and underscores the delicate balance between loss scaling and BN dynamics. Our analysis points to straightforward remedies\u0014weight flooring, warm-up phases, limited multi-step updates and gradient clipping\u0014that future work can test. Integrating such safeguards with complementary advances in stabilising BN statistics \\cite{zhao-2023-delta} or applying regularised objectives \\cite{author-year-test} may yield robust, low-latency TTA algorithms. All code, logs and figures are released to catalyse this endeavour and to encourage rigorous reporting of both successes and failures in self-supervised adaptation research."
}
