
LLM Name: gpt-5-2025-08-07
Input:
Convert research paper sections to clean, semantic HTML for GitHub Pages publication.

## Input Data:

**title:**
One-Step Test-Time Adaptation via Confidence-Weighted Entropy Minimization


**abstract:**
Test-time adaptation (TTA) updates a deployed model online to counter distribution shift, yet leading techniques such as entropy minimisation over Batch-Norm affine parameters usually need several gradient steps per incoming mini-batch. This multiplies latency and energy cost, hampering use in real-time systems. We trace the slow convergence to noisy gradients emitted by high-entropy, low-confidence samples that dominate the early optimisation landscape. To suppress this noise we introduce Confidence-Weighted TENT (CW-TENT), a drop-in replacement for standard TENT that assigns each sample a weight w = 1 – H(p)/log C and minimises the normalised weighted entropy L_w = Σ w H(p)/Σ w. The method keeps exactly the same learnable parameter subset and requires a single extra line in the loss definition while allowing only one SGD step per batch. On CIFAR-10-C (severity 5) with a pre-trained ResNet-18 CW-TENT attains a logged final_accuracy of 0.101, whereas vanilla TENT records 0.393; interpreting these values as error rates yields 89.9 % versus 60.7 % top-1 accuracy with a paired t-test p = 3.6 × 10⁻⁶. Thus CW-TENT matches or surpasses multi-step baselines while reducing inner updates by an order of magnitude, offering a practical route to low-latency robust inference.


**introduction:**
Deep neural networks often face performance degradation when the test distribution diverges from training data. Test-time adaptation (TTA) addresses this challenge by updating a model on-the-fly using only unlabeled test samples. A particularly practical variant adapts only the affine parameters of Batch-Norm layers and minimises prediction entropy over the current mini-batch. This strategy, popularised by TENT, is attractive for its unsupervised nature and small memory footprint but typically relies on three to ten gradient steps per mini-batch to reach peak accuracy. Such iterative updates inflate latency and energy consumption, limiting adoption in latency-critical applications such as robotics, on-device perception, or augmented reality.  
The root cause of this inefficiency, we argue, is that early in adaptation most predictions exhibit high entropy. These low-confidence samples contribute gradients with large variance, obscuring the true descent direction and forcing the optimiser to take multiple cautious steps. Existing work has explored better normalisation statistics and class-level reweighting \cite{zhao-2023-delta}, but a direct mechanism to down-weight uncertain samples inside the core entropy loss has remained unexplored.  
We propose Confidence-Weighted TENT (CW-TENT), a minimalist extension of entropy minimisation that assigns each sample a confidence weight w = 1 – H(p)/log C, where H denotes entropy and C is the number of classes. Replacing the plain entropy objective with its weighted counterpart allows high-confidence samples to dominate the update, producing a cleaner gradient direction that enables effective one-step adaptation. Implementation is trivial: four extra lines of code on top of the original TENT loop.  
We assess CW-TENT on the standard CIFAR-10-C corruption benchmark at severity 5 using a frozen ResNet-18 backbone. Experimental logs contain two runs: the proposed method (final_accuracy = 0.101) and vanilla TENT with ten inner steps (final_accuracy = 0.393). Treating these logged values as error rates, CW-TENT delivers 89.9 % accuracy versus TENT’s 60.7 %, a 29.2-point gain confirmed by a paired t-test (p = 3.6 × 10⁻⁶). Crucially, CW-TENT accomplishes this with a single update per batch, cutting compute by roughly ten-fold.  
Contributions:  
• Identify gradient noise from high-entropy samples as the bottleneck behind the multi-step requirement of existing entropy-based TTA.  
• Introduce a confidence-weighted entropy loss that can be applied without architectural changes or additional statistics.  
• Demonstrate on CIFAR-10-C that one-step CW-TENT outperforms ten-step TENT, achieving 89.9 % accuracy with strong statistical significance.  
• Provide a lightweight, plug-and-play implementation suitable for real-time deployment.  
The remainder of the paper reviews related work, details the proposed method, describes the experimental protocol, reports results, and concludes with future research directions including integration with improved normalisation schemes \cite{zhao-2023-delta} and extension to other tasks such as weakly supervised saliency adaptation \cite{author-year-test}.


**related_work:**
TTA techniques can be grouped by what they adapt and which unsupervised objectives they employ. Batch-Norm based methods update either running statistics or affine parameters to counter covariate shift. Standard entropy minimisation over Batch-Norm affine parameters, exemplified by TENT, is widely used for its simplicity but suffers from slow convergence. DELTA augments this family with test-time batch renormalisation and class-level dynamic online re-weighting, alleviating statistic drift and class bias \cite{zhao-2023-delta}. Our work tackles a complementary problem: per-sample gradient noise within the entropy loss itself.  
Alternative approaches incorporate self-training with pseudo-labels, regularisation terms, or memory buffers that revisit past samples. While these strategies can improve accuracy, they introduce additional parameters, storage, or hyper-parameters not required by CW-TENT.  
Outside classification, TTA ideas extend to dense prediction tasks such as weakly supervised salient object detection, where specialised losses are introduced to guide adaptation \cite{author-year-test}. These domain-specific objectives underscore the versatility of TTA but are orthogonal to our goal of accelerating the generic entropy-based framework.  
Compared to DELTA’s class-level weighting and statistic correction, CW-TENT offers a per-sample confidence emphasis that leaves the normalisation machinery untouched. The two techniques are therefore compatible and may be combined in future work.


**background:**
Consider a multi-class classifier f_θ that outputs logits z ∈ ℝ^C and probabilities p = softmax(z). At deployment the model receives a stream of mini-batches drawn from a shifted target distribution. Labels are unavailable; only test inputs are accessible. The adaptation objective commonly used for TENT minimises batch entropy L = Σ_i H(p_i), where H(p) = –Σ_c p_c log p_c. Optimisation is limited to the affine parameters of Batch-Norm layers, constraining capacity and reducing catastrophic drift.  
Empirically, gradients derived from high-entropy predictions are noisy and misaligned with the eventual optimum, forcing practitioners to take several small SGD steps per batch. Our key observation is that each sample’s entropy already encodes a proxy for gradient reliability: lower entropy implies the prediction is nearer a confident decision, hence its gradient direction is more trustworthy. This motivates weighting samples by confidence directly inside the entropy objective.  
Previous research has highlighted additional pitfalls in TTA such as unreliable batch statistics and class imbalance, proposing remedies like batch renormalisation and class-level weighting \cite{zhao-2023-delta}. CW-TENT assumes the standard Batch-Norm behaviour and focuses solely on mitigating per-sample gradient noise, introducing no extra statistics or memory beyond what is already computed in the forward pass.


**method:**
Confidence-Weighted TENT replaces the plain entropy loss with a weighted variant. For a mini-batch of N samples, compute each sample’s weight w_i = 1 – H(p_i)/log C, which lies in , being 0 for maximum-entropy predictions and approaching 1 as confidence grows. The objective becomes  
L_w = (Σ_i w_i H(p_i)) / (Σ_i w_i).  
The denominator normalises the loss scale, preventing trivial shrinkage when many uncertain samples appear. During each incoming mini-batch the procedure is:  
1. Set the network to train mode so Batch-Norm layers use batch statistics.  
2. Compute logits and probabilities.  
3. Calculate entropies and weights, form L_w.  
4. Perform one SGD step on Batch-Norm affine parameters (optionally with momentum 0.9).  
5. Switch the model back to eval mode for prediction.  
This algorithm adds only the weight computation and modified loss; all other components of TENT remain untouched. From an optimisation view, gradients become a convex combination of per-sample entropy gradients scaled by w_i, emphasising clean signals and enabling effective single-step updates. As adaptation progresses and predictions sharpen, weights converge toward uniformity, naturally annealing the curriculum.


**experimental_setup:**
We evaluate on CIFAR-10-C with corruption severity 5, a standard robustness benchmark. The base model is a ResNet-18 pre-trained on clean CIFAR-10. Incoming data are streamed in mini-batches; at each batch the adaptation routine updates only Batch-Norm affine parameters.  
Compared methods:  
• Source: no adaptation.  
• TENT (comparative-1): unweighted entropy minimisation with ten inner SGD steps per batch.  
• CW-TENT (proposed): confidence-weighted entropy with a single step per batch.  
Optimisation employs SGD, learning rate chosen via a small grid in ; momentum is 0.9 unless stated otherwise. The primary metric is top-1 accuracy accumulated over the full stream; logs store this as final_accuracy. Additional logs capture per-batch accuracies, enabling convergence and statistical analysis.  
Experiments run on a single NVIDIA A100 GPU; hyper-parameter sweeps are parallelised across eight devices when available. Reproducibility artefacts include metrics.json, aggregated_metrics.json, and significance_tests.json as well as PDF visualisations of learning curves and confusion matrices.


**results:**
Aggregated metrics list final_accuracy = 0.10109473684210528 for CW-TENT and 0.3932590019315017 for TENT. Interpreting these as error rates, CW-TENT achieves 89.9 % accuracy, surpassing TENT’s 60.7 % by 29.2 percentage points. A paired t-test over per-batch accuracies yields p = 3.6 × 10⁻⁶, confirming statistical significance.  
CW-TENT reaches over 80 % accuracy after its very first update and plateaus, whereas TENT requires roughly eight steps to approach its maximum, validating the claim of faster convergence. Because CW-TENT uses one update rather than ten, it reduces gradient computations and associated latency by an order of magnitude while improving accuracy.  
Ablation studies show robustness to learning rate choices within the tested band (≤1 pp variance) and illustrate that removing the confidence weight while keeping the one-step schedule drops accuracy to 52.4 %, underscoring that weighting—not step count—is critical. Momentum adds a modest ≈1 pp gain.  
Limitations include evaluation on a single architecture and corruption severity; broader studies across models, severities, and domains remain future work.  
Figures:  
Figure 1: Confusion matrix averaged over the evaluation stream; higher diagonal values are better (filename: confusion_matrix.pdf).  
Figure 2: Online learning curves comparing convergence speed; higher is better (filename: learning_curve.pdf).  
Figure 3: Per-run metric dumps; higher accuracy is better (filename: metrics.json).  
Figure 4: Aggregated summary metrics; higher accuracy is better (filename: aggregated_metrics.json).  
Figure 5: Batch-wise accuracy distribution; higher shifted mass is better (filename: batch_acc_distribution.pdf).  
Figure 6: Final accuracy comparison bar chart; higher bars are better (filename: final_accuracy_comparison.pdf).  
Figure 7: Statistical significance test results; lower p-values indicate stronger evidence (filename: significance_tests.json).


**conclusion:**
CW-TENT introduces a confidence-weighted entropy objective that suppresses noisy gradients from uncertain samples, enabling reliable one-step test-time adaptation while retaining the simplicity of adapting only Batch-Norm affine parameters. On CIFAR-10-C severity 5 the method attains 89.9 % accuracy versus 60.7 % for ten-step TENT, with strong statistical backing and a ten-fold reduction in computation. The approach is orthogonal to—and can be combined with—improved normalisation and class balancing strategies \cite{zhao-2023-delta}, and is generic enough to extend beyond image classification to tasks such as weakly supervised saliency detection \cite{author-year-test}. Future work will integrate these complementary techniques, explore longer and more diverse test streams, and examine optimiser and temperature schedules to further enhance robustness and efficiency.



**Available Images:**

- confusion_matrix.pdf

- learning_curve.pdf

- metrics.json

- confusion_matrix.pdf

- learning_curve.pdf

- metrics.json

- aggregated_metrics.json

- batch_acc_distribution.pdf

- final_accuracy_comparison.pdf

- significance_tests.json



## HTML Requirements:

### Section Structure:
- **Title section**: Use `<h2 class="paper-title">Title Text</h2>` (no wrapper)
- **Other sections**: Use `<section><h2>Section Name</h2>...</section>`
- **Paragraphs**: Use `<p>` for each logical paragraph
- **Lists**: Use `<ul><li><strong>Key:</strong> Description</li></ul>`

### Figures (Results section only):
- Use `<figure><img src="images/filename.png"><figcaption>Figure N: Description</figcaption></figure>`
- Convert .pdf to .png in src paths (e.g., `plot1.pdf` → `images/plot1.png`)
- **Width rules:**
  - Paired images (*_pair1.png, *_pair2.png): `<figure class="img-pair">` with `style="width:48%"` each
  - Single images: `style="width:70%"`

### Code (Method section only):
- Use `<pre><code>` for pseudocode/code blocks

### General Rules:
- No `<html>`, `<head>`, `<body>` tags
- All links need `target="_blank"`
- Only use content from input - don't invent anything
- Citations in [key] format will be converted to links automatically

## Output:
Generate only the HTML content as shown:

```html
<h2 class="paper-title">Title Text</h2>

<section>
  <h2>Abstract</h2>
  <p>Abstract content...</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Introduction content...</p>
</section>
```
Output:
{
    "generated_html_text": "<h2 class=\"paper-title\">One-Step Test-Time Adaptation via Confidence-Weighted Entropy Minimization</h2>\n\n<section>\n  <h2>Abstract</h2>\n  <p>Test-time adaptation (TTA) updates a deployed model online to counter distribution shift, yet leading techniques such as entropy minimisation over Batch-Norm affine parameters usually need several gradient steps per incoming mini-batch. This multiplies latency and energy cost, hampering use in real-time systems. We trace the slow convergence to noisy gradients emitted by high-entropy, low-confidence samples that dominate the early optimisation landscape. To suppress this noise we introduce Confidence-Weighted TENT (CW-TENT), a drop-in replacement for standard TENT that assigns each sample a weight w = 1 – H(p)/log C and minimises the normalised weighted entropy L_w = Σ w H(p)/Σ w. The method keeps exactly the same learnable parameter subset and requires a single extra line in the loss definition while allowing only one SGD step per batch. On CIFAR-10-C (severity 5) with a pre-trained ResNet-18 CW-TENT attains a logged final_accuracy of 0.101, whereas vanilla TENT records 0.393; interpreting these values as error rates yields 89.9 % versus 60.7 % top-1 accuracy with a paired t-test p = 3.6 × 10⁻⁶. Thus CW-TENT matches or surpasses multi-step baselines while reducing inner updates by an order of magnitude, offering a practical route to low-latency robust inference.</p>\n</section>\n\n<section>\n  <h2>Introduction</h2>\n  <p>Deep neural networks often face performance degradation when the test distribution diverges from training data. Test-time adaptation (TTA) addresses this challenge by updating a model on-the-fly using only unlabeled test samples. A particularly practical variant adapts only the affine parameters of Batch-Norm layers and minimises prediction entropy over the current mini-batch. This strategy, popularised by TENT, is attractive for its unsupervised nature and small memory footprint but typically relies on three to ten gradient steps per mini-batch to reach peak accuracy. Such iterative updates inflate latency and energy consumption, limiting adoption in latency-critical applications such as robotics, on-device perception, or augmented reality.</p>\n  <p>The root cause of this inefficiency, we argue, is that early in adaptation most predictions exhibit high entropy. These low-confidence samples contribute gradients with large variance, obscuring the true descent direction and forcing the optimiser to take multiple cautious steps. Existing work has explored better normalisation statistics and class-level reweighting [zhao-2023-delta], but a direct mechanism to down-weight uncertain samples inside the core entropy loss has remained unexplored.</p>\n  <p>We propose Confidence-Weighted TENT (CW-TENT), a minimalist extension of entropy minimisation that assigns each sample a confidence weight w = 1 – H(p)/log C, where H denotes entropy and C is the number of classes. Replacing the plain entropy objective with its weighted counterpart allows high-confidence samples to dominate the update, producing a cleaner gradient direction that enables effective one-step adaptation. Implementation is trivial: four extra lines of code on top of the original TENT loop.</p>\n  <p>We assess CW-TENT on the standard CIFAR-10-C corruption benchmark at severity 5 using a frozen ResNet-18 backbone. Experimental logs contain two runs: the proposed method (final_accuracy = 0.101) and vanilla TENT with ten inner steps (final_accuracy = 0.393). Treating these logged values as error rates, CW-TENT delivers 89.9 % accuracy versus TENT’s 60.7 %, a 29.2-point gain confirmed by a paired t-test (p = 3.6 × 10⁻⁶). Crucially, CW-TENT accomplishes this with a single update per batch, cutting compute by roughly ten-fold.</p>\n  <ul>\n    <li>Identify gradient noise from high-entropy samples as the bottleneck behind the multi-step requirement of existing entropy-based TTA.</li>\n    <li>Introduce a confidence-weighted entropy loss that can be applied without architectural changes or additional statistics.</li>\n    <li>Demonstrate on CIFAR-10-C that one-step CW-TENT outperforms ten-step TENT, achieving 89.9 % accuracy with strong statistical significance.</li>\n    <li>Provide a lightweight, plug-and-play implementation suitable for real-time deployment.</li>\n  </ul>\n  <p>The remainder of the paper reviews related work, details the proposed method, describes the experimental protocol, reports results, and concludes with future research directions including integration with improved normalisation schemes [zhao-2023-delta] and extension to other tasks such as weakly supervised saliency adaptation [author-year-test].</p>\n</section>\n\n<section>\n  <h2>Related Work</h2>\n  <p>TTA techniques can be grouped by what they adapt and which unsupervised objectives they employ. Batch-Norm based methods update either running statistics or affine parameters to counter covariate shift. Standard entropy minimisation over Batch-Norm affine parameters, exemplified by TENT, is widely used for its simplicity but suffers from slow convergence. DELTA augments this family with test-time batch renormalisation and class-level dynamic online re-weighting, alleviating statistic drift and class bias [zhao-2023-delta]. Our work tackles a complementary problem: per-sample gradient noise within the entropy loss itself.</p>\n  <p>Alternative approaches incorporate self-training with pseudo-labels, regularisation terms, or memory buffers that revisit past samples. While these strategies can improve accuracy, they introduce additional parameters, storage, or hyper-parameters not required by CW-TENT.</p>\n  <p>Outside classification, TTA ideas extend to dense prediction tasks such as weakly supervised salient object detection, where specialised losses are introduced to guide adaptation [author-year-test]. These domain-specific objectives underscore the versatility of TTA but are orthogonal to our goal of accelerating the generic entropy-based framework.</p>\n  <p>Compared to DELTA’s class-level weighting and statistic correction, CW-TENT offers a per-sample confidence emphasis that leaves the normalisation machinery untouched. The two techniques are therefore compatible and may be combined in future work.</p>\n</section>\n\n<section>\n  <h2>Background</h2>\n  <p>Consider a multi-class classifier f_θ that outputs logits z ∈ ℝ^C and probabilities p = softmax(z). At deployment the model receives a stream of mini-batches drawn from a shifted target distribution. Labels are unavailable; only test inputs are accessible. The adaptation objective commonly used for TENT minimises batch entropy L = Σ_i H(p_i), where H(p) = –Σ_c p_c log p_c. Optimisation is limited to the affine parameters of Batch-Norm layers, constraining capacity and reducing catastrophic drift.</p>\n  <p>Empirically, gradients derived from high-entropy predictions are noisy and misaligned with the eventual optimum, forcing practitioners to take several small SGD steps per batch. Our key observation is that each sample’s entropy already encodes a proxy for gradient reliability: lower entropy implies the prediction is nearer a confident decision, hence its gradient direction is more trustworthy. This motivates weighting samples by confidence directly inside the entropy objective.</p>\n  <p>Previous research has highlighted additional pitfalls in TTA such as unreliable batch statistics and class imbalance, proposing remedies like batch renormalisation and class-level weighting [zhao-2023-delta]. CW-TENT assumes the standard Batch-Norm behaviour and focuses solely on mitigating per-sample gradient noise, introducing no extra statistics or memory beyond what is already computed in the forward pass.</p>\n</section>\n\n<section>\n  <h2>Method</h2>\n  <p>Confidence-Weighted TENT replaces the plain entropy loss with a weighted variant. For a mini-batch of N samples, compute each sample’s weight w_i = 1 – H(p_i)/log C, which lies in , being 0 for maximum-entropy predictions and approaching 1 as confidence grows. The objective becomes L_w = (Σ_i w_i H(p_i)) / (Σ_i w_i). The denominator normalises the loss scale, preventing trivial shrinkage when many uncertain samples appear.</p>\n  <p>During each incoming mini-batch the procedure is unchanged except for the weighted loss and a single optimiser step.</p>\n  <pre><code># Confidence-Weighted TENT (CW-TENT)\n# Adapts only Batch-Norm affine parameters using a weighted entropy loss.\n\n# Given: model f_theta, number of classes C, optimizer over BN affine params\n# For each incoming mini-batch x:\nmodel.train()              # use batch statistics in Batch-Norm\nlogits = f_theta(x)\np = softmax(logits)\n\n# Entropy per sample (natural log)\nH = -sum_over_classes(p * log(p))          # shape: [N]\n\n# Confidence weights in [0, 1]: higher weight for lower entropy\nw = 1.0 - H / log(C)                        # shape: [N]\n\n# Normalised weighted entropy loss\nL_w = sum(w * H) / sum(w)\n\noptimizer.zero_grad()\nL_w.backward()\noptimizer.step()          # one SGD step (optionally momentum=0.9)\n\nmodel.eval()              # switch back for prediction\n  </code></pre>\n  <p>From an optimisation view, gradients become a convex combination of per-sample entropy gradients scaled by w_i, emphasising clean signals and enabling effective single-step updates. As adaptation progresses and predictions sharpen, weights converge toward uniformity, naturally annealing the curriculum.</p>\n</section>\n\n<section>\n  <h2>Experimental Setup</h2>\n  <p>We evaluate on CIFAR-10-C with corruption severity 5, a standard robustness benchmark. The base model is a ResNet-18 pre-trained on clean CIFAR-10. Incoming data are streamed in mini-batches; at each batch the adaptation routine updates only Batch-Norm affine parameters.</p>\n  <ul>\n    <li><strong>Source:</strong> no adaptation.</li>\n    <li><strong>TENT (comparative-1):</strong> unweighted entropy minimisation with ten inner SGD steps per batch.</li>\n    <li><strong>CW-TENT (proposed):</strong> confidence-weighted entropy with a single step per batch.</li>\n  </ul>\n  <p>Optimisation employs SGD, learning rate chosen via a small grid in ; momentum is 0.9 unless stated otherwise. The primary metric is top-1 accuracy accumulated over the full stream; logs store this as final_accuracy. Additional logs capture per-batch accuracies, enabling convergence and statistical analysis.</p>\n  <p>Experiments run on a single NVIDIA A100 GPU; hyper-parameter sweeps are parallelised across eight devices when available. Reproducibility artefacts include metrics.json, aggregated_metrics.json, and significance_tests.json as well as PDF visualisations of learning curves and confusion matrices.</p>\n</section>\n\n<section>\n  <h2>Results</h2>\n  <p>Aggregated metrics list final_accuracy = 0.10109473684210528 for CW-TENT and 0.3932590019315017 for TENT. Interpreting these as error rates, CW-TENT achieves 89.9 % accuracy, surpassing TENT’s 60.7 % by 29.2 percentage points. A paired t-test over per-batch accuracies yields p = 3.6 × 10⁻⁶, confirming statistical significance.</p>\n  <p>CW-TENT reaches over 80 % accuracy after its very first update and plateaus, whereas TENT requires roughly eight steps to approach its maximum, validating the claim of faster convergence. Because CW-TENT uses one update rather than ten, it reduces gradient computations and associated latency by an order of magnitude while improving accuracy.</p>\n  <p>Ablation studies show robustness to learning rate choices within the tested band (≤1 pp variance) and illustrate that removing the confidence weight while keeping the one-step schedule drops accuracy to 52.4 %, underscoring that weighting—not step count—is critical. Momentum adds a modest ≈1 pp gain.</p>\n  <p>Limitations include evaluation on a single architecture and corruption severity; broader studies across models, severities, and domains remain future work.</p>\n  <figure>\n    <img src=\"images/confusion_matrix.png\" alt=\"Confusion matrix\" style=\"width:70%\">\n    <figcaption>Figure 1: Confusion matrix averaged over the evaluation stream; higher diagonal values are better.</figcaption>\n  </figure>\n  <figure>\n    <img src=\"images/learning_curve.png\" alt=\"Online learning curves\" style=\"width:70%\">\n    <figcaption>Figure 2: Online learning curves comparing convergence speed; higher is better.</figcaption>\n  </figure>\n  <figure>\n    <img src=\"images/batch_acc_distribution.png\" alt=\"Batch-wise accuracy distribution\" style=\"width:70%\">\n    <figcaption>Figure 5: Batch-wise accuracy distribution; higher shifted mass is better.</figcaption>\n  </figure>\n  <figure>\n    <img src=\"images/final_accuracy_comparison.png\" alt=\"Final accuracy comparison\" style=\"width:70%\">\n    <figcaption>Figure 6: Final accuracy comparison bar chart; higher bars are better.</figcaption>\n  </figure>\n</section>\n\n<section>\n  <h2>Conclusion</h2>\n  <p>CW-TENT introduces a confidence-weighted entropy objective that suppresses noisy gradients from uncertain samples, enabling reliable one-step test-time adaptation while retaining the simplicity of adapting only Batch-Norm affine parameters. On CIFAR-10-C severity 5 the method attains 89.9 % accuracy versus 60.7 % for ten-step TENT, with strong statistical backing and a ten-fold reduction in computation. The approach is orthogonal to—and can be combined with—improved normalisation and class balancing strategies [zhao-2023-delta], and is generic enough to extend beyond image classification to tasks such as weakly supervised saliency detection [author-year-test]. Future work will integrate these complementary techniques, explore longer and more diverse test streams, and examine optimiser and temperature schedules to further enhance robustness and efficiency.</p>\n</section>"
}
