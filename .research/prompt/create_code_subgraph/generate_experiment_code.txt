
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path}

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0, **WandB disabled**)

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * Log ALL metrics to WandB: `wandb.log({"train_loss": 0.5, "val_acc": 0.85, "epoch": 1, ...})`
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `{results_dir}/config.yaml`
  * Retrieve experimental data from WandB API for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        metrics_df = run.history()  # pandas DataFrame with all logged metrics
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Follow naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- Pass all Hydra overrides to train.py subprocess (e.g., `wandb.mode=disabled`, `trial_mode=true`)
- In trial_mode, automatically set `wandb.mode=disabled`

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251020-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: trial_mode=true automatically disables WandB (sets wandb.mode=disabled)
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: The package is not yet production-ready.

1. pyproject.toml lists a non-existent dependency "owmegaconf" (typo).  Installing the environment will fail.
2. evaluate.py CLI does not follow the required interface.  The script expects positional arguments (./results) but the specification requires key-value syntax (results_dir=…, run_ids=…).  Current parser will treat the whole string "results_dir=./results" as the path and crash.
3. Training does not log y_true / y_pred to WandB, yet evaluate.py needs them to build confusion matrices.  Confusion-matrix generation will therefore be skipped, violating requirement 8.
4. train.py stores a single merged config at {results_dir}/config.yaml; successive runs overwrite it.  evaluate.py relies on this file to fetch WandB credentials, so later runs can corrupt earlier ones.
5. Minor: Optuna parameter injection replaces underscores with dots (OmegaConf.update), so search-space keys like "learning_rate" will be written to an unintended hierarchy ("learning.rate").

Fix these issues to satisfy all mandatory criteria.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml \u2013 root Hydra configuration\nrun: default-run-id           # overridden from CLI\nresults_dir: ./results        # overridden from CLI\ntrial_mode: false\n\nwandb:\n  entity: gengaru617-personal\n  project: 251020-test\n  mode: online               # auto-set to \"disabled\" in trial_mode\n\noptuna:\n  n_trials: 0\n  direction: maximize\n  metric: top1_accuracy\n  search_space: {}\n\nlogging:\n  log_every_n_steps: 50\n", "evaluate_py": "\"\"\"src/evaluate.py \u2013 independent evaluation \u0026 visualisation\n-----------------------------------------------------------\nThis script is executed *after* all training runs. It fetches the\nlogged metrics from WandB, stores them locally and plots comparison\nfigures.\n\nExample\n-------\nuv run python -m src.evaluate \\\n    results_dir=./results \\\n    run_ids=\u0027[\"run-1\",\"run-2\"]\u0027\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom sklearn.metrics import confusion_matrix\n\nsns.set_style(\"whitegrid\")\nplt.rcParams.update({\"figure.dpi\": 150})\n\n# ---------------------------------------------------------------------\n#                           Helper I/O\n# ---------------------------------------------------------------------\n\ndef _save_json(obj: Any, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef _save_fig(fig, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    fig.tight_layout()\n    fig.savefig(path, format=\"pdf\")\n    plt.close(fig)\n\n# ---------------------------------------------------------------------\n#                       Per-run processing\n# ---------------------------------------------------------------------\n\ndef process_single_run(run: wandb.apis.public.Run, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    df = run.history(keys=None, pandas=True)\n    df.to_json(out_dir / \"metrics.json\", orient=\"records\")\n\n    # Learning curve\n    if {\"global_step\", \"acc_after\"}.issubset(df.columns):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"global_step\"], df[\"acc_after\"], label=\"Acc after\", lw=1.5)\n        if \"acc_before\" in df.columns:\n            ax.plot(df[\"global_step\"], df[\"acc_before\"], label=\"Acc before\", lw=1.2, ls=\"--\")\n        ax.set_xlabel(\"Batch #\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.set_title(f\"Learning curve \u2013 {run.id}\")\n        ax.legend()\n        _save_fig(fig, out_dir / \"learning_curve.pdf\")\n\n    # Confusion matrix (final)\n    if {\"y_true\", \"y_pred\"}.issubset(df.columns):\n        y_true = df[\"y_true\"].dropna().astype(int)\n        y_pred = df[\"y_pred\"].dropna().astype(int)\n        if not y_true.empty:\n            cm = confusion_matrix(y_true, y_pred)\n            fig, ax = plt.subplots(figsize=(5, 4))\n            sns.heatmap(cm, ax=ax, cmap=\"Blues\", cbar=True)\n            ax.set_xlabel(\"Predicted\")\n            ax.set_ylabel(\"True\")\n            ax.set_title(f\"Confusion matrix \u2013 {run.id}\")\n            _save_fig(fig, out_dir / \"confusion_matrix.pdf\")\n\n    print(out_dir / \"metrics.json\")\n\n# ---------------------------------------------------------------------\n#                    Aggregated comparison \u0026 plots\n# ---------------------------------------------------------------------\n\ndef aggregate(runs: List[wandb.apis.public.Run], out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    rows: List[Dict[str, Any]] = []\n    for r in runs:\n        rows.append({\n            \"run_id\": r.id,\n            \"method\": r.config.get(\"method\", {}).get(\"name\", \"n/a\"),\n            \"final_acc\": r.summary.get(\"final_top1_accuracy\", None),\n        })\n    df = pd.DataFrame(rows)\n    _save_json(df.to_dict(orient=\"records\"), out_dir / \"aggregated_metrics.json\")\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_acc\", hue=\"method\", ax=ax)\n    for p in ax.patches:\n        v = p.get_height()\n        ax.annotate(f\"{v:.2%}\", (p.get_x() + p.get_width() / 2, v), ha=\"center\", va=\"bottom\", fontsize=8)\n    ax.set_ylabel(\"Top-1 Accuracy\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_title(\"Final accuracy comparison\")\n    ax.legend()\n    _save_fig(fig, out_dir / \"final_accuracy_comparison.pdf\")\n\n    print(out_dir / \"aggregated_metrics.json\")\n\n# ---------------------------------------------------------------------\n#                                CLI\n# ---------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=Path, help=\"Directory where to store artefacts\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of WandB run IDs\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_dir: Path = args.results_dir.expanduser().resolve()\n\n    # Load WandB credentials from stored config\n    with open(results_dir / \"config.yaml\") as f:\n        cfg = yaml.safe_load(f)\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n    runs: List[wandb.apis.public.Run] = []\n    for r_id in run_ids:\n        run = api.run(f\"{entity}/{project}/{r_id}\")\n        runs.append(run)\n        process_single_run(run, results_dir / r_id)\n\n    aggregate(runs, results_dir / \"comparison\")\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"src/main.py \u2013 thin wrapper that spawns `src.train` in a subprocess\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    py = sys.executable\n    cmd: List[str] = [py, \"-u\", \"-m\", \"src.train\", f\"run={cfg.run}\", f\"results_dir={cfg.results_dir}\"]\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"trial_mode=true\")\n    print(\"[main] Launching:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py \u2013 backbone creation \u0026 (CW-)TENT adapters\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport os\n\nimport torch\nimport torch.nn as nn\nimport timm\n\n__all__ = [\"create_backbone\", \"TentAdapter\", \"CWTentAdapter\"]\n\n# ---------------------------------------------------------------------\n#                        Backbone helper\n# ---------------------------------------------------------------------\n\ndef create_backbone(cfg) -\u003e nn.Module:\n    \"\"\"Loads timm model \u0026 unfreezes only BatchNorm affine params.\"\"\"\n    os.environ.setdefault(\"TORCH_HOME\", \".cache\")  # ensures weights cached locally\n    model = timm.create_model(\n        cfg.model.name,\n        pretrained=cfg.model.pretrained,\n        num_classes=cfg.model.num_classes,\n    )\n\n    # freeze everything\n    for p in model.parameters():\n        p.requires_grad_(False)\n    # activate BN affine params\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n            if m.weight is not None:\n                m.weight.requires_grad_(True)\n            if m.bias is not None:\n                m.bias.requires_grad_(True)\n    return model\n\n# ---------------------------------------------------------------------\n\n\ndef entropy_from_probs(p: torch.Tensor) -\u003e torch.Tensor:\n    return -(p * (p + 1e-8).log()).sum(1)\n\n\nclass BaseAdapter(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super().__init__()\n        self.backbone = backbone\n        self.classes = getattr(backbone, \"num_classes\", None)\n        if self.classes is None:\n            raise ValueError(\"Backbone missing num_classes attribute\")\n        self.backbone.train()\n\n    @torch.no_grad()\n    def predict(self, x: torch.Tensor) -\u003e torch.Tensor:\n        self.backbone.eval()\n        return self.backbone(x).softmax(1)\n\n\nclass TentAdapter(BaseAdapter):\n    def __init__(self, backbone: nn.Module, lr: float, momentum: float = 0.9, inner_steps: int = 1, weight_decay: float = 0.0):\n        super().__init__(backbone)\n        params = [p for p in backbone.parameters() if p.requires_grad]\n        self.optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n        self.inner_steps = inner_steps\n\n    def adapt(self, x: torch.Tensor) -\u003e float:\n        self.backbone.train()\n        last_loss = 0.0\n        for _ in range(self.inner_steps):\n            logits = self.backbone(x)\n            probs = logits.softmax(1)\n            loss = entropy_from_probs(probs).mean()\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            last_loss = loss.item()\n        self.backbone.eval()\n        return last_loss\n\n\nclass CWTentAdapter(TentAdapter):\n    def adapt(self, x: torch.Tensor) -\u003e float:  # type: ignore[override]\n        self.backbone.train()\n        logits = self.backbone(x)\n        probs = logits.softmax(1)\n        H = entropy_from_probs(probs)\n        w = 1.0 - H / math.log(self.classes)\n        loss = (w * H).sum() / w.sum()\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.backbone.eval()\n        return loss.item()\n", "preprocess_py": "\"\"\"src/preprocess.py \u2013 data loading \u0026 preprocessing utilities\"\"\"\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import List\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\ntry:\n    from robustbench.data import load_cifar10c\n    _ROBUSTBENCH = True\n    CORRUPTIONS_ALL = [\n        \"gaussian_noise\", \"shot_noise\", \"impulse_noise\", \"defocus_blur\",\n        \"glass_blur\", \"motion_blur\", \"zoom_blur\", \"snow\", \"frost\",\n        \"fog\", \"brightness\", \"contrast\", \"elastic_transform\",\n        \"pixelate\", \"jpeg_compression\",\n    ]\nexcept ImportError:  # pragma: no cover\n    _ROBUSTBENCH = False\n    CORRUPTIONS_ALL: List[str] = []\n\nIMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nIMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# ---------------------------------------------------------------------\n\n\ndef _to_tensor(arr):\n    if isinstance(arr, torch.Tensor):\n        return arr\n    return torch.from_numpy(arr).permute(0, 3, 1, 2).float().div(255.0)\n\n\ndef build_dataloader(cfg, device: torch.device):\n    name = str(cfg.dataset.name).lower()\n    batch_size = cfg.dataset.batch_size\n    severity = getattr(cfg.dataset, \"corruption_severity\", 5)\n\n    if name in {\"cifar10-c\", \"cifar-10-c\", \"cifar10c\"} and _ROBUSTBENCH:\n        c_types = cfg.dataset.corruption_types\n        if c_types in {\"all\", \"*\", None}:\n            c_types = CORRUPTIONS_ALL\n        elif isinstance(c_types, str):\n            c_types = [c_types]\n\n        xs, ys = [], []\n        for c in c_types:\n            x, y = load_cifar10c(c, severity, data_dir=\".cache/cifar10c\")\n            xs.append(_to_tensor(x))\n            ys.append(torch.from_numpy(y))\n        images = torch.cat(xs)\n        labels = torch.cat(ys).long()\n        dataset: Dataset = TensorDataset(images, labels)\n    else:\n        warnings.warn(\"RobustBench not available \u2013 using vanilla CIFAR-10 test set.\")\n        transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize((0.4914, 0.4822, 0.4465), (0.254, 0.247, 0.262)),\n        ])\n        dataset = CIFAR10(root=\".cache/cifar10\", train=False, download=True, transform=transform)\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=getattr(cfg.dataset, \"shuffle\", False),\n        num_workers=getattr(cfg.dataset, \"num_workers\", 4),\n        pin_memory=True,\n    )\n\n    def _device_iter():\n        for x, y in loader:\n            if x.max() \u003c= 1.0:  # not normalised yet\n                x = (x - IMAGENET_MEAN) / IMAGENET_STD\n            yield x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n    return _device_iter()\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"cw-tent-experiments\"\nversion = \"0.1.0\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0\"\ntorchvision = \"*\"\ntimm = \"*\"\nrobustbench = \"*\"\nhydra-core = \"*\"\nowmegaconf = \"*\"\nwandb = \"*\"\noptuna = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\npandas = \"*\"\npyyaml = \"*\"\n", "train_py": "\"\"\"src/train.py \u2013 executes ONE experiment run. Fully Hydra-driven.\n--------------------------------------------------------------------\nCLI (examples)\n--------------\n# Full run with WandB logging\nuv run python -u -m src.main run=proposed-ResNet-18\\ (11M)-CIFAR-10-C\\ (severity\\ 5) results_dir=./results\n\n# Trial run (quick CI check, WandB disabled)\nuv run python -u -m src.main run=proposed-ResNet-18\\ (11M)-CIFAR-10-C\\ (severity\\ 5) results_dir=./results trial_mode=true\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom .model import CWTentAdapter, TentAdapter, create_backbone\nfrom .preprocess import build_dataloader\n\n# ---------------------------------------------------------------------\n#                               Utilities\n# ---------------------------------------------------------------------\n\ndef seed_everything(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef get_results_dir(cfg: DictConfig) -\u003e Path:\n    out = Path(cfg.results_dir).expanduser().resolve()\n    out.mkdir(parents=True, exist_ok=True)\n    return out\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    return (pred.argmax(1) == target).float().mean().item()\n\n\n# ---------------------------------------------------------------------\n#                        Core adaptation routine\n# ---------------------------------------------------------------------\n\ndef run_adaptation(cfg: DictConfig) -\u003e Dict[str, Any]:\n    \"\"\"Sequentially adapts on the test stream. Returns final stats.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    loader = build_dataloader(cfg, device)\n\n    backbone = create_backbone(cfg).to(device)\n    method = str(cfg.method.name).upper()\n    if method == \"TENT\":\n        Adapter = TentAdapter\n    elif method in {\"CW-TENT\", \"CWTENT\", \"CWTENTADAPTER\"}:\n        Adapter = CWTentAdapter\n    else:\n        raise ValueError(f\"Unsupported method: {cfg.method.name}\")\n\n    adapter = Adapter(\n        backbone,\n        lr=cfg.training.learning_rate,\n        momentum=cfg.training.momentum,\n        inner_steps=cfg.method.inner_steps,\n        weight_decay=cfg.training.weight_decay,\n    )\n\n    total_correct, total_seen = 0, 0\n    for step, (imgs, lbls) in enumerate(loader, 1):\n        pred_before = adapter.predict(imgs)\n        acc_before = accuracy(pred_before, lbls)\n\n        loss = adapter.adapt(imgs)\n        pred_after = adapter.predict(imgs)\n        acc_after = accuracy(pred_after, lbls)\n\n        total_correct += (pred_after.argmax(1) == lbls).sum().item()\n        total_seen += lbls.size(0)\n\n        if step == 1 or step % cfg.logging.log_every_n_steps == 0:\n            log_dict = {\n                \"train_loss\": loss,\n                \"acc_before\": acc_before,\n                \"acc_after\": acc_after,\n                \"global_step\": step,\n                \"epoch\": 1,\n            }\n            if cfg.wandb.mode != \"disabled\":\n                wandb.log(log_dict)\n            print(json.dumps(log_dict, separators=(\",\", \":\")))\n\n        if cfg.get(\"trial_mode\", False) and step \u003e= cfg.trial_limit_batches:\n            break\n\n    final_acc = total_correct / max(1, total_seen)\n    if cfg.wandb.mode != \"disabled\":\n        wandb.log({\"final_top1_accuracy\": final_acc})\n    return {\"top1_accuracy\": final_acc, \"steps\": step}\n\n\n# ---------------------------------------------------------------------\n#                               Hydra entry\n# ---------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg_cli: DictConfig) -\u003e None:  # noqa: C901  (complexity acceptable here)\n    # ------------------ merge CLI cfg with run-specific YAML -----------------\n    run_yaml = Path(__file__).with_suffix(\"\")  # src/train\n    run_yaml = run_yaml.parents[1] / \"config\" / \"run\" / f\"{cfg_cli.run}.yaml\"\n    if not run_yaml.exists():\n        raise FileNotFoundError(f\"Run-config not found: {run_yaml}\")\n    cfg_run = OmegaConf.load(run_yaml)\n    cfg: DictConfig = OmegaConf.merge(cfg_run, cfg_cli)\n\n    # ----------------------------- trial-mode tweaks -------------------------\n    if cfg.get(\"trial_mode\", False):\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.trial_limit_batches = 2\n    else:\n        cfg.trial_limit_batches = int(1e12)\n\n    # ----------------------- persist merged config ---------------------------\n    results_dir = get_results_dir(cfg)\n    with open(results_dir / \"config.yaml\", \"w\") as f:\n        f.write(OmegaConf.to_yaml(cfg))\n\n    # --------------------------- WandB initialisation ------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=str(cfg.run),\n            mode=cfg.wandb.mode,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n\n    seed_everything(42)\n\n    # ----------------------- Optuna hyper-parameter search -------------------\n    if cfg.optuna.n_trials \u003e 0:\n        def objective(trial: optuna.Trial):\n            # sample \u0026 inject params\n            for name, pspec in cfg.optuna.search_space.items():\n                if pspec.type == \"loguniform\":\n                    val = trial.suggest_float(name, pspec.low, pspec.high, log=True)\n                elif pspec.type == \"uniform\":\n                    val = trial.suggest_float(name, pspec.low, pspec.high)\n                elif pspec.type == \"int\":\n                    val = trial.suggest_int(name, pspec.low, pspec.high)\n                elif pspec.type == \"categorical\":\n                    val = trial.suggest_categorical(name, pspec.choices)\n                else:\n                    raise ValueError(f\"Unknown space type: {pspec.type}\")\n                OmegaConf.update(cfg, name.replace(\"_\", \".\"), val, merge=False)\n            res = run_adaptation(cfg)\n            return res[cfg.optuna.metric]\n\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(objective, n_trials=int(cfg.optuna.n_trials))\n\n        best = study.best_params\n        for k, v in best.items():\n            OmegaConf.update(cfg, k.replace(\"_\", \".\"), v, merge=False)\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\"optuna_best_value\": study.best_value})\n            wandb.run.summary[\"optuna_best_params\"] = best\n\n    # -------------------------- final run w/ best params ---------------------\n    stats = run_adaptation(cfg)\n\n    with open(results_dir / \"run_stats.json\", \"w\") as f:\n        json.dump(stats, f, indent=2)\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Most popular test-time adaptation (TTA) methods such as TENT adapt only BatchNorm affine parameters by minimizing prediction entropy on the incoming test stream. Although very effective, they often require 3-10 gradient steps for each incoming mini-batch to reach their best performance. The slow convergence mainly comes from noisy gradients produced by low-confidence (high-entropy) samples that dominate early in adaptation. A simple mechanism to down-weight these uncertain samples during optimization is missing.",
    "Methods": "Confidence-Weighted Entropy Minimization (CW-TENT).\n1. Keep the original TENT objective L = Σ_i H(p_i) where H is entropy.\n2. Introduce a scalar weight per sample w_i = 1 – H(p_i)/log(C)  (ranges in [0,1]; C = #classes).\n3. Replace the loss with the weighted variant  L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\n4. Use the same SGD update of BatchNorm affine parameters, but with a single gradient step per mini-batch (optionally with momentum=0.9).\nTheoretical intuition: high-confidence samples (low entropy) are already close to the target domain optimum and provide reliable gradients; amplifying their contribution yields a cleaner gradient direction, allowing larger learning rate or fewer steps, hence faster convergence.",
    "Experimental Setup": "Dataset: CIFAR-10-C with 15 corruption types, severity 5 (standard TTA benchmark).\nModel: Pre-trained ResNet-18.\nBaselines: 1) Source model (no adaptation). 2) Original TENT (default 10 inner steps). 3) CW-TENT (1 inner step).\nMetric: Top-1 accuracy after processing the full test stream; also accuracy after first K (e.g., 1, 3) adaptation steps to measure convergence speed.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef entropy(p):\n    return -(p * p.log()).sum(1)\n\nclass CWTentAdapter:\n    def __init__(self, model, lr=1e-3, momentum=0.9):\n        self.model = model.eval()\n        self.model.requires_grad_(False)\n        # enable gradients for affine BN params only\n        for m in model.modules():\n            if isinstance(m, torch.nn.BatchNorm2d):\n                m.weight.requires_grad_(True)\n                m.bias.requires_grad_(True)\n        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n\n    @torch.no_grad()\n    def predict(self, x):\n        return self.model(x).softmax(1)\n\n    def adapt(self, x):\n        self.model.train()        # BN uses batch statistics\n        self.optimizer.zero_grad()\n        logits = self.model(x)\n        probs = logits.softmax(1)\n        H = entropy(probs)\n        w = 1 - H / torch.log(torch.tensor(probs.size(1), device=x.device))\n        loss = (w * H).sum() / w.sum()\n        loss.backward()\n        self.optimizer.step()\n        self.model.eval()\n        return probs.detach()\n",
    "Expected Result": "With only one gradient step per mini-batch, CW-TENT is expected to match or surpass the accuracy of TENT that uses 5–10 steps. For example on CIFAR-10-C severity 5: Source 58.7%, TENT (10 steps) 71.0%, CW-TENT (1 step) ≈71.5%. Convergence plot should show CW-TENT reaching peak accuracy after the first step, whereas TENT needs several.",
    "Expected Conclusion": "A tiny modification—confidence-weighted entropy—suppresses noisy gradients from uncertain samples and lets TENT converge in a single step. The change is trivial to implement (four extra lines) yet meaningfully cuts computation and latency at test time, making TTA more practical for real-time deployment."
}

# Experimental Design
- Summary: The experiment demonstrates that Confidence-Weighted TENT (CW-TENT) enables fast, one-step test-time adaptation by down-weighting high-entropy (low-confidence) samples in the classic entropy-minimisation objective used by TENT. A pre-trained ResNet-18 is sequentially exposed to the CIFAR-10-C corruption stream (severity-5). At every incoming mini-batch the model first produces predictions, then performs one SGD update on BatchNorm affine parameters using the confidence-weighted entropy loss. Performance is evaluated as the stream proceeds and compared against: (1) the frozen source model (no adaptation) and (2) the original TENT that applies 10 inner optimisation steps. All runs share identical data order, initial weights and hardware (8×H200, 144 GB each). Hyper-parameter sweeps on learning-rate, momentum and weight-decay are carried out with random search (30 trials) on a held-out corruption set to obtain fair settings for both CW-TENT and the baseline.
- Evaluation metrics: ['Top-1 Accuracy', 'Average Accuracy After First Adaptation Step']

# Experiment Runs

- Run ID: proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5)
  Method: proposed
  Model: ResNet-18 (11M)
  Dataset: CIFAR-10-C (severity 5)
  Config File: config/run/proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5).yaml
  
  Config Content:
  ```yaml
  run_id: proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5)
method:
  name: CW-TENT
  description: Confidence-Weighted Entropy Minimization for one-step test-time adaptation
  inner_steps: 1
  confidence_weighting: true
model:
  name: resnet18.a1_in1k
  pretrained: true
  num_params: 11000000
  freeze_except:
    - BatchNorm2d.weight
    - BatchNorm2d.bias
  checkpoint: timm/resnet18.a1_in1k
  input_size: 3x32x32
  num_classes: 10
dataset:
  name: CIFAR-10-C
  corruption_severity: 5
  corruption_types: all  # 15 standard corruptions
  split: test
  batch_size: 64
  num_workers: 8
  shuffle: false
training:
  optimizer: sgd
  learning_rate: 0.001
  momentum: 0.9
  weight_decay: 0.0
  grad_clip: null
  precision: fp32
  accumulate_grad_batches: 1
  epochs: 1  # adaptation is online, one pass over stream
hardware:
  gpus: 8
  gpu_type: A100
  vram_per_gpu: 80GB
logging:
  log_every_n_steps: 20
  checkpoint_interval: null
  tensorboard: true
optuna:
  n_trials: 30
  direction: maximize
  metric: top1_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    momentum:
      type: uniform
      low: 0.0
      high: 0.99
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
    batch_size:
      type: categorical
      choices: [32, 64, 128]

  ```
  

- Run ID: comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5)
  Method: comparative-1
  Model: ResNet-18 (11M)
  Dataset: CIFAR-10-C (severity 5)
  Config File: config/run/comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5).yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5)
method:
  name: TENT
  description: Original entropy minimization test-time adaptation baseline
  inner_steps: 10
  confidence_weighting: false
model:
  name: resnet18.a1_in1k
  pretrained: true
  num_params: 11000000
  freeze_except:
    - BatchNorm2d.weight
    - BatchNorm2d.bias
  checkpoint: timm/resnet18.a1_in1k
  input_size: 3x32x32
  num_classes: 10
dataset:
  name: CIFAR-10-C
  corruption_severity: 5
  corruption_types: all
  split: test
  batch_size: 64
  num_workers: 8
  shuffle: false
training:
  optimizer: sgd
  learning_rate: 0.001
  momentum: 0.9
  weight_decay: 0.0
  grad_clip: null
  precision: fp32
  accumulate_grad_batches: 1
  epochs: 1  # adaptation is online
hardware:
  gpus: 8
  gpu_type: A100
  vram_per_gpu: 80GB
logging:
  log_every_n_steps: 20
  checkpoint_interval: null
  tensorboard: true
optuna:
  n_trials: 30
  direction: maximize
  metric: top1_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    momentum:
      type: uniform
      low: 0.0
      high: 0.99
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
    inner_steps:
      type: int
      low: 3
      high: 10
    batch_size:
      type: categorical
      choices: [32, 64, 128]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: timm/resnet18.a1_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet18.a1_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 64, 56, 56])
    #  torch.Size([1, 128, 28, 28])
    #  torch.Size([1, 256, 14, 14])
    #  torch.Size([1, 512, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 512, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor



**HuggingFace Datasets:**



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py – executes ONE experiment run, Hydra-driven and WandB-integrated.\n--------------------------------------------------------------------------\nSee README or workflow docs for launcher syntax.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport random\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom .model import CWTentAdapter, TentAdapter, create_backbone\nfrom .preprocess import build_dataloader\n\n# -------------------------------------------------------------------------\n#                              Helper utils\n# -------------------------------------------------------------------------\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef get_root_results_dir(cfg: DictConfig) -> Path:\n    out = Path(cfg.results_dir).expanduser().resolve()\n    out.mkdir(parents=True, exist_ok=True)\n    return out\n\n\ndef slugify(value: str) -> str:\n    value = value.strip().replace(\" \", \"_\")\n    value = re.sub(r\"[^A-Za-z0-9_.-]\", \"_\", value)\n    return value\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    return (pred.argmax(1) == target).float().mean().item()\n\n\n# -------------------------------------------------------------------------\n#                        Core adaptation loop\n# -------------------------------------------------------------------------\n\ndef run_adaptation(cfg: DictConfig) -> Dict[str, Any]:\n    \"\"\"Perform sequential adaptation, return final statistics.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dataloader = build_dataloader(cfg, device)\n\n    # ------------------------------------------------------------------\n    # Build model + adapter\n    # ------------------------------------------------------------------\n    backbone = create_backbone(cfg).to(device)\n    method_name = str(cfg.method.name).upper()\n    if method_name in {\"TENT\"}:\n        AdapterCls = TentAdapter\n    elif method_name in {\"CW-TENT\", \"CWTENT\", \"CWTENTADAPTER\"}:\n        AdapterCls = CWTentAdapter\n    else:\n        raise ValueError(f\"Unsupported method: {cfg.method.name}\")\n\n    adapter = AdapterCls(\n        backbone,\n        lr=cfg.training.learning_rate,\n        momentum=cfg.training.momentum,\n        inner_steps=cfg.method.inner_steps,\n        weight_decay=cfg.training.weight_decay,\n    )\n\n    total_correct, total_seen = 0, 0\n    for step, (imgs, lbls) in enumerate(dataloader, 1):\n        # prediction BEFORE adaptation\n        pred_before = adapter.predict(imgs)\n        acc_before = accuracy(pred_before, lbls)\n\n        # one/ multiple adaptation step(s)\n        loss = adapter.adapt(imgs)\n\n        # prediction AFTER adaptation\n        pred_after = adapter.predict(imgs)\n        acc_after = accuracy(pred_after, lbls)\n\n        total_correct += (pred_after.argmax(1) == lbls).sum().item()\n        total_seen += lbls.size(0)\n\n        # WandB / console logging ------------------------------------------------\n        if step == 1 or step % cfg.logging.log_every_n_steps == 0:\n            log_payload = {\n                \"train_loss\": loss,\n                \"acc_before\": acc_before,\n                \"acc_after\": acc_after,\n                \"y_true\": lbls.cpu().tolist(),\n                \"y_pred\": pred_after.argmax(1).cpu().tolist(),\n                \"global_step\": step,\n                \"epoch\": 1,\n            }\n            if cfg.wandb.mode != \"disabled\":\n                wandb.log(log_payload)\n            # lightweight console output for CI / human glance\n            print(json.dumps({k: v for k, v in log_payload.items() if k not in {\"y_true\", \"y_pred\"}}, separators=(\",\", \":\")))\n\n        # ----------------------------------------------------------------- CI/trial early exit\n        if cfg.get(\"trial_mode\", False) and step >= cfg.trial_limit_batches:\n            break\n\n    final_acc = total_correct / max(1, total_seen)\n    if cfg.wandb.mode != \"disabled\":\n        wandb.log({\"final_top1_accuracy\": final_acc})\n        wandb.run.summary[\"final_top1_accuracy\"] = final_acc\n    return {\"top1_accuracy\": final_acc, \"stream_steps\": step}\n\n\n# -------------------------------------------------------------------------\n#                          Optuna objective helper\n# -------------------------------------------------------------------------\n\ndef build_objective(cfg_base: DictConfig):\n    \"\"\"Return a closure that Optuna can optimise.\"\"\"\n    # Mapping from flat parameter names -> config dotted paths\n    name2path = {\n        \"learning_rate\": \"training.learning_rate\",\n        \"momentum\": \"training.momentum\",\n        \"weight_decay\": \"training.weight_decay\",\n        \"batch_size\": \"dataset.batch_size\",\n        \"inner_steps\": \"method.inner_steps\",\n    }\n\n    def objective(trial: optuna.Trial):\n        cfg = OmegaConf.create(OmegaConf.to_container(cfg_base, resolve=True))  # deep copy\n        for param_name, spec in cfg_base.optuna.search_space.items():\n            if spec.type == \"loguniform\":\n                val = trial.suggest_float(param_name, spec.low, spec.high, log=True)\n            elif spec.type == \"uniform\":\n                val = trial.suggest_float(param_name, spec.low, spec.high)\n            elif spec.type == \"int\":\n                val = trial.suggest_int(param_name, spec.low, spec.high)\n            elif spec.type == \"categorical\":\n                val = trial.suggest_categorical(param_name, spec.choices)\n            else:\n                raise ValueError(f\"Unknown search_space type: {spec.type}\")\n\n            target_path = name2path.get(param_name, param_name)\n            OmegaConf.update(cfg, target_path, val, merge=False)\n        result = run_adaptation(cfg)\n        return result[cfg.optuna.metric]\n\n    return objective\n\n\n# -------------------------------------------------------------------------\n#                           Main – Hydra entry\n# -------------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg_cli: DictConfig) -> None:  # noqa: C901  (complexity acceptable)\n    # ------------------------------------------------------------------\n    # Merge root config with run-specific YAML\n    # ------------------------------------------------------------------\n    run_cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"run\" / f\"{cfg_cli.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run-config not found: {run_cfg_path}\")\n    cfg_run = OmegaConf.load(run_cfg_path)\n    cfg: DictConfig = OmegaConf.merge(cfg_run, cfg_cli)\n\n    # ------------------------------------------------------------------\n    # Trial-mode tweaks (short, WandB off, limited batches)\n    # ------------------------------------------------------------------\n    if cfg.get(\"trial_mode\", False):\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.trial_limit_batches = 2\n    else:\n        cfg.trial_limit_batches = int(1e12)\n\n    # ------------------------------------------------------------------\n    # Prepare filesystem hierarchy\n    # ------------------------------------------------------------------\n    root_out = get_root_results_dir(cfg)\n    run_dir = root_out / slugify(str(cfg.run.run_id))\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Store per-run config to its own folder\n    with open(run_dir / \"config.yaml\", \"w\") as fh:\n        fh.write(OmegaConf.to_yaml(cfg))\n\n    # Store central WandB config once (to avoid overwriting earlier files)\n    central_cfg_file = root_out / \"wandb_config.yaml\"\n    if not central_cfg_file.exists():\n        with open(central_cfg_file, \"w\") as fh:\n            yaml_subset = {\"wandb\": OmegaConf.to_container(cfg.wandb, resolve=True)}\n            fh.write(OmegaConf.to_yaml(yaml_subset))\n\n    # ------------------------------------------------------------------\n    # WandB initialisation\n    # ------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=str(cfg.run.run_id),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n\n    # ------------------------------------------------------------------\n    seed_everything(42)\n\n    # ------------------------------------------------------------------\n    # (Optional) hyper-parameter optimisation\n    # ------------------------------------------------------------------\n    if cfg.optuna.n_trials and int(cfg.optuna.n_trials) > 0:\n        objective = build_objective(cfg)\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(objective, n_trials=int(cfg.optuna.n_trials))\n\n        # Apply best params to cfg for final run\n        for k, v in study.best_params.items():\n            target_path = {\n                \"learning_rate\": \"training.learning_rate\",\n                \"momentum\": \"training.momentum\",\n                \"weight_decay\": \"training.weight_decay\",\n                \"batch_size\": \"dataset.batch_size\",\n                \"inner_steps\": \"method.inner_steps\",\n            }.get(k, k)\n            OmegaConf.update(cfg, target_path, v, merge=False)\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\"optuna_best_value\": study.best_value})\n            wandb.run.summary[\"optuna_best_params\"] = study.best_params\n\n    # ------------------------------------------------------------------\n    # Final run with (possibly tuned) settings\n    # ------------------------------------------------------------------\n    final_stats = run_adaptation(cfg)\n    with open(run_dir / \"run_stats.json\", \"w\") as f:\n        json.dump(final_stats, f, indent=2)\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py – independent evaluation/visualisation module\n-------------------------------------------------------------------\nUsage (CLI):\n    uv run python -m src.evaluate results_dir=./results \\\n                                     run_ids='[\"run-1\", \"run-2\"]'\n\nThe script parses *key=value* command-line arguments for maximal\nflexibility (matches workflow specification).\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom sklearn.metrics import confusion_matrix\n\nsns.set_style(\"whitegrid\")\nplt.rcParams.update({\"figure.dpi\": 150})\n\n# -------------------------------------------------------------------------\n#                       Helper I/O utilities\n# -------------------------------------------------------------------------\n\ndef _save_json(obj: Any, path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as fh:\n        json.dump(obj, fh, indent=2)\n\n\ndef _save_fig(fig, path: Path) -> None:  # noqa: ANN001 – matplotlib fig type\n    path.parent.mkdir(parents=True, exist_ok=True)\n    fig.tight_layout()\n    fig.savefig(path, format=\"pdf\")\n    plt.close(fig)\n\n\n# -------------------------------------------------------------------------\n#                         CLI parsing (key=value)\n# -------------------------------------------------------------------------\n\ndef _parse_cli(argv: List[str]):\n    args: Dict[str, str] = {}\n    for token in argv[1:]:  # skip script name\n        if \"=\" not in token:\n            raise RuntimeError(f\"Unrecognised arg '{token}'. Expected key=value pair.\")\n        key, value = token.split(\"=\", 1)\n        args[key] = value\n    if {\"results_dir\", \"run_ids\"} - args.keys():\n        missing = {\"results_dir\", \"run_ids\"} - args.keys()\n        raise RuntimeError(f\"Missing CLI arguments: {missing}\")\n    return args\n\n\n# -------------------------------------------------------------------------\n#                          WandB → local processing\n# -------------------------------------------------------------------------\n\ndef _flatten_column(series):\n    \"\"\"Flatten list entries in a pandas Series into one long list.\"\"\"\n    out: List[int] = []\n    for item in series.dropna():\n        if isinstance(item, list):\n            out.extend(item)\n        else:\n            out.append(int(item))\n    return out\n\n\ndef process_single_run(run: wandb.apis.public.Run, out_dir: Path) -> None:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    history_df: pd.DataFrame = run.history(keys=None, pandas=True)\n    history_df.to_json(out_dir / \"metrics.json\", orient=\"records\")\n\n    # ---------------------------- Learning curve ----------------------\n    if {\"global_step\", \"acc_after\"}.issubset(history_df.columns):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(history_df[\"global_step\"], history_df[\"acc_after\"], label=\"Acc after\", lw=2)\n        if \"acc_before\" in history_df.columns:\n            ax.plot(history_df[\"global_step\"], history_df[\"acc_before\"], label=\"Acc before\", lw=1.5, ls=\"--\")\n        ax.set_xlabel(\"Batch #\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.set_title(f\"Learning-curve – {run.id}\")\n        ax.legend()\n        _save_fig(fig, out_dir / \"learning_curve.pdf\")\n\n    # ---------------------------- Confusion matrix --------------------\n    if {\"y_true\", \"y_pred\"}.issubset(history_df.columns):\n        y_true_flat = _flatten_column(history_df[\"y_true\"])\n        y_pred_flat = _flatten_column(history_df[\"y_pred\"])\n        if y_true_flat:\n            cm = confusion_matrix(y_true_flat, y_pred_flat)\n            fig, ax = plt.subplots(figsize=(5, 4))\n            sns.heatmap(cm, annot=False, cmap=\"Blues\", ax=ax, cbar=True)\n            ax.set_xlabel(\"Predicted\")\n            ax.set_ylabel(\"True\")\n            ax.set_title(f\"Confusion-matrix – {run.id}\")\n            _save_fig(fig, out_dir / \"confusion_matrix.pdf\")\n\n    print(out_dir / \"metrics.json\")\n\n\n# -------------------------------------------------------------------------\n#                     Aggregate across multiple runs\n# -------------------------------------------------------------------------\n\ndef aggregate(runs: List[wandb.apis.public.Run], out_dir: Path) -> None:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    rows: List[Dict[str, Any]] = []\n    for r in runs:\n        rows.append({\n            \"run_id\": r.id,\n            \"method\": r.config.get(\"method\", {}).get(\"name\", \"n/a\"),\n            \"final_acc\": r.summary.get(\"final_top1_accuracy\", None),\n        })\n    agg_df = pd.DataFrame(rows)\n    _save_json(agg_df.to_dict(orient=\"records\"), out_dir / \"aggregated_metrics.json\")\n\n    # Bar-plot comparison\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=agg_df, x=\"run_id\", y=\"final_acc\", hue=\"method\", ax=ax)\n    for bar in ax.patches:\n        height = bar.get_height()\n        ax.annotate(f\"{height:.1%}\", (bar.get_x() + bar.get_width() / 2, height),\n                    ha=\"center\", va=\"bottom\", fontsize=8)\n    ax.set_ylabel(\"Top-1 accuracy\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_title(\"Final accuracy across runs\")\n    ax.legend()\n    _save_fig(fig, out_dir / \"final_accuracy_comparison.pdf\")\n\n    print(out_dir / \"aggregated_metrics.json\")\n\n\n# -------------------------------------------------------------------------\n#                               main()\n# -------------------------------------------------------------------------\n\ndef main() -> None:\n    cli_args = _parse_cli(sys.argv)\n    results_dir = Path(cli_args[\"results_dir\"]).expanduser().resolve()\n    run_ids: List[str] = json.loads(cli_args[\"run_ids\"])\n\n    # Load WandB credentials (entity/project)\n    with open(results_dir / \"wandb_config.yaml\", \"r\") as fh:\n        wandb_cfg = yaml.safe_load(fh)[\"wandb\"]\n    entity, project = wandb_cfg[\"entity\"], wandb_cfg[\"project\"]\n\n    api = wandb.Api()\n    runs: List[wandb.apis.public.Run] = []\n    for r_id in run_ids:\n        w_run = api.run(f\"{entity}/{project}/{r_id}\")\n        runs.append(w_run)\n        process_single_run(w_run, results_dir / r_id)\n\n    aggregate(runs, results_dir / \"comparison\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py – dataset loading & preprocessing utilities.\"\"\"\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import Iterator, List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\ntry:\n    from robustbench.data import load_cifar10c\n\n    _ROBUSTBENCH_AVAILABLE = True\n    CORRUPTIONS_ALL = [\n        \"gaussian_noise\",\n        \"shot_noise\",\n        \"impulse_noise\",\n        \"defocus_blur\",\n        \"glass_blur\",\n        \"motion_blur\",\n        \"zoom_blur\",\n        \"snow\",\n        \"frost\",\n        \"fog\",\n        \"brightness\",\n        \"contrast\",\n        \"elastic_transform\",\n        \"pixelate\",\n        \"jpeg_compression\",\n    ]\nexcept ImportError:  # pragma: no-cover\n    _ROBUSTBENCH_AVAILABLE = False\n    CORRUPTIONS_ALL: List[str] = []\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\n# -------------------------------------------------------------------------\n#                       Loader builder (returns iterator)\n# -------------------------------------------------------------------------\n\ndef _np_to_tensor(arr):\n    return torch.from_numpy(arr).permute(0, 3, 1, 2).float().div(255.0)\n\n\ndef build_dataloader(cfg, device: torch.device) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n    name = str(cfg.dataset.name).lower()\n    batch_size = int(cfg.dataset.batch_size)\n    severity = int(getattr(cfg.dataset, \"corruption_severity\", 5))\n\n    # ---------------------------------------------------- CIFAR-10-C via RobustBench\n    if \"cifar\" in name and \"c\" in name and _ROBUSTBENCH_AVAILABLE:\n        corruption_types = cfg.dataset.corruption_types\n        if corruption_types in {\"all\", \"*\", None}:\n            corruption_types = CORRUPTIONS_ALL\n        elif isinstance(corruption_types, str):\n            corruption_types = [corruption_types]\n\n        xs, ys = [], []\n        for c in corruption_types:\n            x_np, y_np = load_cifar10c(c, severity, data_dir=\".cache/cifar10c\")\n            xs.append(_np_to_tensor(x_np))\n            ys.append(torch.from_numpy(y_np))\n        images = torch.cat(xs, dim=0)\n        labels = torch.cat(ys).long()\n        dataset: Dataset = TensorDataset(images, labels)\n    else:\n        # ---------------------------------------------- fallback to vanilla CIFAR-10 test set\n        if not _ROBUSTBENCH_AVAILABLE:\n            warnings.warn(\"RobustBench not found – using standard CIFAR-10 test set.\")\n        transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n        ])\n        dataset = CIFAR10(root=\".cache/cifar10\", train=False, download=True, transform=transform)\n\n    # ---------------------------------------------------- DataLoader\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=getattr(cfg.dataset, \"shuffle\", False),\n        num_workers=getattr(cfg.dataset, \"num_workers\", 4),\n        pin_memory=True,\n        drop_last=False,\n    )\n\n    # Iterator that yields tensors already on `device`\n    def _device_iter():\n        for x, y in loader:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            # ensure normalisation for CIFAR10-C which we loaded as raw 0-1\n            if x.max() <= 1.0:\n                x = T.Normalize(IMAGENET_MEAN, IMAGENET_STD)(x)\n            yield x, y\n\n    return _device_iter()\n",
    "model_py": "\"\"\"src/model.py – backbone creation & TTA adapters (TENT / CW-TENT).\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport os\n\nimport timm\nimport torch\nimport torch.nn as nn\n\n__all__ = [\n    \"create_backbone\",\n    \"TentAdapter\",\n    \"CWTentAdapter\",\n]\n\n# -------------------------------------------------------------------------\n#                         Backbone helper function\n# -------------------------------------------------------------------------\n\ndef create_backbone(cfg) -> nn.Module:  # noqa: ANN001 – cfg is Hydra object\n    \"\"\"Create image model via timm & freeze all but BN-affine params.\"\"\"\n    os.environ.setdefault(\"TORCH_HOME\", \".cache\")  # ensure local cache for weights\n    model = timm.create_model(\n        cfg.model.name,\n        pretrained=bool(cfg.model.pretrained),\n        num_classes=int(cfg.model.num_classes),\n    )\n\n    # Freeze all params\n    for p in model.parameters():\n        p.requires_grad_(False)\n    # Un-freeze BN affine params (weight & bias)\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n            if m.weight is not None:\n                m.weight.requires_grad_(True)\n            if m.bias is not None:\n                m.bias.requires_grad_(True)\n    model.eval()\n    return model\n\n# -------------------------------------------------------------------------\n#                   Entropy utility (numerically stable)\n# -------------------------------------------------------------------------\n\ndef entropy_from_probs(p: torch.Tensor) -> torch.Tensor:\n    return -(p * (p + 1e-8).log()).sum(1)\n\n\n# -------------------------------------------------------------------------\n#                       Base adapter class (common)\n# -------------------------------------------------------------------------\n\nclass _BaseAdapter(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super().__init__()\n        self.backbone = backbone\n        self.classes = getattr(backbone, \"num_classes\", None)\n        if self.classes is None:\n            raise ValueError(\"Backbone missing 'num_classes' attribute.\")\n        self.backbone.train()  # BN layers use batch stats during adaptation\n\n    @torch.no_grad()\n    def predict(self, x: torch.Tensor) -> torch.Tensor:\n        self.backbone.eval()\n        return self.backbone(x).softmax(1)\n\n\n# -------------------------------------------------------------------------\n#                          Original TENT adapter\n# -------------------------------------------------------------------------\n\nclass TentAdapter(_BaseAdapter):\n    def __init__(\n        self,\n        backbone: nn.Module,\n        lr: float,\n        momentum: float = 0.9,\n        inner_steps: int = 1,\n        weight_decay: float = 0.0,\n    ) -> None:\n        super().__init__(backbone)\n        params = [p for p in backbone.parameters() if p.requires_grad]\n        self.optimizer = torch.optim.SGD(\n            params, lr=lr, momentum=momentum, weight_decay=weight_decay\n        )\n        self.inner_steps = int(inner_steps)\n\n    def adapt(self, x: torch.Tensor) -> float:  # returns last loss value\n        self.backbone.train()\n        last_loss = 0.0\n        for _ in range(self.inner_steps):\n            logits = self.backbone(x)\n            probs = logits.softmax(1)\n            loss = entropy_from_probs(probs).mean()\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            last_loss = float(loss.item())\n        self.backbone.eval()\n        return last_loss\n\n\n# -------------------------------------------------------------------------\n#                       Confidence-Weighted TENT adapter\n# -------------------------------------------------------------------------\n\nclass CWTentAdapter(TentAdapter):\n    def adapt(self, x: torch.Tensor) -> float:  # override\n        self.backbone.train()\n        logits = self.backbone(x)\n        probs = logits.softmax(1)\n        H = entropy_from_probs(probs)\n        w = 1.0 - H / math.log(self.classes)\n        loss = (w * H).sum() / w.sum()\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.backbone.eval()\n        return float(loss.item())\n",
    "main_py": "\"\"\"src/main.py – thin orchestrator that spawns `src.train` as subprocess.\nIt exists to keep Hydra overrides independent of the heavy training script.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:  # noqa: D401 – simple description ok\n    py_exec = sys.executable\n    cmd: List[str] = [\n        py_exec,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"trial_mode=true\")\n    # propagate any additional overrides passed to main directly\n    for key, value in cfg.items():\n        if key not in {\"run\", \"results_dir\", \"trial_mode\", \"wandb\", \"optuna\", \"logging\"}:\n            cmd.append(f\"{key}={value}\")\n    print(\"[main] Launching subprocess:\\n \", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"cw-tent-experiments\"\nversion = \"0.2.0\"\n\n[project.dependencies]\ntorch = \">=2.0\"\ntorchvision = \"*\"\ntimm = \"*\"\nrobustbench = \"*\"  # optional but useful – falls back gracefully if missing\nhydra-core = \"*\"\nomegaconf = \"*\"\nwandb = \"*\"\noptuna = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\npandas = \"*\"\npyyaml = \"*\"\n",
    "config_yaml": "# config/config.yaml – root Hydra configuration\nauthor: cw-tent\nrun: default-run-id   # override from CLI\nresults_dir: ./results  # override from CLI\ntrial_mode: false\n\nwandb:\n  entity: gengaru617-personal\n  project: 251020-test\n  mode: online          # automatically set to \"disabled\" in trial_mode\n\noptuna:\n  n_trials: 0           # can be overridden by run-specific file\n  direction: maximize\n  metric: top1_accuracy\n  search_space: {}\n\nlogging:\n  log_every_n_steps: 50\n"
}
