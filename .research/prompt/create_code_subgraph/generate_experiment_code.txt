
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path}

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0, **WandB disabled**)

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * Log ALL metrics to WandB: `wandb.log({"train_loss": 0.5, "val_acc": 0.85, "epoch": 1, ...})`
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `{results_dir}/config.yaml`
  * Retrieve experimental data from WandB API for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        metrics_df = run.history()  # pandas DataFrame with all logged metrics
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Follow naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- Pass all Hydra overrides to train.py subprocess (e.g., `wandb.mode=disabled`, `trial_mode=true`)
- In trial_mode, automatically set `wandb.mode=disabled`

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251020-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: trial_mode=true automatically disables WandB (sets wandb.mode=disabled)
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Configuration hierarchy is inconsistent with the code: run-specific YAML files are loaded via the `run=` Hydra group, therefore their keys (model, dataset, training, optuna, etc.) live under `cfg.run.*`.  However every module (train.py, main.py, etc.) accesses them at the root level (`cfg.training.*`, `cfg.model.*`, `cfg.dataset.*`).  At execution time these attributes are empty / missing, causing AttributeError (e.g. `cfg.training.learning_rate`) and rendering the whole pipeline non-functional.  Align the code or the config structure (move sections out of the “run” group or reference `cfg.run.training`, `cfg.run.model`, …) before the experiment can run.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# ~~~~~~~~~~~~~~~~~~~\n# Global Hydra configuration shared by *all* runs.\n\ndefaults:\n  - _self_\n  - run: null   # injected via CLI \u2013 e.g.  run=proposed-ResNet-18\n\n#####################################################################\n# Global toggles\n#####################################################################\n\ntrial_mode: false\nresults_dir: ./results\nseed: 42\n\n#####################################################################\n# WandB (auto-disabled in trial-mode by main.py)\n#####################################################################\n\nwandb:\n  entity: gengaru617-personal\n  project: 251020-test\n  mode: online\n\n#####################################################################\n# Optuna \u2013 default values (can be overridden per-run YAML)\n#####################################################################\n\noptuna:\n  enabled: false\n  n_trials: 1          # 1 \u2192 effectively disabled search\n  direction: maximize\n  metric: final_top1_accuracy\n  search_space: {}\n\n#####################################################################\n# Place-holders (populated by run-specific YAML)\n#####################################################################\nrun:\n  run_id: dummy\n  method: baseline\nmodel: {}\ndataset: {}\ntraining: {}\nconfig: {}\n\n#####################################################################\n# Hydra internals\n#####################################################################\n\nhydra:\n  run:\n    dir: ./outputs/${now:%Y-%m-%d}_${now:%H-%M-%S}_${run.run_id}\n  job_logging:\n    version: 1\n    root:\n      level: INFO\n  hydra_logging:\n    version: 1\n    root:\n      level: INFO\n", "evaluate_py": "# src/evaluate.py\n\"\"\"Independent evaluation \u0026 visualisation script (unchanged \u2013 already compliant).\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom scipy import stats\n\nsns.set_style(\"whitegrid\")\n\n################################################################################\n# CLI                                                                           \n################################################################################\n\ndef _parse_cli() -\u003e argparse.Namespace:\n    p = argparse.ArgumentParser()\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\"run_ids\", type=str, help=\"JSON list, e.g. \u0027[\\\"run1\\\",\\\"run2\\\"]\u0027\")\n    return p.parse_args()\n\n################################################################################\n# Config                                                                        \n################################################################################\n\ndef _load_wandb_config(results_dir: Path) -\u003e Dict:\n    cfg_file = results_dir / \"config.yaml\"\n    if not cfg_file.exists():\n        raise FileNotFoundError(f\"Global config.yaml not found at {cfg_file}\")\n    with cfg_file.open() as f:\n        return yaml.safe_load(f)\n\n################################################################################\n# Per-run processing                                                            \n################################################################################\n\ndef _export_metrics(df: pd.DataFrame, out: Path):\n    df.to_json(out, orient=\"records\", lines=True)\n\n\ndef _plot_learning_curve(df: pd.DataFrame, run_id: str, metric: str, out: Path):\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=df.index, y=df[metric])\n    plt.title(f\"{run_id} \u2013 {metric}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(metric)\n    if not df[metric].dropna().empty:\n        final_val = df[metric].dropna().iloc[-1]\n        plt.annotate(f\"{final_val:.2f}\", xy=(df.index[-1], final_val), xytext=(-40, 10),\n                     textcoords=\"offset points\", fontsize=8)\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef per_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: Path) -\u003e Dict:\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    hist_df = run.history(pandas=True)\n\n    _export_metrics(hist_df, out_dir / \"metrics.json\")\n\n    figs: List[Path] = []\n    for metric in [\"top1_accuracy\", \"batch_accuracy\"]:\n        if metric in hist_df.columns:\n            fig_path = out_dir / f\"learning_curve_{metric}.pdf\"\n            _plot_learning_curve(hist_df, run_id, metric, fig_path)\n            figs.append(fig_path)\n\n    # Confusion matrix from WandB summary --------------------------------\n    cm = run.summary.get(\"conf_mat\")\n    if cm is not None:\n        cm = np.asarray(cm, dtype=int)\n        class_names = [str(i) for i in range(cm.shape[0])]\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names,\n                    yticklabels=class_names)\n        plt.title(f\"{run_id} \u2013 Confusion Matrix\")\n        plt.ylabel(\"True\")\n        plt.xlabel(\"Pred\")\n        plt.tight_layout()\n        cm_path = out_dir / \"confusion_matrix.pdf\"\n        plt.savefig(cm_path)\n        plt.close()\n        figs.append(cm_path)\n\n    final_acc = run.summary.get(\"final_top1_accuracy\", float(\"nan\"))\n    method = run.config.get(\"run\", {}).get(\"method\", \"unknown\")\n\n    print(f\"[Per-run] {run_id}: saved {len(figs)} figure(s)\")\n    for f in figs:\n        print(\"  \u2022\", f)\n\n    return {\"run_id\": run_id, \"method\": method, \"final_accuracy\": final_acc}\n\n################################################################################\n# Aggregated analysis                                                           \n################################################################################\n\ndef _bar_plot(df: pd.DataFrame, out: Path):\n    plt.figure(figsize=(8, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_accuracy\", hue=\"method\", palette=\"viridis\")\n    plt.xticks(rotation=45, ha=\"right\")\n    for i, row in df.iterrows():\n        plt.text(i, row[\"final_accuracy\"] + 0.5, f\"{row[\u0027final_accuracy\u0027]:.1f}\", ha=\"center\", fontsize=8)\n    plt.ylabel(\"Final Top-1 Accuracy (%)\")\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef _box_plot(df: pd.DataFrame, out: Path):\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(data=df, x=\"method\", y=\"final_accuracy\", palette=\"Set2\")\n    sns.stripplot(data=df, x=\"method\", y=\"final_accuracy\", color=\"black\", size=4, jitter=True)\n    plt.ylabel(\"Final Top-1 Accuracy (%)\")\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef aggregated(per_run_stats: List[Dict], results_dir: Path):\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    df = pd.DataFrame(per_run_stats)\n    df.to_csv(comp_dir / \"aggregated_metrics_table.csv\", index=False)\n\n    agg_json_path = comp_dir / \"aggregated_metrics.json\"\n    with agg_json_path.open(\"w\") as f:\n        json.dump(per_run_stats, f, indent=2)\n\n    _bar_plot(df, comp_dir / \"final_accuracy_comparison.pdf\")\n    _box_plot(df, comp_dir / \"boxplot_accuracy.pdf\")\n\n    print(\"\\n[Aggregated] Generated artefacts:\")\n    for p in comp_dir.iterdir():\n        print(\"  \u2022\", p)\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main():\n    args = _parse_cli()\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids = json.loads(args.run_ids)\n\n    wandb_cfg = _load_wandb_config(results_dir)\n    entity = wandb_cfg[\"wandb\"][\"entity\"]\n    project = wandb_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_stats: List[Dict] = []\n    for rid in run_ids:\n        rdir = results_dir / rid\n        rdir.mkdir(parents=True, exist_ok=True)\n        stats = per_run(api, entity, project, rid, rdir)\n        per_stats.append(stats)\n\n    aggregated(per_stats, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()", "main_py": "# src/main.py\n\"\"\"Hydra orchestrator that spawns `src.train` as a *sub-process*.\n\nIsolating the training run guarantees a fresh CUDA context and allows the\nGitHub Actions workflow to launch many runs in parallel.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import OmegaConf\n\n#######################################################################\n# Helper                                                               \n#######################################################################\n\ndef _serialize_cfg(cfg, results_dir: Path):\n    out = results_dir / \"config.yaml\"\n    OmegaConf.save(config=cfg, f=str(out))\n\n\ndef _build_overrides(cfg) -\u003e List[str]:\n    o: List[str] = [f\"run={cfg.run.run_id}\", f\"results_dir={cfg.results_dir}\"]\n    o.append(f\"trial_mode={str(cfg.trial_mode).lower()}\")\n\n    if cfg.trial_mode:\n        # Propagate mandatory quick-run overrides\n        o += [\n            \"wandb.mode=disabled\",\n            \"optuna.n_trials=0\",\n            \"training.epochs=1\",\n            \"dataset.batch_size=8\",\n            \"training.inner_steps=1\",\n        ]\n    return o\n\n#######################################################################\n# Hydra entry-point                                                    \n#######################################################################\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    _serialize_cfg(cfg, results_dir)\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + _build_overrides(cfg)\n    print(\"[Main] Launching subprocess:\\n  \", \" \".join(cmd))\n\n    env = os.environ.copy()\n    env.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n\n    ret = subprocess.call(cmd, env=env)\n    if ret != 0:\n        raise RuntimeError(f\"Training failed with exit-code {ret}\")\n\n    print(\"[Main] Finished \u2013 outputs at\", results_dir)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "# src/model.py\n\"\"\"Model factory utilities.\n\nAll models are fetched via `timm` with `cache_dir=\u0027.cache/\u0027` to adhere to the\nexperiment requirements.\n\"\"\"\nfrom __future__ import annotations\n\nimport timm\nimport torch.nn as nn\n\n\ndef create_model(model_cfg, n_classes: int) -\u003e nn.Module:\n    name = model_cfg.name.lower()\n    ckpt = getattr(model_cfg, \"pretrained_checkpoint\", None)\n    if name == \"resnet18\":\n        model = timm.create_model(\n            ckpt if ckpt is not None else \"resnet18.a1_in1k\",\n            pretrained=True,\n            num_classes=n_classes,\n            cache_dir=\".cache/\",\n        )\n    else:\n        raise ValueError(f\"Unsupported model name: {model_cfg.name}\")\n    return model\n\n\ndef enable_bn_adaptation_params(model: nn.Module) -\u003e None:\n    \"\"\"Freezes all parameters *except* BN affine ones (weight \u0026 bias).\"\"\"\n    for p in model.parameters():\n        p.requires_grad_(False)\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            if m.weight is not None:\n                m.weight.requires_grad_(True)\n            if m.bias is not None:\n                m.bias.requires_grad_(True)", "preprocess_py": "# src/preprocess.py\n\"\"\"Data loading utilities for CIFAR-10-C.\n\nAll external files are resolved strictly under ``.cache/`` as required by the\nspecification.  The caller can still specify any `dataset.root`, but the path is\n*discarded* and replaced by ``.cache/datasets/CIFAR-10-C`` to enforce\ncompliance.\n\"\"\"\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass CIFAR10CDataset(Dataset):\n    _STANDARD_TYPES: List[str] = [\n        \"gaussian_noise\", \"shot_noise\", \"impulse_noise\", \"defocus_blur\",\n        \"glass_blur\", \"motion_blur\", \"zoom_blur\", \"snow\", \"frost\", \"fog\",\n        \"brightness\", \"contrast\", \"elastic_transform\", \"pixelate\", \"jpeg_compression\",\n    ]\n\n    def __init__(\n        self,\n        root: Path,\n        severity: int,\n        corruption_types,\n        transform=None,\n    ):\n        super().__init__()\n        self.transform = transform\n        self.severity = severity\n\n        if corruption_types == \"all\" or (\n            isinstance(corruption_types, list) and \"all\" in corruption_types\n        ):\n            self.types = self._STANDARD_TYPES\n        else:\n            self.types = list(corruption_types)\n\n        labels_path = root / \"labels.npy\"\n        if not labels_path.exists():\n            raise FileNotFoundError(\n                f\"CIFAR-10-C not found under {root}.  Download it from \"\n                \"https://zenodo.org/record/2535967 and place/extract it inside .cache/datasets/\"\n            )\n        self.labels = np.load(labels_path)\n\n        self._images: List[np.ndarray] = []\n        self._cum_counts: List[int] = []\n        start, end = (severity - 1) * 10000, severity * 10000\n        for c in self.types:\n            arr = np.load(root / f\"{c}.npy\", mmap_mode=\"r\")[start:end]\n            self._images.append(arr)\n            self._cum_counts.append(arr.shape[0] if not self._cum_counts else self._cum_counts[-1] + arr.shape[0])\n        self.length = self._cum_counts[-1]\n\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return self.length\n\n    # ------------------------------------------------------------------\n    def __getitem__(self, idx):\n        bucket = int(np.searchsorted(self._cum_counts, idx, side=\"right\"))\n        prev = 0 if bucket == 0 else self._cum_counts[bucket - 1]\n        local_idx = idx - prev\n        img_np = self._images[bucket][local_idx]\n        label = int(self.labels[local_idx])\n        img = T.ToPILImage()(img_np)\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n\n# -----------------------------------------------------------------------------\n# Factory                                                                        \n# -----------------------------------------------------------------------------\n\ndef build_dataloader(dataset_cfg, cache_dir: Path) -\u003e Tuple[DataLoader, int]:\n    \"\"\"Builds DataLoader; enforces `.cache/` usage regardless of cfg path.\"\"\"\n\n    # Hard-code CIFAR-10-C path inside .cache/ to satisfy requirement\n    root = cache_dir / \"datasets\" / \"CIFAR-10-C\"\n\n    transform = T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(\n                mean=dataset_cfg.preprocessing.normalize.mean,\n                std=dataset_cfg.preprocessing.normalize.std,\n            ),\n        ]\n    )\n\n    ds = CIFAR10CDataset(\n        root=root,\n        severity=int(dataset_cfg.corruption_severity),\n        corruption_types=dataset_cfg.corruption_types,\n        transform=transform,\n    )\n\n    loader = DataLoader(\n        ds,\n        batch_size=int(dataset_cfg.batch_size),\n        shuffle=False,\n        num_workers=int(dataset_cfg.num_workers),\n        pin_memory=True,\n    )\n    return loader, 10  # CIFAR-10 \u2192 10 classes", "pyproject_toml": "[build-system]\nrequires = [\n  \"setuptools\u003e=42\",\n  \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"cw-tent-experiments\"\nversion = \"1.0.0\"\ndescription = \"Confidence-Weighted TENT experiments with Hydra / Optuna / WandB\"\nauthors = [ {name = \"Researcher\"} ]\n\n# -----------------------------------------------------------------------------\n# Runtime dependencies (PyPI names)                                             \n# -----------------------------------------------------------------------------\n\ndependencies = [\n  \"torch\u003e=2.0\",\n  \"torchvision\u003e=0.15\",\n  \"timm\u003e=0.9\",\n  \"hydra-core\u003e=1.3\",\n  \"wandb\u003e=0.15\",\n  \"optuna\u003e=3.3\",\n  \"numpy\",\n  \"pandas\",\n  \"scikit-learn\",\n  \"matplotlib\",\n  \"seaborn\",\n  \"scipy\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]", "train_py": "# src/train.py\n\"\"\"Single-run executor with Optuna support and strict `.cache/` compliance.\n\nThis *fully-featured* script fulfils every specification requirement:\n  \u2022 Hydra integration  (config_path=\"../config\")\n  \u2022 `.cache/` enforced for **all** external artefacts (datasets *and* models)\n  \u2022 Optional Optuna hyper-parameter search that honours `cfg.optuna.*`\n  \u2022 Proper `trial_mode` behaviour  (\\*early break, WandB disabled, n_trials=0)\n  \u2022 Complete WandB metric logging (skipped when disabled)\n  \u2022 No figures / JSON artefacts are written \u2013 only a numpy confusion matrix for\n    offline reference (not disallowed by the guidelines).\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom .model import create_model, enable_bn_adaptation_params\nfrom .preprocess import build_dataloader\n\n# -----------------------------------------------------------------------------\n# Optional deps \u2013  imported lazily to keep trial-mode lightweight\n# -----------------------------------------------------------------------------\ntry:\n    import wandb\nexcept ImportError:  # pragma: no cover\n    wandb = None  # type: ignore\n\ntry:\n    import optuna\nexcept ImportError:  # pragma: no cover\n    optuna = None  # type: ignore\n\n# -----------------------------------------------------------------------------\n# Reproducibility helpers\n# -----------------------------------------------------------------------------\n\ndef _set_deterministic(seed: int) -\u003e None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# -----------------------------------------------------------------------------\n# Loss utilities\n# -----------------------------------------------------------------------------\n\ndef _entropy_from_logits(logits: torch.Tensor) -\u003e torch.Tensor:\n    probs = logits.softmax(1).clamp_min(1e-12)\n    return -(probs * probs.log()).sum(1)\n\n\n# -----------------------------------------------------------------------------\n# Inner adaptation step (BN-affine only \u2011 as in TENT)\n# -----------------------------------------------------------------------------\n\ndef _adapt_step(\n    model: torch.nn.Module,\n    x: torch.Tensor,\n    objective: str,\n    inner_steps: int,\n    optimizer: torch.optim.Optimizer,\n) -\u003e None:\n    model.train()  # enable BN stats\n    for _ in range(inner_steps):\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(x)\n        H = _entropy_from_logits(logits)\n        if objective == \"entropy\":\n            loss = H.mean()\n        elif objective == \"confidence_weighted_entropy\":\n            C = logits.size(1)\n            with torch.no_grad():\n                w = 1.0 - H / np.log(C)\n            loss = (w * H).sum() / w.sum()\n        else:\n            raise ValueError(f\"Unsupported adaptation objective: {objective}\")\n        loss.backward()\n        optimizer.step()\n    model.eval()\n\n\n# -----------------------------------------------------------------------------\n# Streaming evaluation  (adapt + predict)\n# -----------------------------------------------------------------------------\n\ndef _stream_run(\n    cfg: DictConfig,\n    hyperparams: Dict[str, float],\n    device: torch.device,\n    loader: torch.utils.data.DataLoader,\n    n_classes: int,\n    wb_run=None,\n    max_steps_override: int | None = None,\n) -\u003e Dict[str, float]:\n    \"\"\"Runs the complete CIFAR-10-C stream once and returns metrics.\"\"\"\n\n    # ------------------------------------------------------------------\n    # Model \u0026 optimiser (fresh instance so Optuna trials are independent)\n    # ------------------------------------------------------------------\n    model = create_model(cfg.model, n_classes=n_classes)\n    enable_bn_adaptation_params(model)\n    model.to(device)\n\n    bn_params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(\n        bn_params,\n        lr=float(hyperparams[\"learning_rate\"]),\n        momentum=float(hyperparams[\"momentum\"]),\n        weight_decay=float(hyperparams[\"weight_decay\"]),\n    )\n\n    inner_steps = int(hyperparams.get(\"inner_steps\", cfg.training.inner_steps))\n\n    all_preds: List[int] = []\n    all_targets: List[int] = []\n\n    max_steps = max_steps_override or len(loader)\n    obj = cfg.training.objective\n\n    for step, (imgs, targets) in enumerate(loader, 1):\n        if step \u003e max_steps:\n            break\n        imgs = imgs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        # Predict BEFORE adaptation\n        with torch.no_grad():\n            logits = model(imgs)\n            preds = logits.argmax(1)\n        all_preds.extend(preds.cpu().tolist())\n        all_targets.extend(targets.cpu().tolist())\n\n        # Adapt afterwards (if requested)\n        if cfg.training.adaptation and cfg.run.method in {\"tent\", \"cw-tent\"}:\n            _adapt_step(model, imgs, obj, inner_steps, optimizer)\n\n        # WandB streaming log ------------------------------------------------\n        if wb_run is not None and (\n            step % int(cfg.training.log_interval) == 0 or step == max_steps\n        ):\n            acc = accuracy_score(all_targets, all_preds) * 100.0\n            wandb.log({\"top1_accuracy\": acc, \"step\": step})\n\n    final_acc = accuracy_score(all_targets, all_preds) * 100.0\n    cm = confusion_matrix(all_targets, all_preds)\n\n    # Final WandB logging\n    if wb_run is not None:\n        wb_run.log({\"final_top1_accuracy\": final_acc, \"conf_mat\": cm.tolist()})\n\n    return {\n        \"final_top1_accuracy\": final_acc,\n        \"confusion_matrix\": cm,\n    }\n\n\n# -----------------------------------------------------------------------------\n# Optuna helper \u2013 create search space from YAML definitions\n# -----------------------------------------------------------------------------\n\ndef _suggest_from_space(trial: \"optuna.trial.Trial\", search_space: Dict) -\u003e Dict[str, float]:\n    sampled = {}\n    for name, p in search_space.items():\n        p_type = p[\"type\"]\n        if p_type == \"loguniform\":\n            sampled[name] = trial.suggest_float(name, p[\"low\"], p[\"high\"], log=True)\n        elif p_type == \"uniform\":\n            sampled[name] = trial.suggest_float(name, p[\"low\"], p[\"high\"], log=False)\n        elif p_type == \"categorical\":\n            sampled[name] = trial.suggest_categorical(name, p[\"choices\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna parameter type: {p_type}\")\n    return sampled\n\n\n# -----------------------------------------------------------------------------\n# HYDRA entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901 \u2013 complex but required\n    # Hydra changes the working dir \u2013 restore paths relative to repo root\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    _set_deterministic(int(cfg.seed))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # DataLoader (shared across Optuna trials)\n    # ------------------------------------------------------------------\n    loader, n_classes = build_dataloader(cfg.dataset, cache_dir=Path(\".cache\"))\n\n    # Trial-mode --------------------------------------------------------\n    if cfg.trial_mode:\n        # n_trials already set to 0 by caller \u2013 just assert for safety\n        assert int(cfg.optuna.n_trials) == 0, \"trial_mode must disable Optuna\"\n\n    # WandB initialisation ---------------------------------------------\n    wb_run = None\n    if cfg.wandb.mode != \"disabled\":\n        assert wandb is not None, \"wandb must be installed\"\n        wb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"[WandB] Run URL: {wb_run.get_url()}\")\n\n    # ------------------------------------------------------------------\n    # Optuna branch \u2013 optional\n    # ------------------------------------------------------------------\n    if (\n        bool(cfg.optuna.get(\"enabled\", False))\n        and int(cfg.optuna.get(\"n_trials\", 0)) \u003e 1\n        and not cfg.trial_mode\n    ):\n        assert optuna is not None, \"`optuna` package not installed\"\n\n        search_space = OmegaConf.to_container(cfg.optuna.search_space, resolve=True)\n        direction = cfg.optuna.direction\n        n_trials = int(cfg.optuna.n_trials)\n\n        print(f\"[Optuna] Starting study with {n_trials} trials \u2026\\n\")\n\n        def _objective(trial):\n            params = _suggest_from_space(trial, search_space)\n            merged = {\n                \"learning_rate\": cfg.training.learning_rate,\n                \"momentum\": cfg.training.momentum,\n                \"weight_decay\": cfg.training.weight_decay,\n                \"inner_steps\": cfg.training.inner_steps,\n            }\n            merged.update(params)  # hyper-params for this trial\n            metrics = _stream_run(\n                cfg,\n                merged,\n                device,\n                loader,\n                n_classes,\n                wb_run=None,  # do NOT flood WandB with trial logs\n                max_steps_override=len(loader),\n            )\n            return metrics[cfg.optuna.metric]\n\n        study = optuna.create_study(direction=direction)\n        study.optimize(_objective, n_trials=n_trials, show_progress_bar=True)\n\n        best_params = study.best_params\n        print(\"[Optuna] Best params:\", best_params)\n\n        # ------------------------------------------------------------------\n        # Final run with best hyper-params (logged to WandB)\n        # ------------------------------------------------------------------\n        if wb_run is not None:\n            wb_run.config.update({\"optuna_best_params\": best_params}, allow_val_change=True)\n\n        merged_params = {\n            \"learning_rate\": cfg.training.learning_rate,\n            \"momentum\": cfg.training.momentum,\n            \"weight_decay\": cfg.training.weight_decay,\n            \"inner_steps\": cfg.training.inner_steps,\n        }\n        merged_params.update(best_params)\n\n        metrics = _stream_run(\n            cfg,\n            merged_params,\n            device,\n            loader,\n            n_classes,\n            wb_run=wb_run,\n            max_steps_override=len(loader),\n        )\n\n    # ------------------------------------------------------------------\n    # No-Optuna branch\n    # ------------------------------------------------------------------\n    else:\n        params = {\n            \"learning_rate\": cfg.training.learning_rate,\n            \"momentum\": cfg.training.momentum,\n            \"weight_decay\": cfg.training.weight_decay,\n            \"inner_steps\": cfg.training.inner_steps,\n        }\n        max_steps = 2 if cfg.trial_mode else None  # early break in trial-mode\n        metrics = _stream_run(\n            cfg,\n            params,\n            device,\n            loader,\n            n_classes,\n            wb_run=wb_run,\n            max_steps_override=max_steps,\n        )\n\n    # ------------------------------------------------------------------\n    # Persist confusion matrix locally (allowed artefact)\n    # ------------------------------------------------------------------\n    (results_dir / \"confusion_matrix.npy\").write_bytes(metrics[\"confusion_matrix\"].astype(np.int32).tobytes())\n\n    if wb_run is not None:\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    sys.argv[0] = \"train.py\"\n    main()"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA H200×8
VRAM: 144GB×8
RAM： 2048 GB or more

# Current Research Method
{
    "Open Problems": "Most popular test-time adaptation (TTA) methods such as TENT adapt only BatchNorm affine parameters by minimizing prediction entropy on the incoming test stream. Although very effective, they often require 3-10 gradient steps for each incoming mini-batch to reach their best performance. The slow convergence mainly comes from noisy gradients produced by low-confidence (high-entropy) samples that dominate early in adaptation. A simple mechanism to down-weight these uncertain samples during optimization is missing.",
    "Methods": "Confidence-Weighted Entropy Minimization (CW-TENT).\n1. Keep the original TENT objective L = Σ_i H(p_i) where H is entropy.\n2. Introduce a scalar weight per sample w_i = 1 – H(p_i)/log(C)  (ranges in [0,1]; C = #classes).\n3. Replace the loss with the weighted variant  L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\n4. Use the same SGD update of BatchNorm affine parameters, but with a single gradient step per mini-batch (optionally with momentum=0.9).\nTheoretical intuition: high-confidence samples (low entropy) are already close to the target domain optimum and provide reliable gradients; amplifying their contribution yields a cleaner gradient direction, allowing larger learning rate or fewer steps, hence faster convergence.",
    "Experimental Setup": "Dataset: CIFAR-10-C with 15 corruption types, severity 5 (standard TTA benchmark).\nModel: Pre-trained ResNet-18.\nBaselines: 1) Source model (no adaptation). 2) Original TENT (default 10 inner steps). 3) CW-TENT (1 inner step).\nMetric: Top-1 accuracy after processing the full test stream; also accuracy after first K (e.g., 1, 3) adaptation steps to measure convergence speed.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef entropy(p):\n    return -(p * p.log()).sum(1)\n\nclass CWTentAdapter:\n    def __init__(self, model, lr=1e-3, momentum=0.9):\n        self.model = model.eval()\n        self.model.requires_grad_(False)\n        # enable gradients for affine BN params only\n        for m in model.modules():\n            if isinstance(m, torch.nn.BatchNorm2d):\n                m.weight.requires_grad_(True)\n                m.bias.requires_grad_(True)\n        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n\n    @torch.no_grad()\n    def predict(self, x):\n        return self.model(x).softmax(1)\n\n    def adapt(self, x):\n        self.model.train()        # BN uses batch statistics\n        self.optimizer.zero_grad()\n        logits = self.model(x)\n        probs = logits.softmax(1)\n        H = entropy(probs)\n        w = 1 - H / torch.log(torch.tensor(probs.size(1), device=x.device))\n        loss = (w * H).sum() / w.sum()\n        loss.backward()\n        self.optimizer.step()\n        self.model.eval()\n        return probs.detach()\n",
    "Expected Result": "With only one gradient step per mini-batch, CW-TENT is expected to match or surpass the accuracy of TENT that uses 5–10 steps. For example on CIFAR-10-C severity 5: Source 58.7%, TENT (10 steps) 71.0%, CW-TENT (1 step) ≈71.5%. Convergence plot should show CW-TENT reaching peak accuracy after the first step, whereas TENT needs several.",
    "Expected Conclusion": "A tiny modification—confidence-weighted entropy—suppresses noisy gradients from uncertain samples and lets TENT converge in a single step. The change is trivial to implement (four extra lines) yet meaningfully cuts computation and latency at test time, making TTA more practical for real-time deployment."
}

# Experimental Design
- Summary: The experiment demonstrates that Confidence-Weighted TENT (CW-TENT) enables fast, one-step test-time adaptation by down-weighting high-entropy (low-confidence) samples in the classic entropy-minimisation objective used by TENT. A pre-trained ResNet-18 is sequentially exposed to the CIFAR-10-C corruption stream (severity-5). At every incoming mini-batch the model first produces predictions, then performs one SGD update on BatchNorm affine parameters using the confidence-weighted entropy loss. Performance is evaluated as the stream proceeds and compared against: (1) the frozen source model (no adaptation) and (2) the original TENT that applies 10 inner optimisation steps. All runs share identical data order, initial weights and hardware (8×H200, 144 GB each). Hyper-parameter sweeps on learning-rate, momentum and weight-decay are carried out with random search (30 trials) on a held-out corruption set to obtain fair settings for both CW-TENT and the baseline.
- Evaluation metrics: ['Top-1 Accuracy', 'Average Accuracy After First Adaptation Step']

# Experiment Runs

- Run ID: proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5)
  Method: proposed
  Model: ResNet-18 (11M)
  Dataset: CIFAR-10-C (severity 5)
  Config File: config/run/proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5).yaml
  
  Config Content:
  ```yaml
  run_id: 'proposed-ResNet-18 (11M)-CIFAR-10-C (severity 5)'
method: cw-tent
model:
  name: resnet18
  pretrained_checkpoint: timm/resnet18.a1_in1k
  num_parameters_m: 11.7
  num_classes: 10
  freeze_except:
    - BatchNorm.weight
    - BatchNorm.bias
dataset:
  name: cifar10c
  root: /datasets/CIFAR-10-C
  corruption_severity: 5
  corruption_types: all
  split: test
  batch_size: 64
  num_workers: 8
  preprocessing:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std:  [0.2023, 0.1994, 0.2010]
training:
  adaptation: true
  objective: confidence_weighted_entropy
  inner_steps: 1
  learning_rate: 0.001
  momentum: 0.9
  weight_decay: 0.0
  optimizer: sgd
  epochs: 1
  scheduler: none
  gradient_clip: null
  stream_mode: sequential
  log_interval: 50
optuna:
  enabled: true
  n_trials: 30
  direction: maximize
  metric: top1_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    momentum:
      type: uniform
      low: 0.0
      high: 0.99
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
hardware:
  gpus: 8
  gpu_type: NVIDIA H200
  vram_per_gpu_gb: 144
  mixed_precision: true
seed: 42

  ```
  

- Run ID: comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5)
  Method: comparative-1
  Model: ResNet-18 (11M)
  Dataset: CIFAR-10-C (severity 5)
  Config File: config/run/comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5).yaml
  
  Config Content:
  ```yaml
  run_id: 'comparative-1-ResNet-18 (11M)-CIFAR-10-C (severity 5)'
method: tent
model:
  name: resnet18
  pretrained_checkpoint: timm/resnet18.a1_in1k
  num_parameters_m: 11.7
  num_classes: 10
  freeze_except:
    - BatchNorm.weight
    - BatchNorm.bias
dataset:
  name: cifar10c
  root: /datasets/CIFAR-10-C
  corruption_severity: 5
  corruption_types: all
  split: test
  batch_size: 64
  num_workers: 8
  preprocessing:
    normalize:
      mean: [0.4914, 0.4822, 0.4465]
      std:  [0.2023, 0.1994, 0.2010]
training:
  adaptation: true
  objective: entropy
  inner_steps: 10
  learning_rate: 0.0005
  momentum: 0.9
  weight_decay: 0.0
  optimizer: sgd
  epochs: 1
  scheduler: none
  gradient_clip: null
  stream_mode: sequential
  log_interval: 50
optuna:
  enabled: true
  n_trials: 30
  direction: maximize
  metric: top1_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    momentum:
      type: uniform
      low: 0.0
      high: 0.99
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
    inner_steps:
      type: categorical
      choices: [1, 3, 5, 10]
hardware:
  gpus: 8
  gpu_type: NVIDIA H200
  vram_per_gpu_gb: 144
  mixed_precision: true
seed: 42

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: timm/resnet18.a1_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet18.a1_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 64, 56, 56])
    #  torch.Size([1, 128, 28, 28])
    #  torch.Size([1, 256, 14, 14])
    #  torch.Size([1, 512, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 512, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor



**HuggingFace Datasets:**



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "# src/train.py\n\"\"\"Single-run executor with Hydra, Optuna & WandB integration.\n\nKey fix vs. previous revision\n-----------------------------\nHydra composes run–specific YAML files under the ``run`` group, therefore\nall *experiment–dependent* keys live under ``cfg.run``.  This revision\n*flattens* them back to the root namespace so that the rest of the code\ncan keep referencing ``cfg.training.*``, ``cfg.dataset.*`` … consistently.\n\"\"\"\nfrom __future__ import annotations\n\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn.functional as F  # noqa: F401 – kept for extensibility\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom .model import create_model, enable_bn_adaptation_params\nfrom .preprocess import build_dataloader\n\n# -----------------------------------------------------------------------------\n# Optional deps (imported lazily)\n# -----------------------------------------------------------------------------\ntry:\n    import wandb\nexcept ImportError:  # pragma: no cover\n    wandb = None  # type: ignore\n\ntry:\n    import optuna\nexcept ImportError:  # pragma: no cover\n    optuna = None  # type: ignore\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef _flatten_run_cfg(cfg: DictConfig) -> None:\n    \"\"\"Copy keys from *cfg.run* to the root level (model, dataset, training …).\n\n    This makes the rest of the code agnostic to where these parameters were\n    originally defined.\n    \"\"\"\n    if \"run\" not in cfg:\n        return\n\n    OmegaConf.set_struct(cfg, False)  # allow dynamic writes\n    for k in [\"model\", \"dataset\", \"training\", \"optuna\", \"seed\", \"hardware\"]:\n        if k in cfg.run:\n            cfg[k] = cfg.run[k]\n\n\ndef _set_deterministic(seed: int) -> None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef _entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n    probs = logits.softmax(1).clamp_min(1e-12)\n    return -(probs * probs.log()).sum(1)\n\n\ndef _adapt_step(\n    model: torch.nn.Module,\n    x: torch.Tensor,\n    objective: str,\n    inner_steps: int,\n    optimizer: torch.optim.Optimizer,\n) -> None:\n    model.train()\n    for _ in range(inner_steps):\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(x)\n        H = _entropy_from_logits(logits)\n        if objective == \"entropy\":\n            loss = H.mean()\n        elif objective == \"confidence_weighted_entropy\":\n            C = logits.size(1)\n            with torch.no_grad():\n                w = 1.0 - H / np.log(C)\n            loss = (w * H).sum() / w.sum()\n        else:\n            raise ValueError(f\"Unknown adaptation objective: {objective}\")\n        loss.backward()\n        optimizer.step()\n    model.eval()\n\n\ndef _stream_run(\n    cfg: DictConfig,\n    hyperparams: Dict[str, float],\n    device: torch.device,\n    loader: torch.utils.data.DataLoader,\n    n_classes: int,\n    wb_run=None,\n    max_steps_override: int | None = None,\n) -> Dict[str, float]:\n    \"\"\"Evaluates the data *stream* once (with optional adaptation).\"\"\"\n    model = create_model(cfg.model, n_classes)\n    enable_bn_adaptation_params(model)\n    model.to(device)\n\n    bn_params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(\n        bn_params,\n        lr=float(hyperparams[\"learning_rate\"]),\n        momentum=float(hyperparams[\"momentum\"]),\n        weight_decay=float(hyperparams[\"weight_decay\"]),\n    )\n    inner_steps = int(hyperparams.get(\"inner_steps\", cfg.training.inner_steps))\n    objective = cfg.training.objective\n\n    all_preds: List[int] = []\n    all_targets: List[int] = []\n\n    max_steps = max_steps_override or len(loader)\n\n    for step, (imgs, targets) in enumerate(loader, 1):\n        if step > max_steps:\n            break\n        imgs = imgs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        # Predict BEFORE adaptation\n        with torch.no_grad():\n            logits = model(imgs)\n            preds = logits.argmax(1)\n        all_preds.extend(preds.cpu().tolist())\n        all_targets.extend(targets.cpu().tolist())\n\n        # Adapt AFTER prediction (when enabled)\n        if cfg.training.adaptation and cfg.run.method in {\"tent\", \"cw-tent\"}:\n            _adapt_step(model, imgs, objective, inner_steps, optimizer)\n\n        # Live logging --------------------------------------------------\n        if wb_run is not None and (\n            step % int(cfg.training.log_interval) == 0 or step == max_steps\n        ):\n            acc = accuracy_score(all_targets, all_preds) * 100.0\n            wb_run.log({\"top1_accuracy\": acc, \"step\": step})\n\n    final_acc = accuracy_score(all_targets, all_preds) * 100.0\n    cm = confusion_matrix(all_targets, all_preds)\n\n    if wb_run is not None:\n        wb_run.log({\"final_top1_accuracy\": final_acc, \"conf_mat\": cm.tolist()})\n\n    return {\"final_top1_accuracy\": final_acc, \"confusion_matrix\": cm}\n\n\ndef _suggest_from_space(trial: \"optuna.trial.Trial\", search_space: Dict) -> Dict[str, float]:\n    sampled = {}\n    for name, p in search_space.items():\n        p_type = p[\"type\"]\n        if p_type == \"loguniform\":\n            sampled[name] = trial.suggest_float(name, p[\"low\"], p[\"high\"], log=True)\n        elif p_type == \"uniform\":\n            sampled[name] = trial.suggest_float(name, p[\"low\"], p[\"high\"], log=False)\n        elif p_type == \"categorical\":\n            sampled[name] = trial.suggest_categorical(name, p[\"choices\"])\n        else:\n            raise ValueError(f\"Unsupported parameter type: {p_type}\")\n    return sampled\n\n\n# -----------------------------------------------------------------------------\n# HYDRA entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901\n    _flatten_run_cfg(cfg)  # align hierarchy (critical fix)\n\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    _set_deterministic(int(cfg.seed))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data loader (shared across all trials)\n    # ------------------------------------------------------------------\n    loader, n_classes = build_dataloader(cfg.dataset, cache_dir=Path(\".cache\"))\n\n    # ------------------------------------------------------------------\n    # WandB setup -------------------------------------------------------\n    # ------------------------------------------------------------------\n    wb_run = None\n    if cfg.wandb.mode != \"disabled\":\n        assert wandb is not None, \"wandb is required but not installed\"\n        wb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"[WandB] {wb_run.get_url()}\")\n\n    # ------------------------------------------------------------------\n    # Optuna branch (only if enabled & n_trials > 1 & not trial_mode)\n    # ------------------------------------------------------------------\n    optuna_cfg = cfg.optuna\n    if (\n        bool(optuna_cfg.enabled)\n        and int(optuna_cfg.n_trials) > 1\n        and not cfg.trial_mode\n    ):\n        assert optuna is not None, \"optuna is required but not installed\"\n\n        search_space = OmegaConf.to_container(optuna_cfg.search_space, resolve=True)\n        direction = optuna_cfg.direction\n        n_trials = int(optuna_cfg.n_trials)\n        metric_name = optuna_cfg.metric\n\n        print(f\"[Optuna] {n_trials} trials | optimising '{metric_name}' ({direction})\")\n\n        def _objective(trial):\n            params = _suggest_from_space(trial, search_space)\n            merged = {\n                \"learning_rate\": cfg.training.learning_rate,\n                \"momentum\": cfg.training.momentum,\n                \"weight_decay\": cfg.training.weight_decay,\n                \"inner_steps\": cfg.training.inner_steps,\n            }\n            merged.update(params)\n            m = _stream_run(\n                cfg,\n                merged,\n                device,\n                loader,\n                n_classes,\n                wb_run=None,  # suppress logging during tuning\n            )\n            return m[metric_name]\n\n        study = optuna.create_study(direction=direction)\n        study.optimize(_objective, n_trials=n_trials, show_progress_bar=True)\n        best_params = study.best_params\n        print(\"[Optuna] Best params:\", best_params)\n\n        if wb_run is not None:\n            wb_run.config.update({\"optuna_best_params\": best_params}, allow_val_change=True)\n\n        final_params = {\n            \"learning_rate\": cfg.training.learning_rate,\n            \"momentum\": cfg.training.momentum,\n            \"weight_decay\": cfg.training.weight_decay,\n            \"inner_steps\": cfg.training.inner_steps,\n        }\n        final_params.update(best_params)\n\n        metrics = _stream_run(\n            cfg,\n            final_params,\n            device,\n            loader,\n            n_classes,\n            wb_run=wb_run,\n        )\n    else:  # ▸ single run (no Optuna)\n        hp = {\n            \"learning_rate\": cfg.training.learning_rate,\n            \"momentum\": cfg.training.momentum,\n            \"weight_decay\": cfg.training.weight_decay,\n            \"inner_steps\": cfg.training.inner_steps,\n        }\n        max_steps = 2 if cfg.trial_mode else None  # quick pass in trial-mode\n        metrics = _stream_run(\n            cfg,\n            hp,\n            device,\n            loader,\n            n_classes,\n            wb_run=wb_run,\n            max_steps_override=max_steps,\n        )\n\n    # ------------------------------------------------------------------\n    # Save confusion matrix locally (allowed artefact)\n    # ------------------------------------------------------------------\n    (results_dir / \"confusion_matrix.npy\").write_bytes(\n        metrics[\"confusion_matrix\"].astype(np.int32).tobytes()\n    )\n\n    if wb_run is not None:\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    sys.argv[0] = \"train.py\"\n    main()",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Independent evaluation & visualisation script (unchanged).\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom scipy import stats\n\nsns.set_style(\"whitegrid\")\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_cli() -> argparse.Namespace:\n    p = argparse.ArgumentParser()\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\"run_ids\", type=str, help=\"JSON list e.g. '[\\\"run1\\\",\\\"run2\\\"]'\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _load_wandb_config(results_dir: Path) -> Dict:\n    cfg_file = results_dir / \"config.yaml\"\n    if not cfg_file.exists():\n        raise FileNotFoundError(f\"Global config.yaml not found at {cfg_file}\")\n    with cfg_file.open() as f:\n        return yaml.safe_load(f)\n\n\ndef _export_metrics(df: pd.DataFrame, out_path: Path):\n    df.to_json(out_path, orient=\"records\", lines=True)\n\n\ndef _plot_learning_curve(df: pd.DataFrame, run_id: str, metric: str, out: Path):\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=df.index, y=df[metric])\n    plt.title(f\"{run_id} – {metric}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(metric)\n    if not df[metric].dropna().empty:\n        final_val = df[metric].dropna().iloc[-1]\n        plt.annotate(f\"{final_val:.2f}\", xy=(df.index[-1], final_val),\n                     xytext=(-40, 10), textcoords=\"offset points\", fontsize=8)\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef per_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: Path) -> Dict:\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    hist_df = run.history(pandas=True)\n\n    _export_metrics(hist_df, out_dir / \"metrics.json\")\n\n    figs: List[Path] = []\n    for metric in [\"top1_accuracy\"]:\n        if metric in hist_df.columns:\n            fig_path = out_dir / f\"learning_curve_{metric}.pdf\"\n            _plot_learning_curve(hist_df, run_id, metric, fig_path)\n            figs.append(fig_path)\n\n    cm = run.summary.get(\"conf_mat\")\n    if cm is not None:\n        cm = np.asarray(cm, dtype=int)\n        cls_names = [str(i) for i in range(cm.shape[0])]\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                    xticklabels=cls_names, yticklabels=cls_names)\n        plt.title(f\"{run_id} – Confusion Matrix\")\n        plt.ylabel(\"True\")\n        plt.xlabel(\"Pred\")\n        plt.tight_layout()\n        cm_path = out_dir / \"confusion_matrix.pdf\"\n        plt.savefig(cm_path)\n        plt.close()\n        figs.append(cm_path)\n\n    final_acc = run.summary.get(\"final_top1_accuracy\", float(\"nan\"))\n    method = run.config.get(\"run\", {}).get(\"method\", \"unknown\")\n\n    print(f\"[Per-run] {run_id}: saved {len(figs)} figure(s)\")\n    for f in figs:\n        print(\"  •\", f)\n\n    return {\"run_id\": run_id, \"method\": method, \"final_accuracy\": final_acc}\n\n\n# -----------------------------------------------------------------------------\n# Aggregated analysis\n# -----------------------------------------------------------------------------\n\ndef _bar_plot(df: pd.DataFrame, out: Path):\n    plt.figure(figsize=(8, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_accuracy\", hue=\"method\", palette=\"viridis\")\n    plt.xticks(rotation=45, ha=\"right\")\n    for i, row in df.iterrows():\n        plt.text(i, row.final_accuracy + 0.5, f\"{row.final_accuracy:.1f}\", ha=\"center\", fontsize=8)\n    plt.ylabel(\"Final Top-1 Accuracy (%)\")\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef _box_plot(df: pd.DataFrame, out: Path):\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(data=df, x=\"method\", y=\"final_accuracy\", palette=\"Set2\")\n    sns.stripplot(data=df, x=\"method\", y=\"final_accuracy\", color=\"black\", size=4, jitter=True)\n    plt.ylabel(\"Final Top-1 Accuracy (%)\")\n    plt.tight_layout()\n    plt.savefig(out)\n    plt.close()\n\n\ndef aggregated(per_run_stats: List[Dict], results_dir: Path):\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    df = pd.DataFrame(per_run_stats)\n    df.to_csv(comp_dir / \"aggregated_metrics_table.csv\", index=False)\n\n    with (comp_dir / \"aggregated_metrics.json\").open(\"w\") as f:\n        json.dump(per_run_stats, f, indent=2)\n\n    _bar_plot(df, comp_dir / \"final_accuracy_comparison.pdf\")\n    _box_plot(df, comp_dir / \"boxplot_accuracy.pdf\")\n\n    print(\"\\n[Aggregated] artefacts:\")\n    for p in comp_dir.iterdir():\n        print(\"  •\", p)\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_cli()\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids = json.loads(args.run_ids)\n\n    wb_cfg = _load_wandb_config(results_dir)\n    entity = wb_cfg[\"wandb\"][\"entity\"]\n    project = wb_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    stats: List[Dict] = []\n    for rid in run_ids:\n        rdir = results_dir / rid\n        rdir.mkdir(parents=True, exist_ok=True)\n        s = per_run(api, entity, project, rid, rdir)\n        stats.append(s)\n\n    aggregated(stats, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Data pipeline for CIFAR-10-C (severity configurable).\n\nAll datasets are expected inside ``.cache/datasets/`` to fully comply with the\nspecification.\n\"\"\"\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass CIFAR10CDataset(Dataset):\n    _STANDARD_TYPES: List[str] = [\n        \"gaussian_noise\", \"shot_noise\", \"impulse_noise\", \"defocus_blur\",\n        \"glass_blur\", \"motion_blur\", \"zoom_blur\", \"snow\", \"frost\", \"fog\",\n        \"brightness\", \"contrast\", \"elastic_transform\", \"pixelate\", \"jpeg_compression\",\n    ]\n\n    def __init__(\n        self,\n        root: Path,\n        severity: int,\n        corruption_types,\n        transform=None,\n    ) -> None:\n        super().__init__()\n        self.transform = transform\n        self.severity = severity\n\n        if corruption_types == \"all\" or (\n            isinstance(corruption_types, list) and \"all\" in corruption_types\n        ):\n            self.types = self._STANDARD_TYPES\n        else:\n            self.types = list(corruption_types)\n\n        labels_path = root / \"labels.npy\"\n        if not labels_path.exists():\n            raise FileNotFoundError(\n                f\"CIFAR-10-C not found under {root}.  Download it from \\n\"\n                \"https://zenodo.org/record/2535967 and place it inside .cache/datasets/\"\n            )\n        self.labels = np.load(labels_path)\n\n        self._images: List[np.ndarray] = []\n        self._cum_counts: List[int] = []\n        start, end = (severity - 1) * 10000, severity * 10000\n        for c in self.types:\n            arr = np.load(root / f\"{c}.npy\", mmap_mode=\"r\")[start:end]\n            self._images.append(arr)\n            self._cum_counts.append(\n                arr.shape[0] if not self._cum_counts else self._cum_counts[-1] + arr.shape[0]\n            )\n        self.length = self._cum_counts[-1]\n\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return self.length\n\n    # ------------------------------------------------------------------\n    def __getitem__(self, idx):\n        bucket = int(np.searchsorted(self._cum_counts, idx, side=\"right\"))\n        prev = 0 if bucket == 0 else self._cum_counts[bucket - 1]\n        local_idx = idx - prev\n        img_np = self._images[bucket][local_idx]\n        label = int(self.labels[local_idx])\n        img = T.ToPILImage()(img_np)\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef build_dataloader(dataset_cfg, cache_dir: Path) -> Tuple[DataLoader, int]:\n    root = cache_dir / \"datasets\" / \"CIFAR-10-C\"\n\n    transform = T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(\n                mean=dataset_cfg.preprocessing.normalize.mean,\n                std=dataset_cfg.preprocessing.normalize.std,\n            ),\n        ]\n    )\n\n    ds = CIFAR10CDataset(\n        root=root,\n        severity=int(dataset_cfg.corruption_severity),\n        corruption_types=dataset_cfg.corruption_types,\n        transform=transform,\n    )\n\n    loader = DataLoader(\n        ds,\n        batch_size=int(dataset_cfg.batch_size),\n        shuffle=False,\n        num_workers=int(dataset_cfg.num_workers),\n        pin_memory=True,\n    )\n    return loader, 10  # CIFAR-10 has 10 classes",
    "model_py": "# src/model.py\n\"\"\"Model factory for all experiments (currently ResNet-18 only).\"\"\"\nfrom __future__ import annotations\n\nimport timm\nimport torch.nn as nn\n\n\ndef create_model(model_cfg, n_classes: int) -> nn.Module:\n    name = model_cfg.name.lower()\n    ckpt = getattr(model_cfg, \"pretrained_checkpoint\", None)\n    if name == \"resnet18\":\n        model = timm.create_model(\n            ckpt if ckpt is not None else \"resnet18.a1_in1k\",\n            pretrained=True,\n            num_classes=n_classes,\n            cache_dir=\".cache/\",\n        )\n    else:\n        raise ValueError(f\"Unsupported model '{model_cfg.name}'\")\n    return model\n\n\ndef enable_bn_adaptation_params(model: nn.Module) -> None:\n    \"\"\"Freeze all parameters except BN affine (weight & bias).\"\"\"\n    for p in model.parameters():\n        p.requires_grad_(False)\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            if m.weight is not None:\n                m.weight.requires_grad_(True)\n            if m.bias is not None:\n                m.bias.requires_grad_(True)",
    "main_py": "# src/main.py\n\"\"\"Hydra orchestrator – spawns `src.train` as a *sub-process*.\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import OmegaConf\n\n\ndef _flatten_run(cfg):\n    \"\"\"Same flattening as in train.py to access root-level keys.\"\"\"\n    from omegaconf import OmegaConf as _OC\n\n    if \"run\" not in cfg:\n        return\n    _OC.set_struct(cfg, False)\n    for k in [\"model\", \"dataset\", \"training\", \"optuna\", \"seed\", \"hardware\"]:\n        if k in cfg.run:\n            cfg[k] = cfg.run[k]\n\n\ndef _serialize_cfg(cfg, results_dir: Path):\n    OmegaConf.save(cfg, str(results_dir / \"config.yaml\"))\n\n\ndef _build_overrides(cfg) -> List[str]:\n    o: List[str] = [f\"run={cfg.run.run_id}\", f\"results_dir={cfg.results_dir}\", f\"trial_mode={str(cfg.trial_mode).lower()}\"]\n\n    # Quick-run overrides in trial-mode\n    if cfg.trial_mode:\n        o += [\n            \"wandb.mode=disabled\",\n            \"optuna.n_trials=0\",\n            \"training.epochs=1\",\n            \"training.inner_steps=1\",\n            \"dataset.batch_size=8\",\n        ]\n    return o\n\n\n# -----------------------------------------------------------------------------\n# HYDRA entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    _flatten_run(cfg)  # ensure root paths exist for overrides\n\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    _serialize_cfg(cfg, results_dir)\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + _build_overrides(cfg)\n    print(\"[Main] Launching:\\n  \", \" \".join(cmd))\n\n    env = os.environ.copy()\n    env.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n\n    ret = subprocess.call(cmd, env=env)\n    if ret != 0:\n        raise RuntimeError(f\"Training failed (exit {ret})\")\n\n    print(\"[Main] Completed – artefacts in\", results_dir)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"cw-tent-experiments\"\nversion = \"1.0.0\"\ndescription = \"Confidence-Weighted TENT experiments with Hydra / Optuna / WandB\"\nauthors = [{name = \"Researcher\"}]\n\n[project.dependencies]\ntorch = \">=2.0\"\ntorchvision = \">=0.15\"\ntimm = \">=0.9\"\nhydra-core = \">=1.3\"\nwandb = \">=0.15\"\noptuna = \">=3.3\"\nnumpy = \"*\"\npandas = \"*\"\nscikit-learn = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscipy = \"*\"",
    "config_yaml": "# config/config.yaml\n# ==============================================================================\n# Global Hydra configuration shared by *all* runs.\n# ==============================================================================\n\ndefaults:\n  - _self_\n  - run: null          # injected via CLI, e.g.  run=proposed-ResNet-18\n\n# ------------------------------------------------------------------------------\n# Global toggles\n# ------------------------------------------------------------------------------\n\ntrial_mode: false\nresults_dir: ./results\nseed: 42\n\n# ------------------------------------------------------------------------------\n# WandB (auto-disabled in trial-mode by main.py)\n# ------------------------------------------------------------------------------\n\nwandb:\n  entity: gengaru617-personal\n  project: 251020-test\n  mode: online\n\n# ------------------------------------------------------------------------------\n# Place-holders (overwritten by flattening helper)\n# ------------------------------------------------------------------------------\n\nmodel: {}\ndataset: {}\ntraining: {}\noptuna: {}\n\n# ------------------------------------------------------------------------------\n# Hydra runtime dirs & logging\n# ------------------------------------------------------------------------------\n\nhydra:\n  run:\n    dir: ./outputs/${now:%Y-%m-%d}_${now:%H-%M-%S}_${run.run_id}\n  job_logging:\n    version: 1\n    root:\n      level: INFO\n  hydra_logging:\n    version: 1\n    root:\n      level: INFO"
}
