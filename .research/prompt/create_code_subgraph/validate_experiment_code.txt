
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics using `wandb.log()`
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves ALL data from WandB API using `wandb.Api()` (not from local files)
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
{
    "Open Problems": "Most popular test-time adaptation (TTA) methods such as TENT adapt only BatchNorm affine parameters by minimizing prediction entropy on the incoming test stream. Although very effective, they often require 3-10 gradient steps for each incoming mini-batch to reach their best performance. The slow convergence mainly comes from noisy gradients produced by low-confidence (high-entropy) samples that dominate early in adaptation. A simple mechanism to down-weight these uncertain samples during optimization is missing.",
    "Methods": "Confidence-Weighted Entropy Minimization (CW-TENT).\n1. Keep the original TENT objective L = Σ_i H(p_i) where H is entropy.\n2. Introduce a scalar weight per sample w_i = 1 – H(p_i)/log(C)  (ranges in [0,1]; C = #classes).\n3. Replace the loss with the weighted variant  L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\n4. Use the same SGD update of BatchNorm affine parameters, but with a single gradient step per mini-batch (optionally with momentum=0.9).\nTheoretical intuition: high-confidence samples (low entropy) are already close to the target domain optimum and provide reliable gradients; amplifying their contribution yields a cleaner gradient direction, allowing larger learning rate or fewer steps, hence faster convergence.",
    "Experimental Setup": "Dataset: CIFAR-10-C with 15 corruption types, severity 5 (standard TTA benchmark).\nModel: Pre-trained ResNet-18.\nBaselines: 1) Source model (no adaptation). 2) Original TENT (default 10 inner steps). 3) CW-TENT (1 inner step).\nMetric: Top-1 accuracy after processing the full test stream; also accuracy after first K (e.g., 1, 3) adaptation steps to measure convergence speed.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef entropy(p):\n    return -(p * p.log()).sum(1)\n\nclass CWTentAdapter:\n    def __init__(self, model, lr=1e-3, momentum=0.9):\n        self.model = model.eval()\n        self.model.requires_grad_(False)\n        # enable gradients for affine BN params only\n        for m in model.modules():\n            if isinstance(m, torch.nn.BatchNorm2d):\n                m.weight.requires_grad_(True)\n                m.bias.requires_grad_(True)\n        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n\n    @torch.no_grad()\n    def predict(self, x):\n        return self.model(x).softmax(1)\n\n    def adapt(self, x):\n        self.model.train()        # BN uses batch statistics\n        self.optimizer.zero_grad()\n        logits = self.model(x)\n        probs = logits.softmax(1)\n        H = entropy(probs)\n        w = 1 - H / torch.log(torch.tensor(probs.size(1), device=x.device))\n        loss = (w * H).sum() / w.sum()\n        loss.backward()\n        self.optimizer.step()\n        self.model.eval()\n        return probs.detach()\n",
    "Expected Result": "With only one gradient step per mini-batch, CW-TENT is expected to match or surpass the accuracy of TENT that uses 5–10 steps. For example on CIFAR-10-C severity 5: Source 58.7%, TENT (10 steps) 71.0%, CW-TENT (1 step) ≈71.5%. Convergence plot should show CW-TENT reaching peak accuracy after the first step, whereas TENT needs several.",
    "Expected Conclusion": "A tiny modification—confidence-weighted entropy—suppresses noisy gradients from uncertain samples and lets TENT converge in a single step. The change is trivial to implement (four extra lines) yet meaningfully cuts computation and latency at test time, making TTA more practical for real-time deployment."
}

# Experimental Design
- Strategy: This experiment evaluates the ability of Confidence-Weighted TENT (CW-TENT) to accelerate and improve test-time adaptation (TTA) of image classifiers under distribution shift. A pre-trained ResNet-18 is exposed to a streaming evaluation set composed of CIFAR-10-C (corruption severity 5). At every incoming mini-batch the model adapts only the affine parameters of its Batch-Norm layers. CW-TENT assigns each sample a confidence weight w_i = 1 – H(p_i)/log(C) and minimizes the weighted entropy L_w = Σ w_i·H(p_i) / Σ w_i with a single SGD update per batch. Performance is tracked online and compared against: (1) the static source model (no adaptation) and (2) vanilla TENT that performs 10 inner gradient steps of un-weighted entropy minimization. The main workflow is: 1) load the source checkpoint, 2) build three adapters (None, TENT, CW-TENT), 3) iterate over the corruption stream, calling adapter.adapt(x) then measuring accuracy, 4) aggregate final and early-step accuracies, 5) perform a small hyper-parameter grid search over learning-rate, momentum and weight-temperature for CW-TENT. All code runs on a single A100 GPU but can be parallelised across the 8 available devices for faster hyper-parameter sweeps. The primary goal is to demonstrate that CW-TENT reaches or surpasses the accuracy of TENT while using only one gradient step, reducing latency and computation.
- Proposed Method: Confidence-Weighted TENT (CW-TENT) augments the original Test-time Entropy Minimisation (TENT) framework with per-sample confidence weights to dampen the influence of uncertain (high-entropy) predictions during adaptation.

Objective: For a mini-batch of predictions p_i over C classes, compute entropy H(p_i) = −Σ_c p_{ic} log p_{ic}. Define weight w_i = 1 − H(p_i)/log(C) ∈ [0,1]. Optimise the weighted entropy
    L_w = Σ_i w_i · H(p_i) / Σ_i w_i.

Optimisation protocol:
1. Freeze all network parameters except γ, β of BatchNorm layers; keep them in training mode so that batch statistics are updated.
2. For each incoming batch x:
   a. Forward pass → logits → softmax p.
   b. Compute H(p), w, and L_w.
   c. One SGD step on BN affine params (lr≈1e-3, momentum optional).
   d. Switch model back to eval mode for inference until next batch arrives.

Theoretical intuition: samples with low entropy have gradients that more accurately point toward the target-domain optimum. Re-weighting removes noise caused by high-entropy instances, allowing the optimiser to converge in a single step.

Implementation: apart from four extra lines to compute w and L_w, the code is identical to TENT. No additional memory or forward passes are required, preserving TENT’s simplicity while cutting adaptation steps 10×.
- Evaluation Metrics: ['Top-1 Accuracy']

# Experiment Runs

- Run ID: proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5
  Method: proposed
  Model: ResNet-18 (11.7M)
  Dataset: CIFAR-10-C (severity 5)
  
  Config Content:
  ```yaml
  run_id: proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5
method: proposed
model:
  name: resnet18.a1_in1k
  architecture: ResNet-18
  num_parameters: 11.7M
  pretrained: true
  checkpoint: timm/resnet18.a1_in1k
  trainable_parameters: batchnorm_affine

dataset:
  name: cifar10-c
  corruption_severity: 5
  corruption_types: all
  split: test
  image_size: 32
  normalization: cifar

training:
  adaptation_algorithm: cw-tent
  objective: confidence_weighted_entropy
  batch_size: 128    # default, will be tuned by Optuna
  learning_rate: 0.001  # default, will be tuned by Optuna
  optimizer: sgd
  momentum: 0.9
  weight_temperature: 1.0
  inner_steps: 1  # single gradient step per batch
  weight_decay: 0.0
  epochs: 1  # single pass over the streaming test set
  shuffle_stream: false

optuna:
  n_trials: 40
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 5e-4
      high: 1e-2
    momentum:
      type: categorical
      choices: [0.0, 0.5, 0.9]
    batch_size:
      type: categorical
      choices: [64, 128, 256]
    weight_temperature:
      type: uniform
      low: 0.8
      high: 1.2

other:
  seed: 42
  device: cuda
  num_workers: 8

  ```
  

- Run ID: comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5
  Method: comparative-1
  Model: ResNet-18 (11.7M)
  Dataset: CIFAR-10-C (severity 5)
  
  Config Content:
  ```yaml
  run_id: comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5
method: comparative-1
model:
  name: resnet18.a1_in1k
  architecture: ResNet-18
  num_parameters: 11.7M
  pretrained: true
  checkpoint: timm/resnet18.a1_in1k
  trainable_parameters: batchnorm_affine

dataset:
  name: cifar10-c
  corruption_severity: 5
  corruption_types: all
  split: test
  image_size: 32
  normalization: cifar

training:
  adaptation_algorithm: tent
  objective: entropy_minimization
  batch_size: 128    # default, will be tuned by Optuna
  learning_rate: 0.001  # default, will be tuned by Optuna
  optimizer: sgd
  momentum: 0.9
  inner_steps: 10  # standard TENT
  weight_decay: 0.0
  epochs: 1
  shuffle_stream: false

optuna:
  n_trials: 40
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 5e-4
      high: 1e-2
    momentum:
      type: categorical
      choices: [0.0, 0.5, 0.9]
    batch_size:
      type: categorical
      choices: [64, 128, 256]
    inner_steps:
      type: int
      low: 3
      high: 10

other:
  seed: 42
  device: cuda
  num_workers: 8

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# Placeholder for run identifier (passed via CLI as run=\u003cid\u003e)\nrun: null\n\n# General output directory (can be overridden via CLI)\nresults_dir: outputs\n\n# Toggle trial mode for CI validation\ntrial_mode: false\n\n# -----------------------------------------------------------------------------\n# WandB configuration\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251020-test\n  mode: online  # automatically overridden to \"disabled\" in trial_mode\n\n# -----------------------------------------------------------------------------\n# Model (ResNet-18 by default)\n# -----------------------------------------------------------------------------\nmodel:\n  name: resnet18.a1_in1k\n  pretrained: true\n\n# -----------------------------------------------------------------------------\n# Dataset (CIFAR-10-C default)\n# -----------------------------------------------------------------------------\ndataset:\n  name: cifar10-c\n  corruption_severity: 5\n  corruption_types: all\n  split: test\n  image_size: 32\n  normalization: cifar\n\n# -----------------------------------------------------------------------------\n# Training / adaptation parameters (CW-TENT default)\n# -----------------------------------------------------------------------------\ntraining:\n  adaptation_algorithm: cw-tent  # options: none | tent | cw-tent\n  batch_size: 128\n  learning_rate: 0.001\n  optimizer: sgd\n  momentum: 0.9\n  weight_temperature: 1.0\n  inner_steps: 1\n  weight_decay: 0.0\n  epochs: 1\n  shuffle_stream: false\n\n# -----------------------------------------------------------------------------\n# Optuna hyper-parameter search (disabled by default)\n# -----------------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# -----------------------------------------------------------------------------\n# Miscellaneous\n# -----------------------------------------------------------------------------\nother:\n  seed: 42\n  device: cuda\n  num_workers: 4", "evaluate_py": "import argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom scipy import stats\n\nsns.set(style=\"whitegrid\")\n\n########################################################################################################################\n# Helper utilities\n########################################################################################################################\n\ndef mkdir(p: Path) -\u003e None:\n    p.mkdir(parents=True, exist_ok=True)\n\n\ndef save_json(obj: Dict, path: Path) -\u003e None:\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n########################################################################################################################\n# Plot helpers\n########################################################################################################################\n\ndef plot_learning_curve(df: pd.DataFrame, run_id: str, out_path: Path) -\u003e None:\n    if df.empty or \"batch_acc\" not in df.columns:\n        return\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(data=df, x=\"step\", y=\"batch_acc\", marker=\"o\")\n    plt.title(f\"Learning curve \u2013 {run_id}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Batch accuracy\")\n    best = df[\"batch_acc\"].max()\n    plt.annotate(f\"Best={best:.3f}\", xy=(df[\"step\"].iloc[-1], df[\"batch_acc\"].iloc[-1]))\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef plot_confusion_matrix(cm: np.ndarray, run_id: str, out_path: Path) -\u003e None:\n    if cm.size == 0:\n        return\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"Confusion matrix \u2013 {run_id}\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef plot_bar_comparison(df: pd.DataFrame, out_path: Path) -\u003e None:\n    if df.empty:\n        return\n    plt.figure(figsize=(8, 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"final_accuracy\")\n    plt.title(\"Final accuracy comparison\")\n    for i, row in df.iterrows():\n        plt.text(i, row[\"final_accuracy\"] + 0.005, f\"{row[\u0027final_accuracy\u0027]:.3f}\", ha=\"center\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef plot_batch_acc_distribution(batch_dict: Dict[str, List[float]], out_path: Path) -\u003e None:\n    if not batch_dict:\n        return\n    plt.figure(figsize=(8, 4))\n    data = []\n    for rid, vals in batch_dict.items():\n        for v in vals:\n            data.append({\"run_id\": rid, \"batch_acc\": v})\n    df = pd.DataFrame(data)\n    sns.boxplot(data=df, x=\"run_id\", y=\"batch_acc\")\n    plt.title(\"Batch accuracy distribution across runs\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n########################################################################################################################\n# Per-run processing\n########################################################################################################################\n\ndef process_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: Path) -\u003e Tuple[Dict, List[float]]:\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history_df = run.history(keys=[\"step\", \"batch_acc\"], samples=10000)\n    final_accuracy = run.summary.get(\"final_accuracy\")\n    cm_list = run.summary.get(\"confusion_matrix\")\n    confusion_matrix = np.array(cm_list) if cm_list is not None else np.empty((0, 0))\n\n    mkdir(out_dir)\n    metrics = {\"run_id\": run_id, \"final_accuracy\": final_accuracy}\n    save_json(metrics, out_dir / \"metrics.json\")\n\n    plot_learning_curve(history_df, run_id, out_dir / \"learning_curve.pdf\")\n    if cm_list is not None:\n        plot_confusion_matrix(confusion_matrix, run_id, out_dir / \"confusion_matrix.pdf\")\n\n    # Print generated file paths --------------------------------------------------------\n    for p in (out_dir / \"metrics.json\", out_dir / \"learning_curve.pdf\", out_dir / \"confusion_matrix.pdf\"):\n        print(str(p))\n\n    batch_vals = history_df[\"batch_acc\"].dropna().tolist() if \"batch_acc\" in history_df else []\n    return metrics, batch_vals\n\n########################################################################################################################\n# Aggregated analysis\n########################################################################################################################\n\ndef aggregated_analysis(all_metrics: List[Dict], batch_dict: Dict[str, List[float]], comparison_dir: Path) -\u003e None:\n    mkdir(comparison_dir)\n\n    df = pd.DataFrame(all_metrics)\n    save_json(df.to_dict(orient=\"records\"), comparison_dir / \"aggregated_metrics.json\")\n\n    bar_path = comparison_dir / \"final_accuracy_comparison.pdf\"\n    plot_bar_comparison(df, bar_path)\n    print(str(bar_path))\n\n    box_path = comparison_dir / \"batch_acc_distribution.pdf\"\n    plot_batch_acc_distribution(batch_dict, box_path)\n    print(str(box_path))\n\n    # Statistical significance (Welch\u0027s t-test) ----------------------------------------\n    sig_results = {}\n    run_ids = list(batch_dict.keys())\n    for i in range(len(run_ids)):\n        for j in range(i + 1, len(run_ids)):\n            r1, r2 = run_ids[i], run_ids[j]\n            vals1, vals2 = batch_dict[r1], batch_dict[r2]\n            if len(vals1) \u003e 1 and len(vals2) \u003e 1:\n                t_stat, p_val = stats.ttest_ind(vals1, vals2, equal_var=False)\n                sig_results[f\"{r1}_vs_{r2}\"] = {\"t_stat\": float(t_stat), \"p_value\": float(p_val)}\n    save_json(sig_results, comparison_dir / \"significance_tests.json\")\n    print(str(comparison_dir / \"significance_tests.json\"))\n\n########################################################################################################################\n# Entry point\n########################################################################################################################\n\ndef main() -\u003e None:  # noqa: D401\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\u0027JSON list e.g. \"[\\\"run1\\\", \\\"run2\\\"]\"\u0027)\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(\n            f\"Config not found at {cfg_path}. Ensure that training finished and results_dir is correct.\"\n        )\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n    all_metrics: List[Dict] = []\n    batch_dict: Dict[str, List[float]] = {}\n\n    for rid in run_ids:\n        out_dir = results_dir / rid\n        metrics, batch_vals = process_run(api, entity, project, rid, out_dir)\n        all_metrics.append(metrics)\n        batch_dict[rid] = batch_vals\n\n    aggregated_analysis(all_metrics, batch_dict, results_dir / \"comparison\")\n\n\nif __name__ == \"__main__\":\n    main()", "main_py": "import subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n########################################################################################################################\n# Orchestrator \u2013 launches src.train as subprocess\n########################################################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:  # noqa: D401\n    # -------------------------------------------------------------------------\n    # Validate \u0026 extract run_id\n    # -------------------------------------------------------------------------\n    if \"run\" not in cfg or cfg.run in {None, \"\", \"null\"}:\n        raise ValueError(\"Missing CLI argument: run=\u003crun_id\u003e\")\n    run_id: str = str(cfg.run)\n\n    # Sub-process command construction ----------------------------------------\n    overrides: List[str] = [f\"run_id={run_id}\", f\"results_dir={cfg.results_dir}\"]\n\n    if cfg.get(\"trial_mode\", False):\n        overrides.extend([\"trial_mode=true\", \"wandb.mode=disabled\", \"optuna.n_trials=0\"])\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"Launching subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "import os\n\nimport timm\nimport torch\n\n########################################################################################################################\n# Model utilities\n########################################################################################################################\n\nos.environ.setdefault(\"TORCH_HOME\", \".cache/\")  # timm \u0026 torch-hub cache\n\n\ndef build_model(model_cfg):\n    \"\"\"Instantiate a timm model according to the provided cfg.\"\"\"\n    pretrained = bool(getattr(model_cfg, \"pretrained\", True))\n    model = timm.create_model(model_cfg.name, pretrained=pretrained)\n    model.eval()\n    return model\n\n\ndef enable_bn_adaptation(model: torch.nn.Module) -\u003e None:\n    \"\"\"Enable gradients for BatchNorm affine parameters only (TENT style).\"\"\"\n    for m in model.modules():\n        if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n            for p in m.parameters(recurse=False):\n                p.requires_grad_(True)\n        else:\n            for p in m.parameters(recurse=False):\n                p.requires_grad_(False)", "preprocess_py": "from typing import List\n\nimport torch\nimport torchvision.transforms as T\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\n\n# CIFAR-10 statistics -----------------------------------------------------------------------------\nCIFAR_MEAN = (0.4914, 0.4822, 0.4465)\nCIFAR_STD = (0.2023, 0.1994, 0.2010)\n\n########################################################################################################################\n# Dataset wrapper (CIFAR-10-C)\n########################################################################################################################\n\n\nclass CIFAR10C(Dataset):\n    \"\"\"HuggingFace \u0027cifar10_corrupted\u0027 wrapper with corruption filtering.\"\"\"\n\n    def __init__(\n        self,\n        split: str,\n        severity: int,\n        corruption_types: List[str] | str = \"all\",\n        cache_dir: str = \".cache/\",\n    ) -\u003e None:\n        self.ds = load_dataset(\"cifar10_corrupted\", split=split, cache_dir=cache_dir)\n        self.ds = self.ds.filter(lambda e: e[\"corruption_severity\"] == severity)\n        if corruption_types != \"all\":\n            allowed = set(corruption_types)\n            self.ds = self.ds.filter(lambda e: e[\"corruption_type\"] in allowed)\n        self.ds = self.ds.remove_columns(\n            [c for c in self.ds.column_names if c not in {\"image\", \"label\"}]\n        )\n        self.n_classes = 10\n\n    def __len__(self) -\u003e int:\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        record = self.ds[idx]\n        return record[\"image\"], int(record[\"label\"])\n\n########################################################################################################################\n# Dataloader builder\n########################################################################################################################\n\ndef build_transform(image_size: int, normalization: str):\n    if normalization == \"cifar\":\n        mean, std = CIFAR_MEAN, CIFAR_STD\n    else:  # ImageNet statistics\n        mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n    return T.Compose(\n        [\n            T.Resize(image_size, interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean, std),\n        ]\n    )\n\n\ndef build_dataloader(cfg):  # cfg is DictConfig\n    transform = build_transform(cfg.dataset.image_size, cfg.dataset.normalization)\n\n    base_ds = CIFAR10C(\n        split=cfg.dataset.split,\n        severity=cfg.dataset.corruption_severity,\n        corruption_types=cfg.dataset.corruption_types,\n        cache_dir=\".cache/\",\n    )\n\n    class _Wrapped(Dataset):\n        def __init__(self, inner_ds, tf):\n            self.inner_ds = inner_ds\n            self.tf = tf\n\n        def __len__(self):\n            return len(self.inner_ds)\n\n        def __getitem__(self, i):\n            img, label = self.inner_ds[i]\n            return self.tf(img), torch.tensor(label, dtype=torch.long)\n\n    ds = _Wrapped(base_ds, transform)\n    loader = DataLoader(\n        ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=cfg.training.shuffle_stream,\n        num_workers=cfg.other.num_workers,\n        pin_memory=True,\n    )\n    return loader", "pyproject_toml": "[tool.poetry]\nname = \"cw-tent-experiments\"\nversion = \"0.1.0\"\ndescription = \"Reproducible experiments for CW-TENT paper\"\nauthors = [\"AI Researcher \u003cai@researcher.com\u003e\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\ntorch = \"^2.0.0\"\ntorchvision = \"^0.15.0\"\ntimm = \"^0.9.2\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\noptuna = \"^3.3.0\"\ndatasets = \"^2.14.4\"\nseaborn = \"^0.12.2\"\nmatplotlib = \"^3.7.2\"\nPyYAML = \"^6.0\"\npandas = \"^2.0.3\"\nscipy = \"^1.10.0\"\ntqdm = \"^4.65.0\"\n\n[build-system]\nrequires = [\"poetry-core\u003e=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"", "train_py": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport hydra\nimport optuna\nimport torch\nimport torch.nn.functional as F  # noqa: F401 \u2013 kept for potential extensions\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom tqdm import tqdm\n\nfrom src.model import build_model, enable_bn_adaptation\nfrom src.preprocess import build_dataloader\n\n########################################################################################################################\n# Utility helpers\n########################################################################################################################\n\nos.environ.setdefault(\"WANDB_CACHE_DIR\", \".cache/\")\nos.environ.setdefault(\"TORCH_HOME\", \".cache/\")  # timm \u0026 torch hub\n\n\ndef set_seed(seed: int) -\u003e None:\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef entropy(p: torch.Tensor) -\u003e torch.Tensor:\n    eps = 1e-8\n    return -(p * (p + eps).log()).sum(dim=1)\n\n\ndef accuracy(pred: torch.Tensor, trg: torch.Tensor) -\u003e torch.Tensor:\n    return (pred.argmax(dim=1) == trg).float().mean()\n\n########################################################################################################################\n# Adaptation algorithms\n########################################################################################################################\n\nclass BaseAdapter:\n    \"\"\"No adaptation \u2013 frozen network.\"\"\"\n\n    def __init__(self, model: torch.nn.Module):\n        self.model = model.eval()\n        self.model.requires_grad_(False)\n\n    @torch.no_grad()\n    def adapt(self, x: torch.Tensor) -\u003e torch.Tensor:  # noqa: D401\n        return self.model(x).softmax(dim=1)\n\n\nclass TentAdapter(BaseAdapter):\n    \"\"\"Original TENT (entropy minimisation).\"\"\"\n\n    def __init__(self, model: torch.nn.Module, lr: float, momentum: float, inner_steps: int):\n        enable_bn_adaptation(model)\n        params = [p for p in model.parameters() if p.requires_grad]\n        self.opt = torch.optim.SGD(params, lr=lr, momentum=momentum)\n        self.inner_steps = inner_steps\n        super().__init__(model)\n\n    def adapt(self, x: torch.Tensor) -\u003e torch.Tensor:  # type: ignore[override]\n        self.model.train()\n        output = None\n        for _ in range(self.inner_steps):\n            self.opt.zero_grad()\n            output = self.model(x)\n            loss = entropy(output.softmax(1)).mean()\n            loss.backward()\n            self.opt.step()\n        self.model.eval()\n        assert output is not None\n        return output.detach().softmax(1)\n\n\nclass CWTentAdapter(BaseAdapter):\n    \"\"\"Proposed Confidence-Weighted TENT.\"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        lr: float,\n        momentum: float,\n        temperature: float,\n    ) -\u003e None:\n        enable_bn_adaptation(model)\n        params = [p for p in model.parameters() if p.requires_grad]\n        self.opt = torch.optim.SGD(params, lr=lr, momentum=momentum)\n        self.temp = temperature\n        super().__init__(model)\n\n    def adapt(self, x: torch.Tensor) -\u003e torch.Tensor:  # type: ignore[override]\n        self.model.train()\n        self.opt.zero_grad()\n        logits = self.model(x)\n        probs = logits.softmax(1)\n        H = entropy(probs)\n        num_classes = probs.size(1)\n        weights = 1.0 - H / math.log(num_classes)\n        weights = weights.pow(self.temp)\n        loss = (weights * H).sum() / weights.sum()\n        loss.backward()\n        self.opt.step()\n        self.model.eval()\n        return logits.detach().softmax(1)\n\n########################################################################################################################\n# Core routine (single run)\n########################################################################################################################\n\ndef run_single(cfg: DictConfig) -\u003e Dict[str, Any]:\n    set_seed(int(cfg.other.seed))\n\n    device = torch.device(cfg.other.device if torch.cuda.is_available() else \"cpu\")\n\n    # ----------------------------------------------------------------------------------\n    # Build model \u0026 data\n    # ----------------------------------------------------------------------------------\n    model = build_model(cfg.model)\n    model.to(device)\n\n    loader = build_dataloader(cfg)\n\n    # Trial-mode: shorten dataloader ---------------------------------------------------------------\n    if cfg.get(\"trial_mode\", False):\n        limited = []\n        for idx, batch in enumerate(loader):\n            if idx \u003e= 2:\n                break\n            limited.append(batch)\n        loader = limited  # type: ignore[assignment]\n\n    # ----------------------------------------------------------------------------------\n    # Select adaptation algorithm\n    # ----------------------------------------------------------------------------------\n    algo = str(cfg.training.adaptation_algorithm).lower()\n    if algo in {\"tent\"}:\n        adapter = TentAdapter(\n            model,\n            lr=float(cfg.training.learning_rate),\n            momentum=float(cfg.training.momentum),\n            inner_steps=int(cfg.training.inner_steps),\n        )\n    elif algo in {\"cw-tent\", \"cwtent\", \"cw_tent\"}:\n        adapter = CWTentAdapter(\n            model,\n            lr=float(cfg.training.learning_rate),\n            momentum=float(cfg.training.momentum),\n            temperature=float(cfg.training.weight_temperature),\n        )\n    else:\n        adapter = BaseAdapter(model)\n\n    # ----------------------------------------------------------------------------------\n    # WandB\n    # ----------------------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n        )\n        print(f\"WandB URL: {run.url}\")\n    else:\n        run = None\n\n    # ----------------------------------------------------------------------------------\n    # Online adaptation loop\n    # ----------------------------------------------------------------------------------\n    total_correct = 0\n    total_seen = 0\n    preds_all: List[int] = []\n    targets_all: List[int] = []\n\n    iterator = tqdm(enumerate(loader), total=len(loader), desc=\"Adapting\", disable=cfg.get(\"trial_mode\", False))\n    for step, (imgs, targets) in iterator:\n        imgs = imgs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        probs = adapter.adapt(imgs)\n        batch_acc = accuracy(probs, targets).item()\n\n        iterator.set_postfix(acc=f\"{batch_acc:.3f}\")\n\n        total_correct += (probs.argmax(1) == targets).sum().item()\n        total_seen += targets.size(0)\n\n        preds_all.extend(probs.argmax(1).cpu().tolist())\n        targets_all.extend(targets.cpu().tolist())\n\n        if run is not None:\n            run.log({\"batch_acc\": batch_acc, \"step\": step})\n\n    final_acc = total_correct / max(total_seen, 1)\n\n    # Confusion matrix ---------------------------------------------------------------------------\n    num_classes = int(max(max(preds_all), max(targets_all)) + 1) if preds_all else 1\n    conf_mat = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n    for t, p in zip(targets_all, preds_all):\n        conf_mat[t, p] += 1\n\n    if run is not None:\n        run.log({\"final_accuracy\": final_acc})\n        run.summary[\"final_accuracy\"] = final_acc\n        run.summary[\"confusion_matrix\"] = conf_mat.tolist()\n        run.finish()\n\n    print(f\"Final accuracy for run_id={cfg.run_id}: {final_acc:.4f}\")\n    return {\"final_accuracy\": final_acc}\n\n########################################################################################################################\n# Optuna wrapper\n########################################################################################################################\n\ndef optuna_objective(trial: optuna.Trial, base_cfg: DictConfig) -\u003e float:\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))  # deepcopy\n\n    # Inject hyper-parameters -----------------------------------------------------------\n    for hp, space in base_cfg.optuna.search_space.items():\n        s_type = space[\"type\"]\n        if s_type == \"loguniform\":\n            cfg.training[hp] = trial.suggest_float(hp, float(space[\"low\"]), float(space[\"high\"]), log=True)\n        elif s_type == \"uniform\":\n            cfg.training[hp] = trial.suggest_float(hp, float(space[\"low\"]), float(space[\"high\"]))\n        elif s_type == \"categorical\":\n            cfg.training[hp] = trial.suggest_categorical(hp, space[\"choices\"])\n        elif s_type == \"int\":\n            cfg.training[hp] = trial.suggest_int(hp, int(space[\"low\"]), int(space[\"high\"]))\n        else:\n            raise ValueError(f\"Unsupported Optuna space type: {s_type}\")\n\n    # Disable WandB during hyper-param search to avoid clutter\n    cfg.wandb.mode = \"disabled\"\n\n    return run_single(cfg)[\"final_accuracy\"]\n\n########################################################################################################################\n# Hydra entry-point\n########################################################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:  # noqa: D401\n    # -------------------------------------------------------------\n    # Ensure run_id is present (passed from main.py)\n    # -------------------------------------------------------------\n    if \"run_id\" not in cfg or cfg.run_id in {None, \"\", \"null\"}:\n        raise ValueError(\"cfg.run_id must be provided via CLI \u2013 e.g., run=\u003cid\u003e in orchestrator call.\")\n\n    # Persist the composed config so that evaluate.py can reuse it\n    Path(cfg.results_dir).mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, Path(cfg.results_dir) / \"config.yaml\")\n\n    # Optuna hyper-parameter optimisation ---------------------------------------------\n    if int(cfg.optuna.n_trials) \u003e 0 and not cfg.get(\"trial_mode\", False):\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=int(cfg.optuna.n_trials))\n        best_params = study.best_params\n        print(\"[Optuna] Best hyper-parameters:\", best_params)\n        for k, v in best_params.items():\n            cfg.training[k] = v\n        run_single(cfg)\n    else:\n        run_single(cfg)\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Hydra integration does not satisfy the required design:\n1. config/config.yaml does not declare a `defaults: - run: <run_id>` entry, nor is a `config/run/` folder used. The `run` argument therefore becomes a plain string instead of a config group, so the per-run YAML files specified in the specification cannot be loaded.\n2. Because no run YAML is loaded, critical fields (run_id/method etc.) are not populated from the run file but are instead passed manually from src.main, violating the requirement that \"all parameters are loaded from run configs dynamically\".\n\nUntil the config/run structure is added and referenced in the root defaults list, the experiment cannot be executed via the mandated CLI and thus is not production-ready."
}
