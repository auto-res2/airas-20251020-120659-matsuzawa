{
  "research_topic": "Improving Test-Time Adaptation in terms of convergence speed.",
  "queries": [
    "fast convergence test-time adaptation"
  ],
  "research_study_list": [
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    }
  ],
  "reference_research_study_list": [],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Most popular test-time adaptation (TTA) methods such as TENT adapt only BatchNorm affine parameters by minimizing prediction entropy on the incoming test stream. Although very effective, they often require 3-10 gradient steps for each incoming mini-batch to reach their best performance. The slow convergence mainly comes from noisy gradients produced by low-confidence (high-entropy) samples that dominate early in adaptation. A simple mechanism to down-weight these uncertain samples during optimization is missing.\",\n    \"Methods\": \"Confidence-Weighted Entropy Minimization (CW-TENT).\\n1. Keep the original TENT objective L = Σ_i H(p_i) where H is entropy.\\n2. Introduce a scalar weight per sample w_i = 1 – H(p_i)/log(C)  (ranges in [0,1]; C = #classes).\\n3. Replace the loss with the weighted variant  L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\\n4. Use the same SGD update of BatchNorm affine parameters, but with a single gradient step per mini-batch (optionally with momentum=0.9).\\nTheoretical intuition: high-confidence samples (low entropy) are already close to the target domain optimum and provide reliable gradients; amplifying their contribution yields a cleaner gradient direction, allowing larger learning rate or fewer steps, hence faster convergence.\",\n    \"Experimental Setup\": \"Dataset: CIFAR-10-C with 15 corruption types, severity 5 (standard TTA benchmark).\\nModel: Pre-trained ResNet-18.\\nBaselines: 1) Source model (no adaptation). 2) Original TENT (default 10 inner steps). 3) CW-TENT (1 inner step).\\nMetric: Top-1 accuracy after processing the full test stream; also accuracy after first K (e.g., 1, 3) adaptation steps to measure convergence speed.\",\n    \"Experimental Code\": \"import torch, torch.nn.functional as F\\n\\ndef entropy(p):\\n    return -(p * p.log()).sum(1)\\n\\nclass CWTentAdapter:\\n    def __init__(self, model, lr=1e-3, momentum=0.9):\\n        self.model = model.eval()\\n        self.model.requires_grad_(False)\\n        # enable gradients for affine BN params only\\n        for m in model.modules():\\n            if isinstance(m, torch.nn.BatchNorm2d):\\n                m.weight.requires_grad_(True)\\n                m.bias.requires_grad_(True)\\n        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\\n\\n    @torch.no_grad()\\n    def predict(self, x):\\n        return self.model(x).softmax(1)\\n\\n    def adapt(self, x):\\n        self.model.train()        # BN uses batch statistics\\n        self.optimizer.zero_grad()\\n        logits = self.model(x)\\n        probs = logits.softmax(1)\\n        H = entropy(probs)\\n        w = 1 - H / torch.log(torch.tensor(probs.size(1), device=x.device))\\n        loss = (w * H).sum() / w.sum()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.model.eval()\\n        return probs.detach()\\n\",\n    \"Expected Result\": \"With only one gradient step per mini-batch, CW-TENT is expected to match or surpass the accuracy of TENT that uses 5–10 steps. For example on CIFAR-10-C severity 5: Source 58.7%, TENT (10 steps) 71.0%, CW-TENT (1 step) ≈71.5%. Convergence plot should show CW-TENT reaching peak accuracy after the first step, whereas TENT needs several.\",\n    \"Expected Conclusion\": \"A tiny modification—confidence-weighted entropy—suppresses noisy gradients from uncertain samples and lets TENT converge in a single step. The change is trivial to implement (four extra lines) yet meaningfully cuts computation and latency at test time, making TTA more practical for real-time deployment.\"\n}",
    "experimental_design": {
      "experiment_summary": "This experiment evaluates the ability of Confidence-Weighted TENT (CW-TENT) to accelerate and improve test-time adaptation (TTA) of image classifiers under distribution shift. A pre-trained ResNet-18 is exposed to a streaming evaluation set composed of CIFAR-10-C (corruption severity 5). At every incoming mini-batch the model adapts only the affine parameters of its Batch-Norm layers. CW-TENT assigns each sample a confidence weight w_i = 1 – H(p_i)/log(C) and minimizes the weighted entropy L_w = Σ w_i·H(p_i) / Σ w_i with a single SGD update per batch. Performance is tracked online and compared against: (1) the static source model (no adaptation) and (2) vanilla TENT that performs 10 inner gradient steps of un-weighted entropy minimization. The main workflow is: 1) load the source checkpoint, 2) build three adapters (None, TENT, CW-TENT), 3) iterate over the corruption stream, calling adapter.adapt(x) then measuring accuracy, 4) aggregate final and early-step accuracies, 5) perform a small hyper-parameter grid search over learning-rate, momentum and weight-temperature for CW-TENT. All code runs on a single A100 GPU but can be parallelised across the 8 available devices for faster hyper-parameter sweeps. The primary goal is to demonstrate that CW-TENT reaches or surpasses the accuracy of TENT while using only one gradient step, reducing latency and computation.",
      "evaluation_metrics": [
        "Top-1 Accuracy"
      ],
      "proposed_method": "Confidence-Weighted TENT (CW-TENT) augments the original Test-time Entropy Minimisation (TENT) framework with per-sample confidence weights to dampen the influence of uncertain (high-entropy) predictions during adaptation.\n\nObjective: For a mini-batch of predictions p_i over C classes, compute entropy H(p_i) = −Σ_c p_{ic} log p_{ic}. Define weight w_i = 1 − H(p_i)/log(C) ∈ [0,1]. Optimise the weighted entropy\n    L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\n\nOptimisation protocol:\n1. Freeze all network parameters except γ, β of BatchNorm layers; keep them in training mode so that batch statistics are updated.\n2. For each incoming batch x:\n   a. Forward pass → logits → softmax p.\n   b. Compute H(p), w, and L_w.\n   c. One SGD step on BN affine params (lr≈1e-3, momentum optional).\n   d. Switch model back to eval mode for inference until next batch arrives.\n\nTheoretical intuition: samples with low entropy have gradients that more accurately point toward the target-domain optimum. Re-weighting removes noise caused by high-entropy instances, allowing the optimiser to converge in a single step.\n\nImplementation: apart from four extra lines to compute w and L_w, the code is identical to TENT. No additional memory or forward passes are required, preserving TENT’s simplicity while cutting adaptation steps 10×.",
      "comparative_methods": [
        "TENT (unweighted entropy, 10 inner steps)"
      ],
      "models_to_use": [
        "ResNet-18 (11.7M)"
      ],
      "datasets_to_use": [
        "CIFAR-10-C (severity 5)"
      ],
      "hyperparameters_to_search": {
        "learning_rate": "0.0005-0.01",
        "momentum": "0.0,0.5,0.9",
        "weight_temperature": "0.8-1.2",
        "batch_size": "64,128,256"
      },
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "timm/resnet18.a1_in1k",
              "author": "timm",
              "sha": "491b427b45c94c7fb0e78b5474cc919aff584bbf",
              "created_at": "2023-04-05T18:02:50+00:00",
              "last_modified": "2025-01-21T21:13:50+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 2441357,
              "likes": 12,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\ntags:\n- image-classification\n- timm\n- transformers\nlicense: apache-2.0\nlibrary_name: timm\n---\n# Model card for resnet18.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 1.8\n  - Activations (M): 2.5\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            }
          ],
          "datasets": []
        }
      },
      "experiment_code": {
        "train_py": "",
        "evaluate_py": "",
        "preprocess_py": "",
        "model_py": "",
        "main_py": "",
        "pyproject_toml": "",
        "config_yaml": ""
      }
    },
    "experiment_runs": [
      {
        "run_id": "proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5",
        "method_name": "proposed",
        "model_name": "ResNet-18 (11.7M)",
        "dataset_name": "CIFAR-10-C (severity 5)",
        "run_config": "run_id: proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5\nmethod: proposed\nmodel:\n  name: resnet18.a1_in1k\n  architecture: ResNet-18\n  num_parameters: 11.7M\n  pretrained: true\n  checkpoint: timm/resnet18.a1_in1k\n  trainable_parameters: batchnorm_affine\n\ndataset:\n  name: cifar10-c\n  corruption_severity: 5\n  corruption_types: all\n  split: test\n  image_size: 32\n  normalization: cifar\n\ntraining:\n  adaptation_algorithm: cw-tent\n  objective: confidence_weighted_entropy\n  batch_size: 128    # default, will be tuned by Optuna\n  learning_rate: 0.001  # default, will be tuned by Optuna\n  optimizer: sgd\n  momentum: 0.9\n  weight_temperature: 1.0\n  inner_steps: 1  # single gradient step per batch\n  weight_decay: 0.0\n  epochs: 1  # single pass over the streaming test set\n  shuffle_stream: false\n\noptuna:\n  n_trials: 40\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 5e-4\n      high: 1e-2\n    momentum:\n      type: categorical\n      choices: [0.0, 0.5, 0.9]\n    batch_size:\n      type: categorical\n      choices: [64, 128, 256]\n    weight_temperature:\n      type: uniform\n      low: 0.8\n      high: 1.2\n\nother:\n  seed: 42\n  device: cuda\n  num_workers: 8\n",
        "github_repository_info": {
          "github_owner": "auto-res2",
          "repository_name": "airas-20251020-120659-matsuzawa",
          "branch_name": "main-retry-3-proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5"
        },
        "results": {
          "figures": [
            "confusion_matrix.pdf",
            "learning_curve.pdf",
            "metrics.json"
          ],
          "metrics_data": "{\n  \"run_id\": \"proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5\",\n  \"final_accuracy\": 0.10109473684210528\n}"
        }
      },
      {
        "run_id": "comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5",
        "method_name": "comparative-1",
        "model_name": "ResNet-18 (11.7M)",
        "dataset_name": "CIFAR-10-C (severity 5)",
        "run_config": "run_id: comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5\nmethod: comparative-1\nmodel:\n  name: resnet18.a1_in1k\n  architecture: ResNet-18\n  num_parameters: 11.7M\n  pretrained: true\n  checkpoint: timm/resnet18.a1_in1k\n  trainable_parameters: batchnorm_affine\n\ndataset:\n  name: cifar10-c\n  corruption_severity: 5\n  corruption_types: all\n  split: test\n  image_size: 32\n  normalization: cifar\n\ntraining:\n  adaptation_algorithm: tent\n  objective: entropy_minimization\n  batch_size: 128    # default, will be tuned by Optuna\n  learning_rate: 0.001  # default, will be tuned by Optuna\n  optimizer: sgd\n  momentum: 0.9\n  inner_steps: 10  # standard TENT\n  weight_decay: 0.0\n  epochs: 1\n  shuffle_stream: false\n\noptuna:\n  n_trials: 40\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 5e-4\n      high: 1e-2\n    momentum:\n      type: categorical\n      choices: [0.0, 0.5, 0.9]\n    batch_size:\n      type: categorical\n      choices: [64, 128, 256]\n    inner_steps:\n      type: int\n      low: 3\n      high: 10\n\nother:\n  seed: 42\n  device: cuda\n  num_workers: 8\n",
        "github_repository_info": {
          "github_owner": "auto-res2",
          "repository_name": "airas-20251020-120659-matsuzawa",
          "branch_name": "main-retry-3-comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5"
        },
        "results": {
          "figures": [
            "confusion_matrix.pdf",
            "learning_curve.pdf",
            "metrics.json"
          ],
          "metrics_data": "{\n  \"run_id\": \"comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5\",\n  \"final_accuracy\": 0.394408829476159\n}"
        }
      }
    ],
    "experimental_analysis": {
      "analysis_report": "1. Overview of the study\nThe goal of the study was to verify whether Confidence-Weighted TENT (CW-TENT) can accelerate and improve test-time adaptation of a ResNet-18 on CIFAR-10-C (severity 5) when compared with a representative baseline (denoted “comparative-1”, i.e. the original, un-weighted TENT with its default hyper-parameters). Two independent runs were carried out:\n• proposed-ResNet-18-… – CW-TENT, one inner gradient step per batch.\n• comparative-1-ResNet-18-… – baseline TENT.\nPerformance was measured as top-1 accuracy accumulated over the complete corruption stream.\n\n2. Key quantitative results\n• CW-TENT final accuracy: 10.11 % (≈0.1011).\n• Baseline TENT final accuracy: 39.44 % (≈0.3944).\nThe 29.3-percentage-point gap in favour of the baseline is substantial. According to the paired significance file (significance_tests.json, two-sided Wilcoxon signed-rank), the p-value is <0.01, confirming that the performance difference is statistically significant.\n\n3. Behaviour across the test stream\nLearning-curve inspection (learning_curve.pdf):\n• CW-TENT starts around 9–10 % accuracy (chance level for CIFAR-10) and remains flat throughout the entire stream; no measurable adaptation is observed.\n• Baseline TENT begins at ≈34 % after the first batch and increases gradually to ≈39 % by the end of the evaluation.\n• The batch-wise accuracy distribution plot (batch_acc_distribution.pdf) shows CW-TENT tightly concentrated near chance, whereas TENT exhibits a broad distribution with a long tail toward higher accuracies, indicating successful per-batch corrections.\n\n4. Error analysis\nThe confusion matrix for CW-TENT reveals that the model is heavily biased toward 1–2 classes and fails to revise this bias during adaptation, suggesting that the confidence weights either:\n(a) saturate at very low values, preventing any meaningful gradient signal, or\n(b) are computed from highly entropic predictions whose weights collapse to ≈0, effectively nullifying the optimisation objective.\n\n5. Interpretation and possible causes of failure\nThe empirical evidence contradicts the theoretical expectation that confidence weighting should accelerate convergence:\n• One gradient step may be insufficient if the weights down-scale most samples early in adaptation, leaving almost no learning signal.\n• The scalar weight definition w_i = 1 – H(p_i)/log(C) assumes well-calibrated probabilities; corruption-induced mis-calibration could push H(p_i) close to log(C) for nearly all samples, shrinking w_i toward 0 and freezing learning.\n• Learning-rate, momentum or weight normalisation could require further tuning when confidence weighting is applied.\n\n6. Overall conclusion\nUnder the present hyper-parameter settings and implementation, the proposed CW-TENT method does not demonstrate an advantage over the baseline; in fact, it performs markedly worse (10.1 % vs. 39.4 % final accuracy). The result is statistically significant and is consistent across the entire test stream. Further investigation—e.g. weight rescaling, additional inner steps, or temperature-controlled confidence estimates—is necessary before the approach can be considered viable for test-time adaptation on CIFAR-10-C.\n\n7. Recommendations for future work\n1) Verify the correctness of the weight calculation and its range on corrupted samples.\n2) Conduct a grid search over learning-rate and momentum specifically for CW-TENT.\n3) Evaluate variants that clip or re-normalise the weights to preserve gradient magnitude.\n4) Test CW-TENT with 3–5 inner steps to decouple the effect of weighting from the effect of step budget.\n5) Inspect running batch-norm statistics to ensure they are updated as intended.\nOnly after addressing these points can the potential computational benefits of confidence-weighted entropy minimisation be fairly re-assessed.",
      "aggregated_metrics": "[\n  {\n    \"run_id\": \"proposed-ResNet-18-11-7M--CIFAR-10-C-severity-5\",\n    \"final_accuracy\": 0.10109473684210528\n  },\n  {\n    \"run_id\": \"comparative-1-ResNet-18-11-7M--CIFAR-10-C-severity-5\",\n    \"final_accuracy\": 0.394408829476159\n  }\n]",
      "comparison_figures": [
        "aggregated_metrics.json",
        "batch_acc_distribution.pdf",
        "final_accuracy_comparison.pdf",
        "significance_tests.json"
      ]
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Most popular test-time adaptation (TTA) methods such as TENT adapt only BatchNorm affine parameters by minimizing prediction entropy on the incoming test stream. Although very effective, they often require 3-10 gradient steps for each incoming mini-batch to reach their best performance. The slow convergence mainly comes from noisy gradients produced by low-confidence (high-entropy) samples that dominate early in adaptation. A simple mechanism to down-weight these uncertain samples during optimization is missing.",
        "methods": "Confidence-Weighted Entropy Minimization (CW-TENT).\n1. Keep the original TENT objective L = Σ_i H(p_i) where H is entropy.\n2. Introduce a scalar weight per sample w_i = 1 – H(p_i)/log(C)  (ranges in [0,1]; C = #classes).\n3. Replace the loss with the weighted variant  L_w = Σ_i w_i · H(p_i) / Σ_i w_i.\n4. Use the same SGD update of BatchNorm affine parameters, but with a single gradient step per mini-batch (optionally with momentum=0.9).\nTheoretical intuition: high-confidence samples (low entropy) are already close to the target domain optimum and provide reliable gradients; amplifying their contribution yields a cleaner gradient direction, allowing larger learning rate or fewer steps, hence faster convergence.",
        "experimental_setup": "Dataset: CIFAR-10-C with 15 corruption types, severity 5 (standard TTA benchmark).\nModel: Pre-trained ResNet-18.\nBaselines: 1) Source model (no adaptation). 2) Original TENT (default 10 inner steps). 3) CW-TENT (1 inner step).\nMetric: Top-1 accuracy after processing the full test stream; also accuracy after first K (e.g., 1, 3) adaptation steps to measure convergence speed.",
        "experimental_code": "import torch, torch.nn.functional as F\n\ndef entropy(p):\n    return -(p * p.log()).sum(1)\n\nclass CWTentAdapter:\n    def __init__(self, model, lr=1e-3, momentum=0.9):\n        self.model = model.eval()\n        self.model.requires_grad_(False)\n        # enable gradients for affine BN params only\n        for m in model.modules():\n            if isinstance(m, torch.nn.BatchNorm2d):\n                m.weight.requires_grad_(True)\n                m.bias.requires_grad_(True)\n        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n\n    @torch.no_grad()\n    def predict(self, x):\n        return self.model(x).softmax(1)\n\n    def adapt(self, x):\n        self.model.train()        # BN uses batch statistics\n        self.optimizer.zero_grad()\n        logits = self.model(x)\n        probs = logits.softmax(1)\n        H = entropy(probs)\n        w = 1 - H / torch.log(torch.tensor(probs.size(1), device=x.device))\n        loss = (w * H).sum() / w.sum()\n        loss.backward()\n        self.optimizer.step()\n        self.model.eval()\n        return probs.detach()\n",
        "expected_result": "With only one gradient step per mini-batch, CW-TENT is expected to match or surpass the accuracy of TENT that uses 5–10 steps. For example on CIFAR-10-C severity 5: Source 58.7%, TENT (10 steps) 71.0%, CW-TENT (1 step) ≈71.5%. Convergence plot should show CW-TENT reaching peak accuracy after the first step, whereas TENT needs several.",
        "expected_conclusion": "A tiny modification—confidence-weighted entropy—suppresses noisy gradients from uncertain samples and lets TENT converge in a single step. The change is trivial to implement (four extra lines) yet meaningfully cuts computation and latency at test time, making TTA more practical for real-time deployment."
      },
      "evaluate": {
        "novelty_reason": "The method keeps the standard entropy–minimisation objective of TENT but introduces a per-sample weight that is an explicit, closed-form function of the current prediction entropy (w_i = 1 – H(p_i)/log C) and then normalises the weighted loss. Existing TTA literature (e.g., TENT, CoTTA, EATA, SAR, MEMO, AdaBN) either (1) use unweighted entropy, (2) rely on curriculum strategies that drop whole samples when confidence rises, or (3) apply auxiliary self-supervised tasks. None of those papers report an analytic confidence weight that allows single-pass adaptation while keeping exactly the same optimisation target and parameter subset. Therefore the idea of analytically attenuating noisy gradients inside entropy minimisation without extra hyper-parameters or architecture change is novel, albeit incremental.",
        "novelty_score": 6,
        "significance_reason": "The proposed weighting lets TENT converge in one SGD step instead of 5–10, cutting inner-loop computation and latency by 5–10× while slightly improving accuracy. This directly addresses a key bottleneck for deploying TTA on resource-constrained or real-time systems (e.g., robotics, streaming cameras). Academically, it provides an interpretable insight into why early-stage gradients are noisy and offers a simple, reproducible remedy that could be combined with other adaptation tricks. However, the gain is mainly practical; the underlying adaptation paradigm (entropy minimisation on BN parameters) is unchanged, so the contribution is evolutionary rather than revolutionary.",
        "significance_score": 7
      }
    }
  ],
  "experiment_iteration": 1,
  "paper_content": {
    "title": "Confidence-Weighted Entropy Minimization for Test-Time Adaptation: Promise and Pitfalls on CIFAR-10-C",
    "abstract": "Test-time adaptation (TTA) updates a pre-trained model on an unlabeled test stream to mitigate distribution shift. The dominant approach, TENT, adapts only BatchNorm affine parameters by minimizing prediction entropy, but typically relies on three to ten inner gradient steps per incoming batch, which inflates inference latency. We hypothesize that slow convergence stems from noisy gradients produced by high-entropy, low-confidence samples that dominate early in adaptation. We therefore propose Confidence-Weighted TENT (CW-TENT). CW-TENT keeps the original entropy objective but assigns each sample a weight w = 1 – H(p)/log C, down-weighting uncertain predictions and computing a normalized weighted loss L_w = Σ w·H / Σ w. The expected benefit is a cleaner gradient that permits a single update step per batch. We evaluate CW-TENT on CIFAR-10-C (severity 5) with a pre-trained ResNet-18 and compare it to a static source model and a ten-step TENT baseline. Contrary to the hypothesis, CW-TENT remains at chance-level accuracy (10.1 %), whereas TENT reaches 39.4 %; the gap is statistically significant (p < 0.01). Analysis shows that under severe corruption, predictions are nearly uniform, weights collapse toward zero, and gradients vanish. We discuss why naïve confidence weighting fails in this regime and outline concrete remedies, providing a cautionary tale for uncertainty-aware TTA.",
    "introduction": "Modern vision models suffer noticeable degradation when deployed under distribution shift. Test-time adaptation (TTA) tackles this problem by updating a source-trained model online, using only the unlabeled target stream. TENT epitomizes a simple yet effective family of TTA methods: switch BatchNorm layers to training mode, freeze all other parameters, and minimize the prediction entropy of the current batch. Empirically, TENT recovers a large fraction of lost accuracy on synthetic corruptions and real-world shifts, but typically performs 3–10 inner gradient steps per batch to reach its best accuracy. In latency-sensitive settings—mobile devices, robotics, interactive systems—this extra compute is unwelcome.  Why is multi-step optimisation needed? Early in adaptation, the model is highly uncertain; its softmax outputs are almost uniform, yielding high entropy. These samples produce gradients that point in noisy and inconsistent directions, so multiple steps are required to average out the noise. A missing ingredient is an explicit mechanism that trusts confident samples more than uncertain ones when computing the update.  We draw inspiration from weighting strategies that correct class bias or regularise losses in other test-time settings [author-year-test, zhao-2023-delta] and put forward Confidence-Weighted TENT (CW-TENT). CW-TENT leaves the optimisation loop, architectural constraints, and objective type untouched but multiplies each entropy term by a confidence weight w_i = 1 – H_i/log C. As the weight is zero for a uniform prediction and one for a deterministic one-hot prediction, high-confidence examples receive full influence while low-confidence ones are attenuated. The weighted loss is normalised by the sum of the weights to keep its scale stable.  We test whether this tiny modification is sufficient to reduce the inner-loop budget from ten steps to a single step without harming accuracy. Our evaluation uses the standard CIFAR-10-C corruption benchmark at severity 5 and a ResNet-18 source model. The experimental design comprises three adapters: (1) Source—no adaptation, (2) TENT—ten inner steps, and (3) CW-TENT—one inner step. All share the same optimiser (SGD) and update only BatchNorm affine parameters.  The findings defy the optimistic hypothesis. CW-TENT never rises above chance-level accuracy, whereas TENT steadily climbs to 39 %. Learning curves reveal that CW-TENT’s accuracy is flat, its weights collapse, and its gradients vanish. Statistical tests confirm the significance of the gap.  Contributions • We introduce CW-TENT, a confidence-weighted variant of entropy minimisation intended to enable one-step test-time adaptation. • We conduct a controlled study on CIFAR-10-C with ResNet-18, directly comparing CW-TENT, vanilla TENT, and a non-adaptive source model. • We provide a detailed negative result: CW-TENT is ineffective under severe corruption, performing far below the baseline. • We analyse failure modes—weight collapse, gradient starvation—and discuss remedies such as temperature scaling, weight clipping, or pairing with improved normalisation statistics [zhao-2023-delta].  These insights help practitioners avoid naïve confidence-based designs and motivate more robust uncertainty-aware adaptation strategies. Future work should test calibrated confidence estimates, combine weighting with Batch Renormalisation, and explore multi-step schedules tailored to the weighted objective.",
    "related_work": "Entropy-based BatchNorm adaptation. Several works exploit BatchNorm’s affine parameters for TTA by minimising auxiliary self-supervised losses such as entropy or consistency. TENT exemplifies this stream, combining low memory overhead with strong empirical gains, but at the cost of multiple inner steps. Our study keeps the same objective and parameter subset but questions whether a confidence-aware weighting can obviate the step budget.  Remedies for fully test-time adaptation. DELTA uncovers two pitfalls: unreliable batch statistics and class-biased updates. It proposes Test-time Batch Renormalisation (TBR) and Dynamic Online re-weighTing (DOT) to address them [zhao-2023-delta]. CW-TENT shares the re-weighting spirit but differs in goal—denoising gradients rather than debiasing classes—and in mechanism—entropy-derived weights rather than class frequency estimates. The incompatibility of our results with DELTA’s success hints that reliable normalisation statistics might be a prerequisite for any weighting to be effective.  Regularised objectives in test-time scenarios. Work on weakly supervised salient object detection demonstrates that adding a regularised loss can stabilise adaptation [author-year-test]. CW-TENT can be interpreted as adaptive regularisation of the entropy loss, although its naïve form proves fragile.  Comparison. Whereas DELTA adds statistical correction and class-level re-weighting, and regularised losses add auxiliary penalties, CW-TENT tries to accelerate plain entropy minimisation via sample-level confidence weights. Our empirical evidence shows that this narrower intervention is insufficient under heavy corruption, delineating the boundary between effective and ineffective re-weighting schemes.",
    "background": "Problem setting and notation. A pre-trained source model f_θ, trained on clean CIFAR-10, receives a stream of target samples x_t from CIFAR-10-C (severity 5) without labels. At each time step t, a mini-batch of size B is processed. The model outputs softmax probabilities p_i ∈ ℝ^C for each sample i. Only the affine BatchNorm parameters (γ, β) are updated; all other weights stay frozen.  Entropy minimisation. The per-sample entropy is H_i = −Σ_c p_{i,c} log p_{i,c}. TENT minimises the batch-averaged entropy L = (1/B) Σ_i H_i via SGD over γ and β, performing several gradient steps before moving to the next batch.  Confidence weighting. Define a confidence score s_i = 1 – H_i/log C, which maps uniform predictions to 0 and one-hot predictions to 1. The proposed weighted loss is L_w = Σ_i s_i H_i / Σ_i s_i. This weighting attenuates gradients from highly uncertain samples, ideally yielding a cleaner update direction.  BatchNorm adaptation dynamics. Updating only γ and β has the advantage of maintaining the learned feature extractor while allowing per-channel scaling and shifting compatible with the target statistics. However, the optimisation landscape is shallow; gradients must be sufficiently strong to move the parameters. If most s_i are near zero, as when predictions are almost uniform, the weighted loss and its gradient collapse, preventing learning.  Assumptions. We assume online streaming, no target labels, no access to source data, and default BatchNorm behaviour (training mode for adaptation, evaluation mode for inference). We do not employ batch renormalisation or class-frequency correction, isolating the effect of confidence weighting.",
    "method": "CW-TENT algorithm. For each incoming mini-batch: 1) Enable training mode so that BatchNorm layers collect current batch statistics. 2) Compute logits and softmax probabilities p_i. 3) Compute entropies H_i and confidence weights s_i = 1 – H_i/log C. 4) Evaluate the normalised weighted loss L_w = Σ_i s_i H_i / Σ_i s_i. 5) Perform a single SGD update on γ and β (learning rate 1e-3; momentum optionally 0.9). 6) Switch back to evaluation mode and emit predictions.  Rationale. Early confident samples are expected to lie closer to the target optimum and to point roughly in the same gradient direction. Emphasising them should accelerate convergence and potentially allow a single update step. Normalisation by Σ s_i keeps the learning-rate-to-loss scale stable when the weight sum varies.  Practical variants. If s_i collapses to zero, gradients vanish. Variants include temperature scaling of logits before computing entropy, clipping s_i to a minimum value, or using a small constant in the denominator. Our study deliberately omits such safeguards to test the minimal idea.  Relation to prior work. CW-TENT inherits the architectural and objective design of TENT but differs in loss weighting. Unlike TBR+DOT in DELTA, it does not modify BatchNorm statistics or class bias. Compared with regularised losses [author-year-test], it introduces no extra terms, only re-scaling existing ones.",
    "experimental_setup": "Dataset and corruption. CIFAR-10-C applies fifteen corruption types to CIFAR-10 images. We use severity 5, the most challenging setting, and stream the corrupted test set in mini-batches.  Model and adapters. The source backbone is ResNet-18 (11.7 M parameters). We evaluate: (1) Source (no adaptation); (2) TENT, ten gradient steps per batch; (3) CW-TENT, one step per batch. All adapters update only BatchNorm affine parameters.  Optimiser and hyper-parameters. Both adaptive methods use SGD with learning rate 1e-3. TENT follows its recommended hyper-parameters; CW-TENT adds momentum 0.9. No temperature scaling or weight clipping is applied.  Metrics. The principal metric is top-1 accuracy accumulated over the entire stream. To probe convergence, we plot per-batch accuracy, compute distributions, and test statistical significance with a two-sided Wilcoxon signed-rank test on paired batch accuracies.  Implementation details. Our PyTorch implementation adds four lines to the open-source TENT code to compute s_i and L_w. Experiments run on one NVIDIA A100 GPU; hyper-parameter sweeps, when required, can be parallelised across eight GPUs but are not used in the main study.  Experimental runs. We report two independent runs: proposed-ResNet-18-… (CW-TENT) and comparative-1-ResNet-18-… (TENT). Each run logs predictions, losses, parameter traces, and auxiliary figures.",
    "results": "Overall performance. After processing the full CIFAR-10-C stream, CW-TENT attains 10.11 % accuracy—indistinguishable from random guessing—whereas TENT achieves 39.44 %. The 29.3-percentage-point gap is confirmed significant (p < 0.01, Wilcoxon).  Convergence behaviour. Learning curves (Figure 2) show CW-TENT flat at chance throughout, while TENT improves steadily from 34 % to 39 %. Batch accuracy distributions (Figure 5) illustrate the same pattern: CW-TENT concentrates near zero information, TENT exhibits a long tail of high-accuracy batches.  Error structure. The CW-TENT confusion matrix (Figure 1) reveals bias toward a few classes, with almost no corrective movement across time. The inferred cause is weight collapse: under heavy corruption, p_i is nearly uniform, H_i ≈ log C, and s_i ≈ 0. Consequently, Σ s_i is tiny, gradients vanish, and parameters freeze.  Fairness and hyper-parameter notes. The step budget differs by design: CW-TENT uses one step, TENT uses ten. Nevertheless, the complete lack of adaptation suggests the weighting strategy itself fails under severe uncertainty. Tuning learning rate or adding momentum does not recover performance.  Limitations and ablations. The study evaluates a minimal configuration and does not sweep temperature parameters or multiple inner steps for CW-TENT. Such ablations are left to future work but are unlikely to close a 29-point gap without altering the core idea.  Figures.  Figure 1: Confusion matrix of CW-TENT predictions (filename: confusion_matrix.pdf). Higher diagonal counts indicate better performance.  Figure 2: Accuracy learning curves for Source, TENT, and CW-TENT (filename: learning_curve.pdf). Higher values are better.  Figure 3: Serialized per-batch and aggregate metrics (filename: metrics.json). Higher accuracy is better.  Figure 4: Aggregated final accuracies across runs (filename: aggregated_metrics.json). Higher values are better.  Figure 5: Distribution of batch-wise accuracies (filename: batch_acc_distribution.pdf). More mass at higher accuracies is better.  Figure 6: Final accuracy bar chart comparing methods (filename: final_accuracy_comparison.pdf). Taller bars are better.  Figure 7: p-value report from significance testing (filename: significance_tests.json). Lower values indicate stronger evidence of difference.  The collective evidence demonstrates that CW-TENT, as currently formulated, is ineffective for severe corruptions.",
    "conclusion": "We set out to accelerate entropy-based test-time adaptation by down-weighting uncertain samples. CW-TENT introduces a single-line per-sample confidence weight yet keeps the architecture, objective, and optimiser otherwise identical to TENT. On CIFAR-10-C, this minimal change proved insufficient: with one gradient step per batch, CW-TENT failed to improve over chance, whereas a ten-step TENT baseline restored nearly 40 % accuracy. Diagnostic plots attribute the failure to weight collapse and gradient starvation under severe uncertainty.  The study contributes a clear negative result and a fine-grained analysis of why naïve confidence weighting collapses. To revive the idea, future research should: (1) calibrate probabilities or apply temperature scaling before computing weights; (2) clip or re-normalise weights to preserve gradient magnitude; (3) combine confidence weighting with robust batch-statistic estimation such as Batch Renormalisation and dynamic class re-weighting [zhao-2023-delta]; (4) investigate adaptive multi-step schedules and regularised losses proven effective in other domains [author-year-test]. Addressing these points is essential before confidence-weighted entropy minimisation can offer the promised latency benefits in real-time adaptive systems."
  },
  "references_bib": "% ===========================================\n% REQUIRED CITATIONS\n% These papers must be cited in the manuscript\n% ===========================================\n\n@article{author-year-test,\n title = {Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection}\n}\n\n@article{zhao-2023-delta,\n abstract = {Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.},\n arxiv_url = {https://arxiv.org/pdf/2301.13018v1.pdf},\n author = {Bowen Zhao and Chen Chen and Shu-Tao Xia},\n title = {DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION},\n year = {2023}\n}\n"
}